{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "source": [
    "# Machine Learning, Overfitting e Cross-Validation\n",
    "\n",
    "Prof. Daniel de Abreu Pereira Uhr\n",
    "\n",
    "## Conteúdo\n",
    "\n",
    "* Machine Learning\n",
    "* Overfitting\n",
    "* Cross-Validation\n",
    "\n",
    "## Referências\n",
    "\n",
    "**Principais:**\n",
    "* Microsoft EconML: https://econml.azurewebsites.net/\n",
    "* UBER CausalML: https://causalml.readthedocs.io/en/latest/\n",
    "* Microsoft DoWhy: https://microsoft.github.io/dowhy/\n",
    "* Google CausalImpact: https://google.github.io/CausalImpact/CausalImpact.html\n",
    "* Prophet: https://facebook.github.io/prophet/\n",
    "* Statsmodels: https://www.statsmodels.org/stable/index.html\n",
    "* Scikit-learn: https://scikit-learn.org/stable/index.html\n",
    "* XGBoost: https://xgboost.readthedocs.io/en/latest/\n",
    "* LightGBM: https://lightgbm.readthedocs.io/en/latest/\n",
    "* CatBoost: https://catboost.ai/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning\n",
    "\n",
    "Machine Learning é uma área da Inteligência Artificial que estuda métodos computacionais para aprender a partir de dados. O aprendizado é feito por meio de algoritmos que constroem modelos matemáticos a partir de dados de treinamento. Esses modelos são usados para fazer previsões ou tomar decisões sem serem explicitamente programados.\n",
    "\n",
    "Os modelos de Machine Learning são divididos em duas categorias principais: supervisionados e não supervisionados.\n",
    "\n",
    "### Aprendizado Supervisionado\n",
    "\n",
    "No aprendizado supervisionado, o algoritmo é treinado em um conjunto de dados rotulados, ou seja, cada exemplo de treinamento é um par de entrada-saída. O objetivo é aprender uma função que mapeia as entradas para as saídas.\n",
    "\n",
    "Os principais tipos de problemas de aprendizado supervisionado são classificação e regressão.\n",
    "\n",
    "#### Classificação\n",
    "\n",
    "Na classificação, o objetivo é prever a classe de uma instância de entrada. Por exemplo, um modelo de classificação pode ser treinado para prever se um e-mail é spam ou não spam.\n",
    "\n",
    "#### Regressão\n",
    "\n",
    "Na regressão, o objetivo é prever um valor contínuo. Por exemplo, um modelo de regressão pode ser treinado para prever o preço de uma casa com base em suas características.\n",
    "\n",
    "### Aprendizado Não Supervisionado\n",
    "\n",
    "No aprendizado não supervisionado, o algoritmo é treinado em um conjunto de dados não rotulados, ou seja, não há saídas associadas a cada exemplo de treinamento. O objetivo é encontrar padrões nos dados, como grupos de instâncias semelhantes.\n",
    "\n",
    "Os principais tipos de problemas de aprendizado não supervisionado são clustering e redução de dimensionalidade.\n",
    "\n",
    "#### Clustering\n",
    "\n",
    "No clustering, o objetivo é agrupar instâncias semelhantes em clusters. Por exemplo, um algoritmo de clustering pode ser usado para agrupar clientes com base em suas compras.\n",
    "\n",
    "#### Redução de Dimensionalidade\n",
    "\n",
    "Na redução de dimensionalidade, o objetivo é reduzir a dimensionalidade dos dados, mantendo o máximo de informação possível. Por exemplo, um algoritmo de redução de dimensionalidade pode ser usado para visualizar dados em um espaço de menor dimensão.\n",
    "\n",
    "## Overfitting\n",
    "\n",
    "Overfitting é um problema comum em Machine Learning, no qual um modelo se ajusta demais aos dados de treinamento e não generaliza bem para novos dados. Isso pode levar a um desempenho ruim do modelo em dados de teste.\n",
    "\n",
    "Existem várias maneiras de evitar o overfitting:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularização é uma técnica usada em estatística, econometria e aprendizado de máquina para prevenir overfitting e melhorar a capacidade de generalização de um modelo. Ela funciona adicionando uma penalidade à função de perda do modelo, desincentivando coeficientes muito grandes ou complexidades excessivas.\n",
    "\n",
    "\n",
    "**Por que Regularização é Necessária?**\n",
    "\n",
    "Quando um modelo é muito flexível (e.g., possui muitos parâmetros ou uma arquitetura muito complexa), ele pode ajustar o ruído dos dados de treino em vez de capturar apenas os padrões relevantes. Isso leva a um bom desempenho nos dados de treino, mas uma performance ruim em dados novos (overfitting).\n",
    "\n",
    "Regularização força o modelo a ser mais simples, penalizando ajustes muito extremos nos parâmetros.\n",
    "\n",
    "**Como funciona a Regularização?**\n",
    "\n",
    "Regularização modifica a função de perda adicionando um termo que penaliza a complexidade do modelo. Esse termo de penalização depende dos coeficientes $\\beta$ do modelo.\n",
    "\n",
    "\n",
    "1. Função de perda padrão (sem regularização): \n",
    "\n",
    "No caso de uma regressão linear, a função de perda geralmente é o erro quadrático médio (MSE):\n",
    "\n",
    "$$L(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "\n",
    "2. Função de perda com regularização:\n",
    "\n",
    "Ao adicionar um termo de regularização, a função de perda se torna:\n",
    "\n",
    "$$L(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda R(\\beta)$$\n",
    "\n",
    "* R(β): Termo de regularização que mede a complexidade do modelo.\n",
    "* $λ>0$: Hiperparâmetro que controla a intensidade da penalização."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "# Definir as variáveis\n",
    "X = ['casada', 'mage', 'medu', 'fhisp', 'mhisp', 'foreign', 'alcohol', 'deadkids', 'nprenatal', 'mrace', 'frace', 'fage', 'fedu']\n",
    "D = \"D\"\n",
    "y = \"Y\"\n",
    "folds = 5\n",
    "\n",
    "# RandomForest - Estimando m_D e m_y\n",
    "rf_model_D = RandomForestRegressor(n_estimators=100)\n",
    "D_res_rf = df[D] - cross_val_predict(rf_model_D, df[X], df[D], cv=folds)\n",
    "\n",
    "rf_model_y = RandomForestRegressor(n_estimators=100)\n",
    "y_res_rf = df[y] - cross_val_predict(rf_model_y, df[X], df[y], cv=folds)\n",
    "\n",
    "# GradientBoosting - Estimando m_D e m_y\n",
    "gb_model_D = GradientBoostingRegressor(n_estimators=100)\n",
    "D_res_gb = df[D] - cross_val_predict(gb_model_D, df[X], df[D], cv=folds)\n",
    "\n",
    "gb_model_y = GradientBoostingRegressor(n_estimators=100)\n",
    "y_res_gb = df[y] - cross_val_predict(gb_model_y, df[X], df[y], cv=folds)\n",
    "\n",
    "# Calculando o MSE para cada modelo\n",
    "mse_D_rf = mean_squared_error(df[D], df[D] - D_res_rf)  # Random Forest para D\n",
    "mse_y_rf = mean_squared_error(df[y], df[y] - y_res_rf)  # Random Forest para y\n",
    "\n",
    "mse_D_gb = mean_squared_error(df[D], df[D] - D_res_gb)  # Gradient Boosting para D\n",
    "mse_y_gb = mean_squared_error(df[y], df[y] - y_res_gb)  # Gradient Boosting para y\n",
    "\n",
    "# Exibir os resultados de MSE\n",
    "print(\"MSE for D - Random Forest:\", mse_D_rf)\n",
    "print(\"MSE for y - Random Forest:\", mse_y_rf)\n",
    "print(\"MSE for D - Gradient Boosting:\", mse_D_gb)\n",
    "print(\"MSE for y - Gradient Boosting:\", mse_y_gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparação para $D$ (Variável de Tratamento):\n",
    "\n",
    "* Random Forest MSE: 0.1506\n",
    "* Gradient Boosting MSE: 0.1323\n",
    "\n",
    "Aqui, o Gradient Boosting tem um desempenho melhor na predição de $D$, pois seu MSE é menor. Isso significa que ele foi mais eficiente na captura da relação entre as variáveis explicativas $X$ e o tratamento $D$, resultando em resíduos menores.\n",
    "\n",
    "Comparação para $Y$\n",
    "* Random Forest MSE: 352060.35\n",
    "* Gradient Boosting MSE: 309063.98\n",
    "\n",
    "Novamente, o Gradient Boosting apresenta um MSE menor na predição de $y$, indicando que ele conseguiu capturar melhor a relação entre as variáveis explicativas  $X$ e o resultado $Y$, quando comparado ao Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos fazer de forma geral a análise para diversos modelos e verificar qual o melhor baseados no critério de erro quadrado médio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Lasso, Ridge, ElasticNet\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Definir os modelos que vamos testar\n",
    "models = {\n",
    "    \"RandomForest\": RandomForestRegressor(n_estimators=100),\n",
    "    \"GradientBoosting\": GradientBoostingRegressor(n_estimators=100),\n",
    "    \"Lasso\": Lasso(alpha=0.1),\n",
    "    \"Ridge\": Ridge(alpha=1.0),\n",
    "    \"ElasticNet\": ElasticNet(alpha=0.1, l1_ratio=0.5),\n",
    "    \"MLP\": MLPRegressor(hidden_layer_sizes=(50,), max_iter=1000),\n",
    "    \"SVR\": SVR()\n",
    "}\n",
    "\n",
    "# Definir as variáveis\n",
    "X = ['casada', 'mage', 'medu', 'fhisp', 'mhisp', 'foreign', 'alcohol', 'deadkids', 'nprenatal', 'mrace', 'frace', 'fage', 'fedu']\n",
    "D = \"D\"\n",
    "y = \"Y\"\n",
    "folds = 5\n",
    "\n",
    "# Dicionário para armazenar os MSE de cada modelo para D e y\n",
    "mse_results = {}\n",
    "\n",
    "# Loop para ajustar os modelos e calcular os MSE para D e y\n",
    "for name, model in models.items():\n",
    "    # Estimativa de D com validação cruzada\n",
    "    D_res = df[D] - cross_val_predict(model, df[X], df[D], cv=folds)\n",
    "    # Estimativa de y com validação cruzada\n",
    "    y_res = df[y] - cross_val_predict(model, df[X], df[y], cv=folds)\n",
    "    \n",
    "    # Calcular MSE para D e y\n",
    "    mse_D = mean_squared_error(df[D], df[D] - D_res)\n",
    "    mse_y = mean_squared_error(df[y], df[y] - y_res)\n",
    "    \n",
    "    # Armazenar os resultados\n",
    "    mse_results[name] = {\"MSE for D\": mse_D, \"MSE for y\": mse_y}\n",
    "\n",
    "# Exibir os resultados\n",
    "for model_name, mse in mse_results.items():\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"  MSE for D: {mse['MSE for D']}\")\n",
    "    print(f\"  MSE for y: {mse['MSE for y']}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para os resíduos de $D$: O Gradient Boosting é o modelo preferido, pois ele obteve o menor MSE para prever a variável de tratamento.\n",
    "\n",
    "Para os resíduos de $y$: O Lasso foi o melhor em prever o resultado.\n",
    "\n",
    "Vou aumentar o número fold para 8 e verificar se os resultados mudam. Esse aumento de 5 para 8 folds é importante para garantir que o modelo seja treinado em uma amostra maior, o que pode melhorar a precisão das previsões. Além de ser uma prática para verificar a robustez dos resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Lasso, Ridge, ElasticNet\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Definir os modelos que vamos testar\n",
    "models = {\n",
    "    \"RandomForest\": RandomForestRegressor(n_estimators=100),\n",
    "    \"GradientBoosting\": GradientBoostingRegressor(n_estimators=100),\n",
    "    \"Lasso\": Lasso(alpha=0.1),\n",
    "    \"Ridge\": Ridge(alpha=1.0),\n",
    "    \"ElasticNet\": ElasticNet(alpha=0.1, l1_ratio=0.5),\n",
    "    \"MLP\": MLPRegressor(hidden_layer_sizes=(50,), max_iter=1000),\n",
    "    \"SVR\": SVR()\n",
    "}\n",
    "\n",
    "# Definir as variáveis\n",
    "X = ['casada', 'mage', 'medu', 'fhisp', 'mhisp', 'foreign', 'alcohol', 'deadkids', 'nprenatal', 'mrace', 'frace', 'fage', 'fedu']\n",
    "D = \"D\"\n",
    "y = \"Y\"\n",
    "folds = 8\n",
    "\n",
    "# Dicionário para armazenar os MSE de cada modelo para D e y\n",
    "mse_results = {}\n",
    "\n",
    "# Loop para ajustar os modelos e calcular os MSE para D e y\n",
    "for name, model in models.items():\n",
    "    # Estimativa de D com validação cruzada\n",
    "    D_res = df[D] - cross_val_predict(model, df[X], df[D], cv=folds)\n",
    "    # Estimativa de y com validação cruzada\n",
    "    y_res = df[y] - cross_val_predict(model, df[X], df[y], cv=folds)\n",
    "    \n",
    "    # Calcular MSE para D e y\n",
    "    mse_D = mean_squared_error(df[D], df[D] - D_res)\n",
    "    mse_y = mean_squared_error(df[y], df[y] - y_res)\n",
    "    \n",
    "    # Armazenar os resultados\n",
    "    mse_results[name] = {\"MSE for D\": mse_D, \"MSE for y\": mse_y}\n",
    "\n",
    "# Exibir os resultados\n",
    "for model_name, mse in mse_results.items():\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"  MSE for D: {mse['MSE for D']}\")\n",
    "    print(f\"  MSE for y: {mse['MSE for y']}\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
