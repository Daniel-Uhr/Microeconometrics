{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ortogonalização, Double Machine Learning e CATE\n",
    "\n",
    "Prof. Daniel de Abreu Pereira Uhr\n",
    "\n",
    "## Conteúdo\n",
    "\n",
    "* Ortogonalização\n",
    "  * Aplicação do Procedimento de Ortogonalização no Python\n",
    "* DML - Orthogonal/Double Machine Learning\n",
    "  * Aplicação do DML no Python\n",
    "* CATE (Conditional Average Treatment Effects - Efeitos Heterogêneos do Tratamento)\n",
    "* CATE e o Arcabouço de Resultados Potenciais\n",
    "  * Estimação do CATE com DML no Python\n",
    "\n",
    "\n",
    "## Referências\n",
    "\n",
    "**Principais:**\n",
    "* Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., & Robins, J. (2018). Double/debiased machine learning for treatment and structural parameters. The Econometrics Journal, Volume 21, Issue 1, 1 February 2018, Pages C1–C68, https://doi.org/10.1111/ectj.12097\n",
    "  \n",
    "* Microsoft EconML: https://econml.azurewebsites.net/\n",
    "* UBER CausalML: https://causalml.readthedocs.io/en/latest/\n",
    "* DoubleML for python: https://github.com/DoubleML/doubleml-for-py ou https://docs.doubleml.org/stable/index.html\n",
    "\n",
    "**Complementares:**\n",
    "* Chernozhukov, V. and C. Hansen (2004). The effects of 401 (k) participation on the wealth distribution: an instrumental quantile regression analysis. Review of Economics and Statistics 86, 735–51. \n",
    "* Chernozhukov, V., D. Chetverikov and K. Kato (2014). Gaussian approximation of suprema of empirical processes. Annals of Statistics 42, 1564–97. \n",
    "* Chernozhukov, V., J. Escanciano, H. Ichimura, W. Newey and J. Robins (2016). Locally robust semiparametric estimation. Preprint (arXiv:1608.00033). \n",
    "* Chernozhukov, V., C. Hansen and M. Spindler (2015a). Post-selection and post-regularization inference in linear models with very many controls and instruments. Americal Economic Review: Papers and Proceedings 105, 486–90. \n",
    "* Chernozhukov, V., C. Hansen and M. Spindler (2015b). Valid post-selection and post-regularization inference: an elementary, general approach. Annual Review of Economics 7, 649–88."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ortogonalização\n",
    "\n",
    "A ideia de ortogonalização é baseada em um teorema elaborado por três econometristas em 1933, Ragnar Frisch, Frederick V. Waugh e Michael C. Lovell. Simplificando, afirma que você pode decompor qualquer modelo de regressão linear multivariável em três estágios ou modelos. \n",
    "\n",
    "Digamos que você tem uma matriz de covariáveis $X$, e voce particiona ela em duas partes, $X_{1}$ e $D$. \n",
    "\n",
    "* **Primeira Etapa**\n",
    "\n",
    "Pegamos o primeiro conjunto de variáveis $X_{1}$ e fazemos uma regressão linear de $X_{1}$ em $Y$, onde $\\theta_{1}$ é o vetor de parâmetros\n",
    "\n",
    "$$ y_{i} = \\theta_{0} + \\theta_{1} X_{1i} + \\epsilon_{i}$$\n",
    "\n",
    "e guardamos os resíduos dessa regressão ($y^{*}$).\n",
    "\n",
    "$$ y^{*}_{i} = y_{i} - \\hat{y}_{i} = y_{i} - ( \\hat{\\theta}_{0} + \\hat{\\theta}_{1} X_{1i} )$$\n",
    "\n",
    "* **Segunda Etapa**\n",
    "\n",
    "Pegamos novamente o primeiro conjunto de características, mas agora executamos um modelo onde estimamos o segundo conjunto de características ($D$)\n",
    "\n",
    "$$ D_{i} = \\gamma_{0} + \\gamma_{1} X_{1i} + e_{i}$$\n",
    "\n",
    "Aqui, estamos usando o primeiro conjunto de recursos para prever o segundo conjunto de recursos. Por fim, consideramos também os resíduos desta segunda etapa.\n",
    "\n",
    "$$ D_{i}^{*} = D_{i} - (\\hat{\\gamma_{0}} + \\hat{\\gamma_{1}} X_{1i})$$\n",
    "\n",
    "* **Terceira etapa**\n",
    "\n",
    "Por fim, pegamos os resíduos do primeiro e do segundo estágio e estimamos o seguinte modelo\n",
    "\n",
    "$$ y_{i}^{*} = \\alpha_{0} + \\beta_{2} D_{i}^{*} + e_{i}$$\n",
    "\n",
    "\n",
    "* **Teorema Frisch – Waugh – Lovell (FWL)**\n",
    "\n",
    "O teorema FWL afirma que a estimativa do parâmetro $\\hat{\\beta}_{2}$, estimado anteriormente, é equivalente ao que obtemos ao executar a regressão completa, com todas as covariáveis.\n",
    "\n",
    "$$ y_{i} = \\beta_{0} + \\beta_{1} X_{1i} + \\beta_{2} D_{i} + e_{i}$$\n",
    "\n",
    "\n",
    "**Intuição do teorema FWL**\n",
    "\n",
    "Sabemos que a regressão é um modelo muito especial. Cada um de seus parâmetros tem a interpretação de uma derivada parcial, quanto seria Y se X aumentasse em uma unidade, mantendo todos as outras covariáveis constantes. Sabemos também que se omitirmos variáveis ​​da regressão, teremos viés. Especificamente, viés variável omitido (ou viés de confusão). Ainda assim, Frisch-Waugh-Lovell está dizendo que posso dividir meu modelo de regressão em duas partes, nenhuma delas contendo o conjunto completo de recursos, e ainda assim obter a mesma estimativa que obteria executando a regressão inteira. \n",
    "\n",
    "O teorema fornece algumas dicas sobre o que a regressão linear está fazendo. Para obter o coeficiente de uma variável $X_{k}$, a regressão primeiro usa todas as outras variáveis ​​para prever $X_{k}$ e pega os resíduos. Isso “limpa” $X_{k}$ de qualquer influência dessas variáveis. Dessa forma, quando tentamos entender o impacto de $X_{k}$ sobre $Y$, estará livre de viés de variável omitida. Em segundo lugar, a regressão usa todas as outras variáveis ​​para prever $Y$ e pega os resíduos. Isso “limpa” $Y$ de qualquer influência dessas variáveis, reduzindo a variância de $Y$ para que seja mais fácil ver como $X_{k}$ afeta $Y$.\n",
    "\n",
    "A regressão linear está estimando o impacto de $D$ sobre $y$ enquanto contabiliza $X_{1}$. Isso é importante para inferência causal. \n",
    "\n",
    "Assim, podemos construir um modelo que preveja um tratamento ($D$) usando as covariáveis $X$, um modelo que prevê o resultado $y$ usando as mesmas covariáveis, pegar os resíduos de ambos os modelos e executar um modelo que estime como o resíduo de $D$ afetam os resíduos de $y$. Este último modelo vai me dizer como $D$ afeta $y$ enquanto controla por $X$. Ou seja, **os dois primeiros modelos controlam as variáveis de confusão**. Eles estão **gerando dados que são praticamente aleatórios**. Lembre que é isso que estaria distorcendo seus dados. Então, usamos no modelo final para estimar o efeito causal de interesse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicação do Procedimento de Ortogonalização no Python\n",
    "\n",
    "Vamos aplicar o procedimento de ortogonalização considerando um modelo de regressão linear simples. Vamos realizar a orgonalização supondo linearidade entre as variáveis para entender o conceito. Posteriormente, vamos aplicar o procedimento de ortogonalização em um modelo de machine learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame\n",
    "df = pd.read_stata(\"https://github.com/Daniel-Uhr/data/raw/main/cattaneo2.dta\")\n",
    "\n",
    "# Criar a variável de resultado\n",
    "df['Y'] = df['bweight']\n",
    "\n",
    "# Criar a variável 'Treated' com valor 1 se 'mbsmoke' for 'smoker', caso contrário 0\n",
    "df['D'] = np.where(df['mbsmoke'] == 'smoker', 1, 0)\n",
    "\n",
    "# Criar a variável 'casada' com valor 1 se 'mmarried' for 'married', caso contrário 0\n",
    "df['casada'] = np.where(df['mmarried'] == 'married', 1, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para desviar este conjunto de dados, precisaremos de dois modelos. O primeiro modelo, vamos chamá-lo $M_{D}(X)$, prevê o tratamento (Se a gestante é fumante, no nosso caso) utilizando os confundidores. É um dos estágios que vimos acima, no teorema de Frisch–Waugh–Lovell.\n",
    "\n",
    "Assim que tivermos este modelo, construiremos os resíduos\n",
    "\n",
    "$$ D_{i}^{*} = D_{i} - M_{D}(X_{i})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_D = smf.ols(\"D ~ 1 + casada + mage + medu + fhisp + mhisp + foreign + alcohol + deadkids + nprenatal + mrace + frace + fage + fedu\", data=df).fit()\n",
    "df['D_star'] = df['D'] - m_D.predict(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Você pode pensar neste resíduo como uma versão do tratamento que é imparcial ou, melhor ainda, que é impossível de prever a partir dos fatores de confusão $X$. Como os fatores de confusão já eram usados ​​para prever $D$, o resíduo é, por definição, imprevisível com com $X$. Outra maneira de dizer isso é que o viés foi explicado pelo modelo $M_{D}(X)$, produzindo $D_{i}^{*}$ que é tão bom quanto atribuído aleatoriamente. É claro que isso só funciona se tivermos em $X$ todos os fatores de confusão que causam ambos $D$ e $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Também podemos construir resíduos para o resultado.\n",
    "\n",
    "$$ y_{i}^{*} = y_{i} - M_{y}(X_{i})$$\n",
    "\n",
    "\n",
    "Este é outro estágio do teorema de Frisch – Waugh – Lovell. Isso não torna o conjunto menos tendencioso, mas facilita a estimativa do efeito, reduzindo a variância em $y$. Mais uma vez você pode pensar $y_{i}^{*}$ como uma versão de $y_{i}$ imprevisível de $X$ ou que teve todas as suas variações devido a $X$ explicadas. Pense nisso. Nós já usamos $X$ para prever $y$ com $M_{y}(X_{i})$. E $y_{i}^{*}$ é o erro dessa previsão. Então, por definição, não é possível prever isso a partir de $X$. Todas as informações em $X$ para prever $y$ já foram usadas. Se for esse o caso, a única coisa que resta para explicar $y_{i}^{*}$ é algo que não usamos usamos para construí-lo (não incluído em $X$), que é apenas o tratamento (novamente, assumindo que não há fatores de confusão não medidos).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_y = smf.ols(\"Y ~  1 + casada + mage + medu + fhisp + mhisp + foreign + alcohol + deadkids + nprenatal + mrace + frace + fage + fedu\", data=df).fit()\n",
    "df['y_star'] = df['Y'] - m_y.predict(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim, aplicando o teorema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 y_star   R-squared:                       0.021\n",
      "Model:                            OLS   Adj. R-squared:                  0.021\n",
      "Method:                 Least Squares   F-statistic:                     100.6\n",
      "Date:                Thu, 19 Dec 2024   Prob (F-statistic):           1.91e-23\n",
      "Time:                        16:44:21   Log-Likelihood:                -35858.\n",
      "No. Observations:                4642   AIC:                         7.172e+04\n",
      "Df Residuals:                    4640   BIC:                         7.173e+04\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept   1.586e-11      8.041   1.97e-12      1.000     -15.764      15.764\n",
      "D_star      -220.0220     21.932    -10.032      0.000    -263.020    -177.024\n",
      "==============================================================================\n",
      "Omnibus:                      553.208   Durbin-Watson:                   1.999\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1427.823\n",
      "Skew:                          -0.673   Prob(JB):                    8.96e-311\n",
      "Kurtosis:                       5.360   Cond. No.                         2.73\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "FWL1 = smf.ols(\"y_star ~ D_star\", data=df).fit()\n",
    "print(FWL1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos comparar o resultado com o OLS tradicional com covariáveis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      Y   R-squared:                       0.104\n",
      "Model:                            OLS   Adj. R-squared:                  0.102\n",
      "Method:                 Least Squares   F-statistic:                     38.49\n",
      "Date:                Thu, 19 Dec 2024   Prob (F-statistic):          5.80e-100\n",
      "Time:                        16:44:25   Log-Likelihood:                -35858.\n",
      "No. Observations:                4642   AIC:                         7.175e+04\n",
      "Df Residuals:                    4627   BIC:                         7.184e+04\n",
      "Df Model:                          14                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept   2851.5203     54.983     51.861      0.000    2743.727    2959.314\n",
      "D           -220.0220     21.963    -10.018      0.000    -263.080    -176.964\n",
      "casada        42.7633     23.589      1.813      0.070      -3.482      89.009\n",
      "mage           3.2878      1.935      1.699      0.089      -0.506       7.082\n",
      "medu          -1.5659      4.130     -0.379      0.705      -9.664       6.532\n",
      "fhisp        -22.9258     63.841     -0.359      0.720    -148.084     102.232\n",
      "mhisp         15.0538     68.152      0.221      0.825    -118.557     148.664\n",
      "foreign       -6.8607     39.628     -0.173      0.863     -84.550      70.828\n",
      "alcohol       -9.5560     46.486     -0.206      0.837    -100.690      81.578\n",
      "deadkids     -13.8988     18.872     -0.736      0.461     -50.897      23.099\n",
      "nprenatal     26.1331      2.328     11.225      0.000      21.569      30.697\n",
      "mrace        176.3951     44.950      3.924      0.000      88.272     264.518\n",
      "frace         50.9355     44.172      1.153      0.249     -35.663     137.534\n",
      "fage          -0.9152      1.194     -0.767      0.443      -3.256       1.425\n",
      "fedu           1.0262      3.139      0.327      0.744      -5.128       7.180\n",
      "==============================================================================\n",
      "Omnibus:                      553.208   Durbin-Watson:                   1.999\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1427.823\n",
      "Skew:                          -0.673   Prob(JB):                    8.96e-311\n",
      "Kurtosis:                       5.360   Cond. No.                         485.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "ols = smf.ols(\"Y ~ D + 1 + casada + mage + medu + fhisp + mhisp + foreign + alcohol + deadkids + nprenatal + mrace + frace + fage + fedu\", data=df).fit()\n",
    "print(ols.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realmente o teorema de Frisch-Waugh-Lovell funciona mesmo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos concluir que depois de fazermos as duas transformações, a única coisa que resta para prever esses resíduos é o tratamento. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para resumir, ao prever o tratamento, construímos $D^{*}$ que funciona como uma versão imparcial do tratamento; ao prever o resultado, construímos $y^{*}$ que é uma versão do resultado que só pode ser explicada se usarmos o tratamento. Esses dados, onde substituímos por $y$ por $y^{*}$ e $D$ por $D^{*}$, são os dados desviados que queríamos. Podemos usá-lo para avaliar nosso modelo causal da mesma forma que fizemos anteriormente, usando dados aleatórios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  DML - Orthogonal/Double Machine Learning\n",
    "\n",
    "* Double Machine Learning - DML\n",
    "* Double/Debiased Machine Learning - DDML \n",
    "\n",
    "Quando temos muitas possíveis variáveis ​​de controle, podemos querer selecionar as mais relevantes, possivelmente capturando não linearidades e interações. Algoritmos de aprendizado de máquina são perfeitos para essa tarefa. No entanto, nesses casos, estamos introduzindo um viés que é chamado de regularização ou pré-teste, ou viés de seleção de recursos ($X$). \n",
    "\n",
    "Ou seja, o que acontece se a dimensão de $X$ aumenta e não conhecemos a forma funcional através da qual $X$ afeta $Y$ e $D$?\n",
    "\n",
    "Nesses casos, podemos usar **algoritmos de aprendizado de máquina** para **descobrir essas relações não lineares de alta dimensão**.\n",
    "\n",
    "No artigo de **Chernozhukov et al (2018)**, os autores mostraram que também é possível fazer ortogonalização com modelos de aprendizado de máquina. \n",
    "\n",
    "$$ y_{i}^{*} = y_{i} - M_{y}(X_{i})$$\n",
    "\n",
    "$$ D_{i}^{*} = D_{i} - M_{D}(X_{i})$$\n",
    "\n",
    "onde $M_{y}$ e $M_{D}$ são modelos de aprendizado de máquina.\n",
    "\n",
    "\n",
    "***Machine Learning (ML)* e *Overfitting*** : Os modelos de aprendizado de máquina (ML) podem ajustar-se perfeitamente aos dados, ou melhor, superajustá-los (*Overfitting* / \"sobreajuste\"). \n",
    "\n",
    "Apenas olhando para as equações anteriores, podemos saber o que acontecerá nesse caso.\n",
    "  * Se $M_{y}$ de alguma forma, os resíduos serão todos muito próximos de zero. Se isso acontecer, será difícil descobrir como $D$ afeta isso. \n",
    "  * Se $M_{D}$ de alguma forma superajusta, seus resíduos também serão próximos de zero. Conseqüentemente, não haverá variação no resíduo do tratamento para ver como isso pode impactar o resultado.\n",
    "\n",
    "O que fazer em caso de *Overfitting*?\n",
    "\n",
    "* **Regularização**: técnica que adiciona um termo à função de perda que penaliza os coeficientes do modelo. Isso faz com que o modelo seja menos sensível aos dados de treinamento, evitando o superajuste.\n",
    "\n",
    "Quais os modelos de ML mais comuns para previsão?\n",
    "\n",
    "* Random Forest: é um modelo de aprendizado de máquina que pode ser usado tanto para classificação quanto para regressão. Ele é um modelo de conjunto que treina várias árvores de decisão em subconjuntos aleatórios dos dados e faz a média de suas previsões.\n",
    "* Gradient Boosting: é um modelo de aprendizado de máquina que constrói um modelo aditivo de forma progressiva. Ele permite a otimização de funções de perda diferenciáveis arbitrárias.\n",
    "* Redes Neurais: são modelos de aprendizado de máquina que são inspirados na forma como o cérebro humano funciona. Eles são compostos por camadas de neurônios que processam e transmitem informações.\n",
    "* Support Vector Machines: são modelos de aprendizado de máquina que são usados para classificação e regressão. Eles são eficazes em espaços de alta dimensão e são capazes de lidar com dados não lineares.\n",
    "* Outros modelos de ML mais usados em economia são: LASSO, Ridge, Elastic Net, etc. LASSO é um método de regressão que adiciona uma penalidade L1 à função de perda. Isso faz com que alguns coeficientes sejam exatamente zero, o que é útil para seleção de recursos. Ridge é um método de regressão que adiciona uma penalidade L2 à função de perda. Isso faz com que os coeficientes sejam menores, o que é útil para reduzir a variância. Elastic Net é um método de regressão que combina as penalidades L1 e L2. Isso permite que você selecione recursos e reduza a variância ao mesmo tempo.\n",
    "\n",
    "**Validação Cruzada (Cross-Validation)**:  é uma técnica estatística usada para avaliar a performance de um modelo preditivo e sua capacidade de generalização para novos dados. Em essência, é um método de dividir os dados disponíveis em partes para treinar e testar o modelo de maneira eficiente e confiável.\n",
    "\n",
    "* **Validação Cruzada K-fold**: é uma técnica de validação cruzada que divide os dados em k partes. O modelo é treinado em k-1 partes e testado na parte restante. Isso é feito k vezes, de modo que cada parte seja usada como conjunto de teste exatamente uma vez.\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"images\\kfold-cv.png\"  alt=\"Imagem\" style=\"width: 450px;\"/>\n",
    "</div>\n",
    "\n",
    "Felizmente, esse tipo de validação cruzada é muito fácil de implementar usando `cross_val_predicta` função do Sklearn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos aplicar o DML considerando dois modelos de machine learning. Primeiramente consideramos o Random Forest e posteriormente o Gradient Boosting.\n",
    "\n",
    "Vejamos a aplicação com o Random Forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "X = ['casada', 'mage', 'medu', 'fhisp', 'mhisp', 'foreign', 'alcohol', 'deadkids', 'nprenatal', 'mrace', 'frace', 'fage', 'fedu']\n",
    "D = \"D\"\n",
    "y = \"Y\"\n",
    "\n",
    "folds = 5\n",
    "\n",
    "np.random.seed(123)\n",
    "m_D = RandomForestRegressor(n_estimators=100)\n",
    "D_res1 = df[D] - cross_val_predict(m_D, df[X], df[D], cv=folds)\n",
    "\n",
    "m_y = RandomForestRegressor(n_estimators=100)\n",
    "y_res1 = df[y] - cross_val_predict(m_y, df[X], df[y], cv=folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 y_res1   R-squared:                       0.014\n",
      "Model:                            OLS   Adj. R-squared:                  0.014\n",
      "Method:                 Least Squares   F-statistic:                     66.82\n",
      "Date:                Thu, 19 Dec 2024   Prob (F-statistic):           3.80e-16\n",
      "Time:                        16:44:59   Log-Likelihood:                -36189.\n",
      "No. Observations:                4642   AIC:                         7.238e+04\n",
      "Df Residuals:                    4640   BIC:                         7.239e+04\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      4.1388      8.646      0.479      0.632     -12.812      21.089\n",
      "D_res1      -182.0597     22.272     -8.174      0.000    -225.723    -138.396\n",
      "==============================================================================\n",
      "Omnibus:                      289.287   Durbin-Watson:                   1.991\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              619.258\n",
      "Skew:                          -0.414   Prob(JB):                    3.39e-135\n",
      "Kurtosis:                       4.587   Cond. No.                         2.58\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "FWL_DML1 = smf.ols(\"y_res1 ~ D_res1\", data=df).fit()\n",
    "print(FWL_DML1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vejamos outro exemplo do DML. Considerando Gradient Boosting Machines (GBM) para prever o tratamento e o resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import numpy as np\n",
    "\n",
    "# Definir as variáveis X, D, y\n",
    "X = ['casada', 'mage', 'medu', 'fhisp', 'mhisp', 'foreign', 'alcohol', 'deadkids', 'nprenatal', 'mrace', 'frace', 'fage', 'fedu']\n",
    "D = \"D\"\n",
    "y = \"Y\"\n",
    "\n",
    "# Definir o número de folds para a validação cruzada\n",
    "folds = 5\n",
    "\n",
    "# Garantir reprodutibilidade nos modelos de Gradient Boosting\n",
    "m_D = GradientBoostingRegressor(n_estimators=100, random_state=123)\n",
    "D_res2 = df[D] - cross_val_predict(m_D, df[X], df[D], cv=folds)\n",
    "\n",
    "m_y = GradientBoostingRegressor(n_estimators=100, random_state=123)\n",
    "y_res2 = df[y] - cross_val_predict(m_y, df[X], df[y], cv=folds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 y_res2   R-squared:                       0.019\n",
      "Model:                            OLS   Adj. R-squared:                  0.018\n",
      "Method:                 Least Squares   F-statistic:                     88.46\n",
      "Date:                Thu, 19 Dec 2024   Prob (F-statistic):           7.93e-21\n",
      "Time:                        16:45:10   Log-Likelihood:                -35882.\n",
      "No. Observations:                4642   AIC:                         7.177e+04\n",
      "Df Residuals:                    4640   BIC:                         7.178e+04\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      0.7775      8.082      0.096      0.923     -15.067      16.622\n",
      "D_res2      -209.0031     22.221     -9.405      0.000    -252.568    -165.439\n",
      "==============================================================================\n",
      "Omnibus:                      476.984   Durbin-Watson:                   1.982\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1222.895\n",
      "Skew:                          -0.589   Prob(JB):                    2.83e-266\n",
      "Kurtosis:                       5.222   Cond. No.                         2.75\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "FWL_DML2 = smf.ols(\"y_res2 ~ D_res2\", data=df).fit()\n",
    "print(FWL_DML2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vimos a mecânica do DML. \n",
    "\n",
    "Mas qual o melhor modelo de ML para prever o tratamento e o resultado? \n",
    "\n",
    "Precisamos avaliar, um critério de decisão comum é utilizar o erro quadrático médio (MSE) dos resíduos gerados para $D$ e $Y$ para ambos os modelos, e então comparar seus desempenhos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for D - Random Forest: 0.15049072305334754\n",
      "MSE for y - Random Forest: 350812.5710231999\n",
      "MSE for D - Gradient Boosting: 0.1322742607784923\n",
      "MSE for y - Gradient Boosting: 309191.01191423537\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "# Definir as variáveis\n",
    "X = ['casada', 'mage', 'medu', 'fhisp', 'mhisp', 'foreign', 'alcohol', 'deadkids', 'nprenatal', 'mrace', 'frace', 'fage', 'fedu']\n",
    "D = \"D\"\n",
    "y = \"Y\"\n",
    "folds = 5\n",
    "\n",
    "# RandomForest - Estimando m_D e m_y\n",
    "rf_model_D = RandomForestRegressor(n_estimators=100)\n",
    "D_res_rf = df[D] - cross_val_predict(rf_model_D, df[X], df[D], cv=folds)\n",
    "\n",
    "rf_model_y = RandomForestRegressor(n_estimators=100)\n",
    "y_res_rf = df[y] - cross_val_predict(rf_model_y, df[X], df[y], cv=folds)\n",
    "\n",
    "# GradientBoosting - Estimando m_D e m_y\n",
    "gb_model_D = GradientBoostingRegressor(n_estimators=100)\n",
    "D_res_gb = df[D] - cross_val_predict(gb_model_D, df[X], df[D], cv=folds)\n",
    "\n",
    "gb_model_y = GradientBoostingRegressor(n_estimators=100)\n",
    "y_res_gb = df[y] - cross_val_predict(gb_model_y, df[X], df[y], cv=folds)\n",
    "\n",
    "# Calculando o MSE para cada modelo\n",
    "mse_D_rf = mean_squared_error(df[D], df[D] - D_res_rf)  # Random Forest para D\n",
    "mse_y_rf = mean_squared_error(df[y], df[y] - y_res_rf)  # Random Forest para y\n",
    "\n",
    "mse_D_gb = mean_squared_error(df[D], df[D] - D_res_gb)  # Gradient Boosting para D\n",
    "mse_y_gb = mean_squared_error(df[y], df[y] - y_res_gb)  # Gradient Boosting para y\n",
    "\n",
    "# Exibir os resultados de MSE\n",
    "print(\"MSE for D - Random Forest:\", mse_D_rf)\n",
    "print(\"MSE for y - Random Forest:\", mse_y_rf)\n",
    "print(\"MSE for D - Gradient Boosting:\", mse_D_gb)\n",
    "print(\"MSE for y - Gradient Boosting:\", mse_y_gb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparação para $D$ (Variável de Tratamento):\n",
    "\n",
    "* Random Forest MSE: 0.1506\n",
    "* Gradient Boosting MSE: 0.1323\n",
    "\n",
    "Aqui, o Gradient Boosting tem um desempenho melhor na predição de $D$, pois seu MSE é menor. Isso significa que ele foi mais eficiente na captura da relação entre as variáveis explicativas $X$ e o tratamento $D$, resultando em resíduos menores.\n",
    "\n",
    "Comparação para $Y$\n",
    "* Random Forest MSE: 352060.35\n",
    "* Gradient Boosting MSE: 309063.98\n",
    "\n",
    "Novamente, o Gradient Boosting apresenta um MSE menor na predição de $y$, indicando que ele conseguiu capturar melhor a relação entre as variáveis explicativas  $X$ e o resultado $Y$, quando comparado ao Random Forest.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos fazer de forma geral a análise para diversos modelos e verificar qual o melhor baseados no critério de erro quadrado médio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: RandomForest\n",
      "  MSE for D: 0.15139084627531682\n",
      "  MSE for y: 349909.7289144392\n",
      "\n",
      "Model: GradientBoosting\n",
      "  MSE for D: 0.1322770984849905\n",
      "  MSE for y: 309273.4665657201\n",
      "\n",
      "Model: Lasso\n",
      "  MSE for D: 0.14514535464451975\n",
      "  MSE for y: 308621.37257196667\n",
      "\n",
      "Model: Ridge\n",
      "  MSE for D: 0.1353045086950994\n",
      "  MSE for y: 308669.98106174375\n",
      "\n",
      "Model: ElasticNet\n",
      "  MSE for D: 0.14369145438382802\n",
      "  MSE for y: 308710.90380429785\n",
      "\n",
      "Model: MLP\n",
      "  MSE for D: 0.1431742709965872\n",
      "  MSE for y: 363171.59636381926\n",
      "\n",
      "Model: SVR\n",
      "  MSE for D: 0.1582687258940923\n",
      "  MSE for y: 330887.97070007736\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Lasso, Ridge, ElasticNet\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Definir os modelos que vamos testar\n",
    "models = {\n",
    "    \"RandomForest\": RandomForestRegressor(n_estimators=100),\n",
    "    \"GradientBoosting\": GradientBoostingRegressor(n_estimators=100),\n",
    "    \"Lasso\": Lasso(alpha=0.1),\n",
    "    \"Ridge\": Ridge(alpha=1.0),\n",
    "    \"ElasticNet\": ElasticNet(alpha=0.1, l1_ratio=0.5),\n",
    "    \"MLP\": MLPRegressor(hidden_layer_sizes=(50,), max_iter=1000),\n",
    "    \"SVR\": SVR()\n",
    "}\n",
    "\n",
    "# Definir as variáveis\n",
    "X = ['casada', 'mage', 'medu', 'fhisp', 'mhisp', 'foreign', 'alcohol', 'deadkids', 'nprenatal', 'mrace', 'frace', 'fage', 'fedu']\n",
    "D = \"D\"\n",
    "y = \"Y\"\n",
    "folds = 5\n",
    "\n",
    "# Dicionário para armazenar os MSE de cada modelo para D e y\n",
    "mse_results = {}\n",
    "\n",
    "# Loop para ajustar os modelos e calcular os MSE para D e y\n",
    "for name, model in models.items():\n",
    "    # Estimativa de D com validação cruzada\n",
    "    D_res = df[D] - cross_val_predict(model, df[X], df[D], cv=folds)\n",
    "    # Estimativa de y com validação cruzada\n",
    "    y_res = df[y] - cross_val_predict(model, df[X], df[y], cv=folds)\n",
    "    \n",
    "    # Calcular MSE para D e y\n",
    "    mse_D = mean_squared_error(df[D], df[D] - D_res)\n",
    "    mse_y = mean_squared_error(df[y], df[y] - y_res)\n",
    "    \n",
    "    # Armazenar os resultados\n",
    "    mse_results[name] = {\"MSE for D\": mse_D, \"MSE for y\": mse_y}\n",
    "\n",
    "# Exibir os resultados\n",
    "for model_name, mse in mse_results.items():\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"  MSE for D: {mse['MSE for D']}\")\n",
    "    print(f\"  MSE for y: {mse['MSE for y']}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para os resíduos de $D$: O Gradient Boosting é o modelo preferido, pois ele obteve o menor MSE para prever a variável de tratamento.\n",
    "\n",
    "Para os resíduos de $y$: O Lasso foi o melhor em prever o resultado.\n",
    "\n",
    "Vou aumentar o número fold para 8 e verificar se os resultados mudam. Esse aumento de 5 para 8 folds é importante para garantir que o modelo seja treinado em uma amostra maior, o que pode melhorar a precisão das previsões. Além de ser uma prática para verificar a robustez dos resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: RandomForest\n",
      "  MSE for D: 0.1520980970340551\n",
      "  MSE for y: 349468.8053384342\n",
      "\n",
      "Model: GradientBoosting\n",
      "  MSE for D: 0.13244114694873113\n",
      "  MSE for y: 308077.9936950858\n",
      "\n",
      "Model: Lasso\n",
      "  MSE for D: 0.14517956178493543\n",
      "  MSE for y: 308414.018017211\n",
      "\n",
      "Model: Ridge\n",
      "  MSE for D: 0.13535137678662412\n",
      "  MSE for y: 308426.02969660814\n",
      "\n",
      "Model: ElasticNet\n",
      "  MSE for D: 0.1437445145464821\n",
      "  MSE for y: 308393.73652545904\n",
      "\n",
      "Model: MLP\n",
      "  MSE for D: 0.14012366416355287\n",
      "  MSE for y: 352474.1787789329\n",
      "\n",
      "Model: SVR\n",
      "  MSE for D: 0.1582865914491929\n",
      "  MSE for y: 330487.8627229285\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Lasso, Ridge, ElasticNet\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Definir os modelos que vamos testar\n",
    "models = {\n",
    "    \"RandomForest\": RandomForestRegressor(n_estimators=100),\n",
    "    \"GradientBoosting\": GradientBoostingRegressor(n_estimators=100),\n",
    "    \"Lasso\": Lasso(alpha=0.1),\n",
    "    \"Ridge\": Ridge(alpha=1.0),\n",
    "    \"ElasticNet\": ElasticNet(alpha=0.1, l1_ratio=0.5),\n",
    "    \"MLP\": MLPRegressor(hidden_layer_sizes=(50,), max_iter=1000),\n",
    "    \"SVR\": SVR()\n",
    "}\n",
    "\n",
    "# Definir as variáveis\n",
    "X = ['casada', 'mage', 'medu', 'fhisp', 'mhisp', 'foreign', 'alcohol', 'deadkids', 'nprenatal', 'mrace', 'frace', 'fage', 'fedu']\n",
    "D = \"D\"\n",
    "y = \"Y\"\n",
    "folds = 8\n",
    "\n",
    "# Dicionário para armazenar os MSE de cada modelo para D e y\n",
    "mse_results = {}\n",
    "\n",
    "# Loop para ajustar os modelos e calcular os MSE para D e y\n",
    "for name, model in models.items():\n",
    "    # Estimativa de D com validação cruzada\n",
    "    D_res = df[D] - cross_val_predict(model, df[X], df[D], cv=folds)\n",
    "    # Estimativa de y com validação cruzada\n",
    "    y_res = df[y] - cross_val_predict(model, df[X], df[y], cv=folds)\n",
    "    \n",
    "    # Calcular MSE para D e y\n",
    "    mse_D = mean_squared_error(df[D], df[D] - D_res)\n",
    "    mse_y = mean_squared_error(df[y], df[y] - y_res)\n",
    "    \n",
    "    # Armazenar os resultados\n",
    "    mse_results[name] = {\"MSE for D\": mse_D, \"MSE for y\": mse_y}\n",
    "\n",
    "# Exibir os resultados\n",
    "for model_name, mse in mse_results.items():\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"  MSE for D: {mse['MSE for D']}\")\n",
    "    print(f\"  MSE for y: {mse['MSE for y']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aumentando para 8 folds, o Gradient Boosting torna-se o melhor modelo para prever tanto o tratamento quanto o resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from econml.dml import LinearDML\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Uncertainty of Mean Point Estimate               \n",
      "================================================================\n",
      "mean_point stderr_mean zstat  pvalue ci_mean_lower ci_mean_upper\n",
      "----------------------------------------------------------------\n",
      "  -185.546      26.526 -6.995    0.0      -237.536      -133.556\n",
      "      Distribution of Point Estimate     \n",
      "=========================================\n",
      "std_point pct_point_lower pct_point_upper\n",
      "-----------------------------------------\n",
      "  138.457        -382.944         206.736\n",
      "     Total Variance of Point Estimate     \n",
      "==========================================\n",
      "stderr_point ci_point_lower ci_point_upper\n",
      "------------------------------------------\n",
      "     140.975       -434.884        236.189\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Definir as variáveis\n",
    "X = df[['casada', 'mage', 'medu', 'fhisp', 'mhisp', 'foreign', 'alcohol', 'deadkids', 'nprenatal', 'mrace', 'frace', 'fage', 'fedu']]\n",
    "D = df['D']\n",
    "y = df['Y']\n",
    "\n",
    "# Converter variáveis categóricas em dummies (se necessário)\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# Definir os modelos de Machine Learning para (i) X em Y, e (ii) X em D\n",
    "model_y = RandomForestRegressor(n_estimators=100, random_state=123)\n",
    "model_d = RandomForestClassifier(n_estimators=100, random_state=123)\n",
    "\n",
    "# Criar o estimador LinearDML\n",
    "estimator = LinearDML(model_y=model_y,\n",
    "                      model_t=model_d,\n",
    "                      discrete_treatment=True,\n",
    "                      random_state=123)\n",
    "\n",
    "# Ajustar o modelo\n",
    "estimator.fit(y, D, X=X)\n",
    "\n",
    "ate_inf = estimator.ate_inference(X=X)\n",
    "print(ate_inf.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arcabouço de Resultados Potenciais, Individual Treatment Effects (ITE) e Conditional Average Treatment Effects (CATE)\n",
    "\n",
    "Podemos definir o efeito do tratamento individual (Individual Treatment Effect - ITE - $\\beta_{i}$) como a diferença entre os resultados potenciais. \n",
    "\n",
    "$$ \n",
    "\\beta_{i}^{ITE} = Y_{i}(1) - Y_{i}(0) \n",
    "$$\n",
    "\n",
    "Segundo o problema fundamental da inferência causal, nunca podemos observar o mesmo indivíduo sob diferentes condições de tratamento. \n",
    "\n",
    "$$\n",
    "Y^{obs}_i(D)= \n",
    "\\begin{cases}\n",
    "Y_i(1), & \\text{se } D=1 \\\\\n",
    "Y_i(0), & \\text{se } D=0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Como estávamos acostumados, pode-se definir o efeito médio do tratamento (Average Treatment Effect - ATE) como\n",
    "\n",
    "$$\n",
    "\\beta^{ATE}= E[Y_i(1) − Y_i(0)] = E [ \\beta_{i}^{ITE} ] = E[\\beta_i]\n",
    "$$\n",
    "\n",
    "e o efeito do tratamento médio condicional (Conditional Average Treatment Effect - CATE) como\n",
    "\n",
    "\n",
    "$$ \\beta^{CATE}(x) = E[Y_i(1) − Y_i(0)|X] = E[\\beta_i|X_{i}=x] $$\n",
    "\n",
    "O CATE é a média do efeito do tratamento para indivíduos com características específicas representadas pelas covariáveis $X$.\n",
    "\n",
    "Repare que **os ITE são inerentemente não observáveis.** Entretanto, o que pode ser estimado, em vez disso, é o **Conditional Average Treatment Effect (CATE)**. Ou seja, o efeito esperado do tratamento individual, condicional em covariáveis $​​X$. \n",
    "\n",
    "Vejamos os pressupostos necessários para identificação do efeito causal.\n",
    "\n",
    "\n",
    "**Hipóteses de Identificação**: são as condições necessárias para garantir que possamos identificar e estimar o CATE usando dados observacionais. Essas hipóteses estabelecem como o efeito causal pode ser extraído, evitando viés e problemas de endogeneidade. \n",
    "\n",
    "* **Não Confundimento** / Inconfundibilidade (Unconfoundedness): Justifica que todas as diferenças entre tratados e não tratados são capturadas pelas covariáveis\n",
    "\n",
    "$$ Y_{i}(0), Y_{i}(1) \\perp T|X $$\n",
    "\n",
    "* **Sobreposição** (Overlap): Garante que temos dados para comparar indivíduos semelhantes em ambos os grupos.\n",
    "  * Para cada valor das covariáveis $X$, deve haver unidades suficientes tanto no grupo tratado quanto no grupo controle. Sem essa suposição, não seria possível comparar grupos semelhantes sob diferentes níveis de tratamento.\n",
    "  \n",
    "$$ 0 < P(D_{i}=1| X_{i}=x) < 1 $$\n",
    "\n",
    "\n",
    "* **Consistência** (Consistency): A suposição de consistência estabelece que o resultado observado ($Y_{i}^{obs}$) para uma unidade corresponde ao resultado potencial associado ao tratamento efetivamente recebido:\n",
    "\n",
    "$$\n",
    "Y^{obs}_i(D)= \n",
    "\\begin{cases}\n",
    "Y_i(1), & \\text{se } D=1 \\\\\n",
    "Y_i(0), & \\text{se } D=0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Essa hipótese conecta os resultados observados aos contrafactuais de forma coerente e supõe que:\n",
    "* Não há múltiplas versões do tratamento ($D$) que possam gerar diferentes resultados potenciais.\n",
    "* O tratamento recebido por uma unidade não afeta os resultados de outras unidades (parte do SUTVA).\n",
    "\n",
    "\n",
    "\n",
    "**E o como conectar o DML com o CATE?**\n",
    "\n",
    "Uma primeira argumentação é que os modelos tradicionais lineares são limitados, principalmente a suposição de linearidade de $Y$ em $X$ (OLS não forneceria um modelo consistente). Os pesquisadores argumentam que a linearidade é uma suposição forte e que os modelos de machine learning são mais flexíveis. Ou seja, seria ótimo se os pesquisadores pudessem substituir o modelo linear por um modelo de machine learning mais flexível. \n",
    "\n",
    "Como argumenta Chernozhukov et al (2018), pelo teorema de Frisch-Waugh-Lovell, pode-se substituir o modelo linear por um modelo de machine learning. Logo, podemos conectar o tratamento (D) como os recursos (X) a um modelo de ML específico (Decision Trees, rede neural ou Gradient Boosting). \n",
    "\n",
    "$$ y_{i} = M(X_{i}, T_{i}) + \\epsilon_{i} $$\n",
    "\n",
    "\n",
    "mas a partir daí, não está claro como podemos obter estimativas do efeito do tratamento, uma vez que este modelo produzirá previsões de $Y$, e não de $\\beta$. Vamos aprender uns conceitos importantes para solucionar essa questão.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimação do CATE com DML\n",
    "\n",
    "Até agora, vimos como o Double/Debiased ML (Double Machine Learning - DML) nos permite focar na estimativa do Efeito Médio do Tratamento (ATE). \n",
    "\n",
    "No entanto, ele também pode ser usado para estimar a **heterogeneidade dos efeitos do tratamento** ou o **Efeito Médio Condicional do Tratamento (CATE)**. Ou seja, em vez de considerar um efeito de tratamento constante para todas as observações, o modelo ajustado permite um efeito diferente com base em suas características específicas. Isto é, enquanto o ATE assume que o tratamento tem o mesmo efeito em todos os indivíduos, o CATE permite que o efeito varie de acordo com as características individuais, capturadas por $X$.\n",
    "\n",
    "Essa ideia é trabalhada no artigo de Chernozhukov et al (2018), que propõe uma abordagem para estimar o CATE usando DML.\n",
    "\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"images\\Chernozhucov.png\"  alt=\"Imagem\" style=\"width: 500px;\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Com base no CATE, podemos definir a seguinte equação:\n",
    "\n",
    "$$ Y = \\theta(X)D + g_{y}(X) + \\epsilon$$\n",
    "\n",
    "* $\\theta(X)$ é o efeito heterogêneo do tratamento (CATE).\n",
    "* $g_{y}(X)$ é uma função de $X$ que captura a relação entre $Y$ e $X$ (ou seja, os fatores não relacionados ao tratamento).\n",
    "* $E(\\epsilon|D,X)=0$, e\n",
    "\n",
    "$$ D = m_{D}(X) + \\eta$$\n",
    "\n",
    "* $m_{D}(X)$ é uma função de $X$ que captura a relação entre $D$ e $X$\n",
    "* $E(\\eta|X)=0$.\n",
    "\n",
    "Isso significa que o efeito do tratamento ($\\theta(X)$) é uma função de $X$, e não um valor constante. Além disso,  tanto o tratamento ($D$) quanto o resultado ($Y$) são funções de $X$.\n",
    "\n",
    "\n",
    "Utilizando o DML, podemos estimar o CATE seguindo os seguintes passos:\n",
    "\n",
    "Com base no procedimento de ortogonalização, os primeiros estágios do DML são:\n",
    "\n",
    "$$ D^{*} = D - m_{0}(X)$$\n",
    "\n",
    "e,\n",
    "\n",
    "$$ Y^{*} = Y - g_{y}(X)$$\n",
    "\n",
    "Posteriormente, teremos o segundo estágio do DML:\n",
    "\n",
    "$$ Y^{*} = \\theta(X)D^{*} + \\epsilon$$\n",
    "\n",
    "\n",
    "No primeiro estágio do DML, ajustamos modelos para $D$ e $Y$ em função de $X$, gerando os resíduos $D^{∗}$ e $Y^{∗}$. Esses resíduos removem a dependência de $X$, permitindo que a segunda etapa estime o efeito causal sem viés. Então, regredindo $Y^{*}$ em $D^{*}$, obtemos o CATE.\n",
    "\n",
    "$$ \\theta = argmin_{\\theta} E[(Y^{*} - \\theta (X) D^{*})^{2}] + \\lambda R(\\theta)$$\n",
    "\n",
    "para algum termo de regularização fortemente convexo $R$, e $\\lambda > 0$ é um parâmetro de regularização. O termo de regularização ajuda a evitar overfitting e é especialmente útil em modelos de alta dimensão, onde $\\theta$ pode ser esparso (muitos coeficientes iguais a zero, em outras palavras, apenas algumas variáveis ou características têm influência significativa, enquanto o restante pode ser ignorado sem perda relevante de informação).\n",
    "\n",
    "* Chernozhukov et al (2016) considera o caso em que $\\theta(X)$ é uma constante (ATE), ou uma função linear de $X$ (CATE) de baixa dimensão.\n",
    "* Nie (2017) cai em um Espaço de Hilbert do Kernel Reprodutor (RKHS).\n",
    "* Chernozhukov et al (2018) consideram o caso de um espaço linear esparso de alta dimensão, onde $\\theta(X) = <\\theta, \\phi(X)>$ para algum mapeamento de características de alta dimensão conhecido e onde $\\theta$ tem muito poucas entradas diferentes de zero (esparsas)\n",
    "* Athey (2019), entre outros resultados, considera o caso em que $\\theta(X)$ é uma função lipschitz não paramétrica e usa modelos de floresta aleatória para ajustar a função. Esse métodos permite maior flexibilidade para modelar $\\theta(X)$ sem assumir linearidade.\n",
    "* Foster (2019) permite modelos arbitrários $\\theta(X)$ e fornecer resultados com base em medidas de complexidade de amostra do espaço do modelo (por exemplo, complexidade de Rademacher, entropia métrica). Esse métodos permite maior flexibilidade para modelar $\\theta(X)$ sem assumir linearidade.\n",
    "\n",
    "A principal vantagem do DML é que se fizermos suposições paramétricas sobre $\\theta(X)$, então se obtém taxas de estimativa rápidas e, para muitos casos de estimadores de estágio final, também normalidade assintótica na estimativa do segundo estágio $\\hat{\\theta}$.\n",
    "\n",
    "O ponto novo na nossa abordagem é que, agora, o $\\theta(X)$ é o efeito heterogêneo do tratamento (CATE), e assumimos que ele segue uma relação linear das covariáveis/recursos (a linearidade assumida para $\\theta(X)$ é uma simplificação útil para interpretação e análise inicial), da seguinte forma:\n",
    "\n",
    "$$ \\theta(X) = X'\\beta + \\theta_{\\text{intercept}}$$\n",
    "\n",
    "* $X'$ é o vetor transposto das covariáveis.\n",
    "* $\\beta$ são os coeficientes que medem o efeito de cada covariável em $\\theta(X)$.\n",
    "* $\\theta_{\\text{intercept}}$ é o intercepto do modelo (cate_intercept)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No exemplo anterior eu havia calculado o efeito médio. Agora, vamos calcular o efeito heterogêneo do tratamento (CATE) para cada característica. Os resultados já estão disponíveis no objeto estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Coefficient Results                       \n",
      "================================================================\n",
      "          point_estimate  stderr zstat  pvalue ci_lower ci_upper\n",
      "----------------------------------------------------------------\n",
      "casada             8.846  57.364  0.154  0.877 -103.584  121.277\n",
      "mage             -11.047   5.421 -2.038  0.042  -21.672   -0.423\n",
      "medu              26.091  10.723  2.433  0.015    5.074   47.107\n",
      "fhisp              21.56 126.313  0.171  0.864 -226.009  269.129\n",
      "mhisp            -164.88 144.534 -1.141  0.254 -448.161  118.402\n",
      "foreign           435.58  132.44  3.289  0.001  176.003  695.158\n",
      "alcohol          -20.385  95.108 -0.214   0.83 -206.793  166.022\n",
      "deadkids          41.641  55.025  0.757  0.449  -66.206  149.489\n",
      "nprenatal         -8.174   7.046  -1.16  0.246  -21.983    5.635\n",
      "mrace           -143.311 103.668 -1.382  0.167 -346.496   59.874\n",
      "frace            -19.611  97.655 -0.201  0.841  -211.01  171.789\n",
      "fage              -0.933   2.945 -0.317  0.751   -6.705    4.838\n",
      "fedu              -1.335   8.278 -0.161  0.872   -17.56    14.89\n",
      "                       CATE Intercept Results                       \n",
      "====================================================================\n",
      "               point_estimate  stderr zstat pvalue ci_lower ci_upper\n",
      "--------------------------------------------------------------------\n",
      "cate_intercept          7.624 161.015 0.047  0.962  -307.96  323.208\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "<sub>A linear parametric conditional average treatment effect (CATE) model was fitted:\n",
      "$Y = \\Theta(X)\\cdot T + g(X, W) + \\epsilon$\n",
      "where for every outcome $i$ and treatment $j$ the CATE $\\Theta_{ij}(X)$ has the form:\n",
      "$\\Theta_{ij}(X) = X' coef_{ij} + cate\\_intercept_{ij}$\n",
      "Coefficient Results table portrays the $coef_{ij}$ parameter vector for each outcome $i$ and treatment $j$. Intercept Results table portrays the $cate\\_intercept_{ij}$ parameter.</sub>\n"
     ]
    }
   ],
   "source": [
    "print(estimator.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretação:**\n",
    "\n",
    "* mage (idade da mãe):\n",
    "  * point_estimate: -11.047 (pvalue: 0.042)\n",
    "  * Isso sugere que, para cada aumento de um ano na idade da mãe, o efeito negativo de fumar durante a gravidez no peso ao nascer diminui em 11.047 gramas (ou seja, o efeito se torna mais negativo).\n",
    "  * Há evidência estatística de que a idade da mãe influencia o efeito de fumar durante a gravidez sobre o peso ao nascer.\n",
    "\n",
    "* medu (educação materna):\n",
    "  * point_estimate: 26.091 (pvalue: 0.015)\n",
    "  * Para cada ano adicional de educação da mãe, o efeito negativo de fumar durante a gravidez no peso ao nascer é reduzido em 26.091 gramas (o efeito negativo é mitigado).\n",
    "  * A educação materna parece reduzir o impacto negativo de fumar durante a gravidez.\n",
    "\n",
    "* foreign (se a mãe é estrangeira):\n",
    "  * point_estimate: 435.58 (pvalue: 0.001 (altamente significativo))\n",
    "  * Interpretation: Mães estrangeiras têm um efeito tratamento condicional que é 435.58 gramas maior do que o de mães não estrangeiras.\n",
    "  * A origem estrangeira da mãe está associada a uma redução significativa do efeito negativo de fumar durante a gravidez.\n",
    "\n",
    "* Outras variáveis: Algumas covariáveis não são estatisticamente significativas (pvalue > 0.05), indicando que não há evidência suficiente para afirmar que essas covariáveis influenciam o efeito do tratamento.\n",
    "\n",
    "* CATE Intercept Results (Resultados do Intercepto do CATE):\n",
    "  * cate_intercept: 7.624\n",
    "  * Este é o valor base do efeito tratamento condicional quando todas as covariáveis estão em zero. Como zero pode não ser um valor interpretável para algumas covariáveis (por exemplo, idade da mãe), o intercepto isoladamente pode não ter uma interpretação prática direta.\n",
    "\n",
    "* OBS: Covariáveis contínuas vs. categóricas: Para variáveis contínuas (como mage), o coeficiente representa a variação no efeito do tratamento por unidade de aumento na covariável. Para variáveis binárias (como foreign), o coeficiente representa a diferença no efeito do tratamento entre os grupos (por exemplo, estrangeiras vs. não estrangeiras).\n",
    "\n",
    "\n",
    "**Resumo:**\n",
    "\n",
    "* O modelo estima que o efeito do tratamento (fumar durante a gravidez) sobre o peso ao nascer não é constante, mas varia linearmente com as covariáveis $X$. O sinal e magnitude dos coeficientes indicam a direção e a intensidade com que cada covariável afeta o efeito do tratamento.\n",
    "* O impacto de fumar durante a gravidez no peso ao nascer não é o mesmo para todas as mães; varia de acordo com características como idade, educação e nacionalidade.\n",
    "* Os resultados sugerem que políticas públicas visando reduzir o tabagismo durante a gravidez podem ser mais eficazes se levarem em consideração essas características. Por exemplo, focar em mães mais jovens ou com menor nível educacional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Como estimar o efeito do tratamento para um indivíduo com uma característica específica?**\n",
    "\n",
    "Para calcular o efeito tratamento condicional ($\\theta(X)$) para um conjunto específico de covariáveis, você pode usar a fórmula:\n",
    "\n",
    "$$ \\hat{\\theta}(X) = X'\\hat{\\beta} + \\hat{\\theta}_{\\text{intercept}}$$\n",
    "\n",
    "Por exemplo, suponha que você queira calcular o efeito para uma mãe com as seguintes características:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   point_estimate   stderr  zstat  pvalue  ci_lower  ci_upper\n",
      "X                                                            \n",
      "0         -253.02  101.255 -2.499   0.012  -451.476   -54.564\n"
     ]
    }
   ],
   "source": [
    "X_new = pd.DataFrame({\n",
    "    'casada': [1],\n",
    "    'mage': [25],\n",
    "    'medu': [12],\n",
    "    'fhisp': [0],\n",
    "    'mhisp': [0],\n",
    "    'foreign': [0],\n",
    "    'alcohol': [1],\n",
    "    'deadkids': [0],\n",
    "    'nprenatal': [10],\n",
    "    'mrace': [1],\n",
    "    'frace': [1],\n",
    "    'fage': [30],\n",
    "    'fedu': [10]\n",
    "})\n",
    "\n",
    "# Certificar-se de que as colunas correspondem às usadas no modelo\n",
    "X_new = pd.get_dummies(X_new, drop_first=True)\n",
    "\n",
    "# Calcular o efeito e obter a inferência\n",
    "effect_inf = estimator.effect_inference(X_new)\n",
    "print(effect_inf.summary_frame())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para o perfil de mãe especificado, fumar durante a gravidez está associado a uma redução média de aproximadamente 253.02 gramas no peso ao nascer do bebê. Logo, há evidência estatística significativa de que fumar durante a gravidez está associado a uma redução substancial no peso ao nascer do bebê."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**E se eu não tiver ideia de como é a heterogeneidade?**\n",
    "\n",
    "Use um caracterizador flexível, por exemplo, um caracterizador polinomial com muitos graus e use `SparseLinearDML`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.955e+05, tolerance: 1.146e+05\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.858e+05, tolerance: 1.146e+05\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.687e+05, tolerance: 1.146e+05\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.601e+06, tolerance: 1.146e+05\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.431e+06, tolerance: 1.146e+05\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.125e+06, tolerance: 1.146e+05\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.056e+06, tolerance: 1.146e+05\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.864e+06, tolerance: 1.146e+05\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.526e+06, tolerance: 1.146e+05\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.324e+05, tolerance: 1.100e+05\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.989e+05, tolerance: 1.167e+05\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.114e+05, tolerance: 1.167e+05\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.837e+05, tolerance: 1.167e+05\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.825e+05, tolerance: 1.167e+05\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.597e+05, tolerance: 1.167e+05\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.620e+05, tolerance: 1.167e+05\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.281e+05, tolerance: 1.167e+05\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.791e+05, tolerance: 1.167e+05\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.035e+05, tolerance: 1.147e+05\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.608e+05, tolerance: 1.147e+05\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.562e+05, tolerance: 1.147e+05\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.855e+05, tolerance: 1.147e+05\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.847e+05, tolerance: 1.147e+05\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.712e+05, tolerance: 1.147e+05\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.484e+05, tolerance: 1.147e+05\n",
      "Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.318e+05, tolerance: 1.147e+05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PolynomialFeatures\n\u001b[0;32m      4\u001b[0m est \u001b[38;5;241m=\u001b[39m SparseLinearDML(featurizer\u001b[38;5;241m=\u001b[39mPolynomialFeatures(degree\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, include_bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[1;32m----> 5\u001b[0m \u001b[43mest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m lb, ub \u001b[38;5;241m=\u001b[39m est\u001b[38;5;241m.\u001b[39mconst_marginal_effect_interval(X, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m.05\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\econml\\dml\\dml.py:1098\u001b[0m, in \u001b[0;36mSparseLinearDML.fit\u001b[1;34m(self, Y, T, X, W, sample_weight, groups, cache_values, inference)\u001b[0m\n\u001b[0;32m   1093\u001b[0m \u001b[38;5;66;03m# TODO: support freq_weight and sample_var in debiased lasso\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m check_high_dimensional(X, T, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, featurizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeaturizer,\n\u001b[0;32m   1095\u001b[0m                        discrete_treatment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscrete_treatment,\n\u001b[0;32m   1096\u001b[0m                        msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe number of features in the final model (< 5) is too small for a sparse model. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1097\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe recommend using the LinearDML estimator for this low-dimensional setting.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1098\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1099\u001b[0m \u001b[43m                   \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1100\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mcache_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minference\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\econml\\dml\\dml.py:589\u001b[0m, in \u001b[0;36mDML.fit\u001b[1;34m(self, Y, T, X, W, sample_weight, freq_weight, sample_var, groups, cache_values, inference)\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, Y, T, \u001b[38;5;241m*\u001b[39m, X\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, W\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, freq_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_var\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, groups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    551\u001b[0m         cache_values\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, inference\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    552\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;124;03m    Estimate the counterfactual model from data, i.e. estimates functions τ(·,·,·), ∂τ(·,·).\u001b[39;00m\n\u001b[0;32m    554\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    587\u001b[0m \u001b[38;5;124;03m    self\u001b[39;00m\n\u001b[0;32m    588\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 589\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreq_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreq_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    590\u001b[0m \u001b[43m                       \u001b[49m\u001b[43msample_var\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mcache_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    592\u001b[0m \u001b[43m                       \u001b[49m\u001b[43minference\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\econml\\dml\\_rlearner.py:415\u001b[0m, in \u001b[0;36m_RLearner.fit\u001b[1;34m(self, Y, T, X, W, sample_weight, freq_weight, sample_var, groups, cache_values, inference)\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;124;03mEstimate the counterfactual model from data, i.e. estimates function :math:`\\\\theta(\\\\cdot)`.\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    412\u001b[0m \u001b[38;5;124;03mself: _RLearner instance\u001b[39;00m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;66;03m# Replacing fit from _OrthoLearner, to enforce Z=None and improve the docstring\u001b[39;00m\n\u001b[1;32m--> 415\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m                   \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreq_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreq_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_var\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mcache_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    418\u001b[0m \u001b[43m                   \u001b[49m\u001b[43minference\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\econml\\_cate_estimator.py:131\u001b[0m, in \u001b[0;36mBaseCateEstimator._wrap_fit.<locals>.call\u001b[1;34m(self, Y, T, inference, *args, **kwargs)\u001b[0m\n\u001b[0;32m    129\u001b[0m     inference\u001b[38;5;241m.\u001b[39mprefit(\u001b[38;5;28mself\u001b[39m, Y, T, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    130\u001b[0m \u001b[38;5;66;03m# call the wrapped fit method\u001b[39;00m\n\u001b[1;32m--> 131\u001b[0m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_postfit(Y, T, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inference \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;66;03m# NOTE: we call inference fit *after* calling the main fit method\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\econml\\_ortho_learner.py:877\u001b[0m, in \u001b[0;36m_OrthoLearner.fit\u001b[1;34m(self, Y, T, X, W, Z, sample_weight, freq_weight, sample_var, groups, cache_values, inference, only_final, check_input)\u001b[0m\n\u001b[0;32m    874\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# treatment featurizer case\u001b[39;00m\n\u001b[0;32m    875\u001b[0m         final_T \u001b[38;5;241m=\u001b[39m output_T\n\u001b[1;32m--> 877\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_final\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m                \u001b[49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinal_T\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m                \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mZ\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mZ\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m                \u001b[49m\u001b[43mnuisances\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnuisances\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[43m                \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    882\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfreq_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreq_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    883\u001b[0m \u001b[43m                \u001b[49m\u001b[43msample_var\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_var\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    884\u001b[0m \u001b[43m                \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    886\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\econml\\_ortho_learner.py:980\u001b[0m, in \u001b[0;36m_OrthoLearner._fit_final\u001b[1;34m(self, Y, T, X, W, Z, nuisances, sample_weight, freq_weight, sample_var, groups)\u001b[0m\n\u001b[0;32m    978\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit_final\u001b[39m(\u001b[38;5;28mself\u001b[39m, Y, T, X\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, W\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, Z\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, nuisances\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    979\u001b[0m                freq_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_var\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, groups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 980\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ortho_learner_model_final\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfilter_none_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mZ\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mZ\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    981\u001b[0m \u001b[43m                                                                   \u001b[49m\u001b[43mnuisances\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnuisances\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    982\u001b[0m \u001b[43m                                                                   \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[43m                                                                   \u001b[49m\u001b[43mfreq_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreq_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    984\u001b[0m \u001b[43m                                                                   \u001b[49m\u001b[43msample_var\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_var\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    985\u001b[0m \u001b[43m                                                                   \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    986\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    987\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ortho_learner_model_final, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\econml\\dml\\_rlearner.py:94\u001b[0m, in \u001b[0;36m_ModelFinal.fit\u001b[1;34m(self, Y, T, X, W, Z, nuisances, sample_weight, freq_weight, sample_var, groups)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, Y, T, X\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, W\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, Z\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, nuisances\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     92\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, freq_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_var\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, groups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     93\u001b[0m     Y_res, T_res \u001b[38;5;241m=\u001b[39m nuisances\n\u001b[1;32m---> 94\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model_final\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT_res\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_res\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mfreq_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreq_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_var\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_var\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\econml\\dml\\dml.py:163\u001b[0m, in \u001b[0;36m_FinalWrapper.fit\u001b[1;34m(self, X, T, T_res, Y_res, sample_weight, freq_weight, sample_var, groups)\u001b[0m\n\u001b[0;32m    160\u001b[0m fts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine(X, T_res)\n\u001b[0;32m    161\u001b[0m filtered_kwargs \u001b[38;5;241m=\u001b[39m filter_none_kwargs(sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m    162\u001b[0m                                      freq_weight\u001b[38;5;241m=\u001b[39mfreq_weight, sample_var\u001b[38;5;241m=\u001b[39msample_var)\n\u001b[1;32m--> 163\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_res\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfiltered_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_intercept \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    165\u001b[0m intercept \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mpredict(np\u001b[38;5;241m.\u001b[39mzeros_like(fts[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m1\u001b[39m]))\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\econml\\sklearn_extensions\\linear_model.py:1060\u001b[0m, in \u001b[0;36mMultiOutputDebiasedLasso.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1058\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflat_target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1059\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(y)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m-> 1060\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;66;03m# Set coef_ attribute\u001b[39;00m\n\u001b[0;32m   1062\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_attribute(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoef_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\multioutput.py:278\u001b[0m, in \u001b[0;36m_MultiOutputEstimator.fit\u001b[1;34m(self, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[0;32m    275\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    276\u001b[0m         routed_params\u001b[38;5;241m.\u001b[39mestimator\u001b[38;5;241m.\u001b[39mfit[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m sample_weight\n\u001b[1;32m--> 278\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_ \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_features_in_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mn_features_in_\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\multioutput.py:67\u001b[0m, in \u001b[0;36m_fit_estimator\u001b[1;34m(estimator, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[0;32m     65\u001b[0m     estimator\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 67\u001b[0m     \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m estimator\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\econml\\sklearn_extensions\\linear_model.py:727\u001b[0m, in \u001b[0;36mDebiasedLasso.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselected_alpha_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    726\u001b[0m     \u001b[38;5;66;03m# Select optimal penalty\u001b[39;00m\n\u001b[1;32m--> 727\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_optimal_alpha\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    728\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselected_alpha_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha\n\u001b[0;32m    729\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    730\u001b[0m     \u001b[38;5;66;03m# Warn about consistency\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\econml\\sklearn_extensions\\linear_model.py:893\u001b[0m, in \u001b[0;36mDebiasedLasso._get_optimal_alpha\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    885\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_optimal_alpha\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight):\n\u001b[0;32m    886\u001b[0m     \u001b[38;5;66;03m# To be done once per target. Assumes y can be flattened.\u001b[39;00m\n\u001b[0;32m    887\u001b[0m     cv_estimator \u001b[38;5;241m=\u001b[39m WeightedLassoCV(cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, n_alphas\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_alphas, fit_intercept\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_intercept,\n\u001b[0;32m    888\u001b[0m                                    precompute\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecompute, copy_X\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    889\u001b[0m                                    max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter, tol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtol,\n\u001b[0;32m    890\u001b[0m                                    random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state,\n\u001b[0;32m    891\u001b[0m                                    selection\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselection,\n\u001b[0;32m    892\u001b[0m                                    n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n\u001b[1;32m--> 893\u001b[0m     \u001b[43mcv_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    894\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cv_estimator\u001b[38;5;241m.\u001b[39malpha_\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\econml\\sklearn_extensions\\linear_model.py:445\u001b[0m, in \u001b[0;36mWeightedLassoCV.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv \u001b[38;5;241m=\u001b[39m _weighted_check_cv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state)\u001b[38;5;241m.\u001b[39msplit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight)\n\u001b[0;32m    444\u001b[0m \u001b[38;5;66;03m# Fit weighted model\u001b[39;00m\n\u001b[1;32m--> 445\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_weighted_linear_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv \u001b[38;5;241m=\u001b[39m cv_temp\n\u001b[0;32m    447\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\econml\\sklearn_extensions\\linear_model.py:135\u001b[0m, in \u001b[0;36mWeightedModelMixin._fit_weighted_linear_model\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    132\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;66;03m# Fit lasso without weights\u001b[39;00m\n\u001b[1;32m--> 135\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:2128\u001b[0m, in \u001b[0;36mLassoCV.fit\u001b[1;34m(self, X, y, sample_weight, **params)\u001b[0m\n\u001b[0;32m   2090\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[0;32m   2091\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit Lasso model with coordinate descent.\u001b[39;00m\n\u001b[0;32m   2092\u001b[0m \n\u001b[0;32m   2093\u001b[0m \u001b[38;5;124;03m    Fit is on grid of alphas and best alpha estimated by cross-validation.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2126\u001b[0m \u001b[38;5;124;03m        Returns an instance of fitted model.\u001b[39;00m\n\u001b[0;32m   2127\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1784\u001b[0m, in \u001b[0;36mLinearModelCV.fit\u001b[1;34m(self, X, y, sample_weight, **params)\u001b[0m\n\u001b[0;32m   1764\u001b[0m \u001b[38;5;66;03m# We do a double for loop folded in one, in order to be able to\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# iterate in parallel on l1_ratio and folds\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m jobs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1767\u001b[0m     delayed(_path_residuals)(\n\u001b[0;32m   1768\u001b[0m         X,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1782\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m folds\n\u001b[0;32m   1783\u001b[0m )\n\u001b[1;32m-> 1784\u001b[0m mse_paths \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1788\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1789\u001b[0m mse_paths \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(mse_paths, (n_l1_ratio, \u001b[38;5;28mlen\u001b[39m(folds), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m   1790\u001b[0m \u001b[38;5;66;03m# The mean is computed over folds.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:1464\u001b[0m, in \u001b[0;36m_path_residuals\u001b[1;34m(X, y, sample_weight, train, test, fit_intercept, path, path_params, alphas, l1_ratio, X_order, dtype)\u001b[0m\n\u001b[0;32m   1461\u001b[0m \u001b[38;5;66;03m# Do the ordering and type casting here, as if it is done in the path,\u001b[39;00m\n\u001b[0;32m   1462\u001b[0m \u001b[38;5;66;03m# X is copied and a reference is kept here\u001b[39;00m\n\u001b[0;32m   1463\u001b[0m X_train \u001b[38;5;241m=\u001b[39m check_array(X_train, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype, order\u001b[38;5;241m=\u001b[39mX_order)\n\u001b[1;32m-> 1464\u001b[0m alphas, coefs, _ \u001b[38;5;241m=\u001b[39m \u001b[43mpath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpath_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1465\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m X_train, y_train\n\u001b[0;32m   1467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1468\u001b[0m     \u001b[38;5;66;03m# Doing this so that it becomes coherent with multioutput.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:186\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    184\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    188\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:361\u001b[0m, in \u001b[0;36mlasso_path\u001b[1;34m(X, y, eps, n_alphas, alphas, precompute, Xy, copy_X, coef_init, verbose, return_n_iter, positive, **params)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[0;32m    189\u001b[0m     {\n\u001b[0;32m    190\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    220\u001b[0m ):\n\u001b[0;32m    221\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute Lasso path with coordinate descent.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \n\u001b[0;32m    223\u001b[0m \u001b[38;5;124;03m    The Lasso optimization function varies for mono and multi-outputs.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;124;03m     [0.2159048  0.4425765  0.23668876]]\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43menet_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    362\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    363\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    364\u001b[0m \u001b[43m        \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    365\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_alphas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_alphas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    367\u001b[0m \u001b[43m        \u001b[49m\u001b[43malphas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malphas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    368\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprecompute\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecompute\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[43m        \u001b[49m\u001b[43mXy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mXy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy_X\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy_X\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoef_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoef_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpositive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpositive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    374\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_n_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_n_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    375\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:186\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    184\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    188\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:697\u001b[0m, in \u001b[0;36menet_path\u001b[1;34m(X, y, l1_ratio, eps, n_alphas, alphas, precompute, Xy, copy_X, coef_init, verbose, return_n_iter, positive, check_input, **params)\u001b[0m\n\u001b[0;32m    683\u001b[0m     model \u001b[38;5;241m=\u001b[39m cd_fast\u001b[38;5;241m.\u001b[39menet_coordinate_descent_gram(\n\u001b[0;32m    684\u001b[0m         coef_,\n\u001b[0;32m    685\u001b[0m         l1_reg,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    694\u001b[0m         positive,\n\u001b[0;32m    695\u001b[0m     )\n\u001b[0;32m    696\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m precompute \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m--> 697\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mcd_fast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menet_coordinate_descent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoef_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml1_reg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml2_reg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositive\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    701\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    702\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecompute should be one of True, False, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or array-like. Got \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    703\u001b[0m         \u001b[38;5;241m%\u001b[39m precompute\n\u001b[0;32m    704\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from econml.dml import SparseLinearDML\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "est = SparseLinearDML(featurizer=PolynomialFeatures(degree=2, include_bias=False))\n",
    "est.fit(y, D, X=X)\n",
    "lb, ub = est.const_marginal_effect_interval(X, alpha=.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classes do DML**\n",
    "\n",
    "**Modelos Lineares**\n",
    "\n",
    "* LinearDML: usa um **modelo linear** final **não regularizado** e funciona essencialmente apenas quando o vetor de características $X$ é de baixa dimensão. Oferece intervalos de confiança por meio de argumentos de normalidade assintótica. Também é possível construir intervalos de confiança baseados em bootstrap definindo inference='bootstrap' (Chernozhukov, 2016).\n",
    "* SparseLinearDML: é uma extensão do LinearDML que usa **regularização L1** para lidar com a alta dimensionalidade de $X$. Usa uma implementação do algoritmo DebiasedLasso, propriedades de normalidade assintótica do DebiasedLasso, esta classe também oferece intervalos de confiança assintoticamente normais (Chernozhukov, 2017; Chernozhukov, 2018). \n",
    "* KernelDML: é uma classe que usa **kernel ridge regression** para estimar o efeito do tratamento (RKHS - Nie, 2017). Ela aproxima qualquer função no RKHS criando recursos aleatórios de Fourier. Em seguida, executa um modelo final regularizado ElasticNet. Além disso, dado que usamos Recursos aleatórios de Fourier, esta classe assume um kernel RBF.\n",
    "\n",
    "\n",
    "**Modelos Não-Lineares**\n",
    "\n",
    "* NonParamDML: não faz nenhuma suposição sobre o modelo de efeito para cada resultado $i$. No entanto, ele se aplica somente quando o tratamento é binário ou unidimensional contínuo.\n",
    "* CausalForestDML: usa uma Floresta Causal como um modelo final (Wager, 2018; Athey, 2019). Este estimador oferece intervalos de confiança via *Bootstrap-of-Little-Bags* conforme descrito em (Athey, 2019) . Usando esta funcionalidade, também podemos construir intervalos de confiança para o CATE.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modelos mais flexiveis (Não-Lineares)**\n",
    "\n",
    "Se a heterogeneidade do efeito não tiver uma forma linear, então essa abordagem anterior não é válida. Pode-se então querer criar caracterizações mais complexas, em cujo caso o problema pode se tornar com dimensões muito altas para para OLS. \n",
    "* O *SparseLinearDML* pode lidar com essas configurações por meio do uso do Lasso desviado. \n",
    "* O *CausalForestDML* não precisa de caracterização explícita e aprende os modelos CATE não lineares baseados em floresta, automaticamente. \n",
    "\n",
    "Como há não lineariedade, não conseguimos interpretar os coeficientes diretamente. Conseguimos o ATE, e apenas podemos interpretar o efeito do tratamento para um indivíduo com uma característica específica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Uncertainty of Mean Point Estimate               \n",
      "================================================================\n",
      "mean_point stderr_mean zstat  pvalue ci_mean_lower ci_mean_upper\n",
      "----------------------------------------------------------------\n",
      "  -197.671     788.447 -0.251  0.802     -1742.998      1347.655\n",
      "      Distribution of Point Estimate     \n",
      "=========================================\n",
      "std_point pct_point_lower pct_point_upper\n",
      "-----------------------------------------\n",
      "   430.33        -798.482         367.699\n",
      "     Total Variance of Point Estimate     \n",
      "==========================================\n",
      "stderr_point ci_point_lower ci_point_upper\n",
      "------------------------------------------\n",
      "     898.238      -1312.975        967.929\n",
      "------------------------------------------\n",
      "\n",
      "Note: The stderr_mean is a conservative upper bound.\n"
     ]
    }
   ],
   "source": [
    "from econml.dml import NonParamDML\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor\n",
    "\n",
    "# Create the NonParamDML estimator\n",
    "estimator_NonParam = NonParamDML(\n",
    "    model_y=RandomForestRegressor(n_estimators=100, random_state=123),\n",
    "    model_t=RandomForestClassifier(n_estimators=100, random_state=123),\n",
    "    model_final=GradientBoostingRegressor(n_estimators=100, random_state=123),\n",
    "    discrete_treatment=True,\n",
    "    random_state=123\n",
    ")\n",
    "\n",
    "# Ajustar o modelo com inferência por bootstrap\n",
    "estimator_NonParam.fit(y, D, X=X, inference='bootstrap')\n",
    "\n",
    "# Obter a inferência do ATE\n",
    "ate_inf = estimator_NonParam.ate_inference(X=X)\n",
    "\n",
    "# Exibir o resumo da inferência\n",
    "print(ate_inf.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   point_estimate   stderr  zstat  pvalue  ci_lower  ci_upper\n",
      "X                                                            \n",
      "0        -284.299  150.622 -1.887    0.00  -661.645   -74.625\n",
      "1         177.621  276.689  0.642    0.11  -360.993   665.202\n",
      "2           9.170  252.484  0.036    0.39  -419.417   461.940\n",
      "3        -227.112  100.160 -2.267    0.02  -398.421   -29.809\n",
      "4        -428.132  291.145 -1.471    0.08  -991.964   156.033\n"
     ]
    }
   ],
   "source": [
    "# Obter as estimativas pontuais do CATE em X\n",
    "cates = estimator_NonParam.effect(X)\n",
    "cate_inf = estimator_NonParam.effect_inference(X)\n",
    "# Obter o resumo das estimativas do CATE\n",
    "print(cate_inf.summary_frame().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No caso do NonParamDML, não é possível obter uma tabela semelhante com coeficientes para cada covariável. Isso se deve à natureza não paramétrica do estimador, que não assume uma forma funcional específica para a relação entre as covariáveis e o efeito do tratamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   point_estimate   stderr  zstat  pvalue  ci_lower  ci_upper\n",
      "X                                                            \n",
      "0        -194.172  156.984 -1.237    0.11  -506.135     116.7\n"
     ]
    }
   ],
   "source": [
    "# Definir as características específicas\n",
    "X_new = pd.DataFrame({\n",
    "    'casada': [1],\n",
    "    'mage': [25],\n",
    "    'medu': [12],\n",
    "    'fhisp': [0],\n",
    "    'mhisp': [0],\n",
    "    'foreign': [0],\n",
    "    'alcohol': [1],\n",
    "    'deadkids': [0],\n",
    "    'nprenatal': [10],\n",
    "    'mrace': [1],\n",
    "    'frace': [1],\n",
    "    'fage': [30],\n",
    "    'fedu': [10]\n",
    "})\n",
    "\n",
    "# Converter variáveis categóricas em dummies, se aplicável\n",
    "X_new = pd.get_dummies(X_new, drop_first=True)\n",
    "\n",
    "# Calcular a estimativa pontual do efeito\n",
    "cate_new = estimator_NonParam.effect(X_new)\n",
    "\n",
    "# Obter a inferência estatística\n",
    "effect_inf = estimator_NonParam.effect_inference(X_new)\n",
    "\n",
    "# Exibir o resumo da inferência\n",
    "print(effect_inf.summary_frame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Uncertainty of Mean Point Estimate               \n",
      "================================================================\n",
      "mean_point stderr_mean zstat  pvalue ci_mean_lower ci_mean_upper\n",
      "----------------------------------------------------------------\n",
      "  -214.954     186.112 -1.155  0.248      -579.728       149.819\n",
      "      Distribution of Point Estimate     \n",
      "=========================================\n",
      "std_point pct_point_lower pct_point_upper\n",
      "-----------------------------------------\n",
      "  143.065         -494.49          58.236\n",
      "     Total Variance of Point Estimate     \n",
      "==========================================\n",
      "stderr_point ci_point_lower ci_point_upper\n",
      "------------------------------------------\n",
      "     234.746       -702.526        231.413\n",
      "------------------------------------------\n",
      "\n",
      "Note: The stderr_mean is a conservative upper bound.\n"
     ]
    }
   ],
   "source": [
    "from econml.dml import CausalForestDML\n",
    "\n",
    "# Criar o estimador ForestDML\n",
    "estimator_Forest = CausalForestDML(\n",
    "    model_y=RandomForestRegressor(n_estimators=100, random_state=123),\n",
    "    model_t=RandomForestClassifier(n_estimators=100, random_state=123),\n",
    "    discrete_treatment=True,\n",
    "    random_state=123\n",
    ")\n",
    "\n",
    "# Ajustar o modelo\n",
    "estimator_Forest.fit(y, D, X=X)\n",
    "\n",
    "# Obter a inferência do ATE\n",
    "ate_inf_forest = estimator_Forest.ate_inference(X=X)\n",
    "\n",
    "# Exibir o resumo da inferência\n",
    "print(ate_inf_forest.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   point_estimate  stderr  zstat  pvalue  ci_lower  ci_upper\n",
      "X                                                           \n",
      "0        -192.011  94.156 -2.039   0.041  -376.554    -7.468\n"
     ]
    }
   ],
   "source": [
    "# Definir as características específicas\n",
    "X_new = pd.DataFrame({\n",
    "    'casada': [1],\n",
    "    'mage': [25],\n",
    "    'medu': [12],\n",
    "    'fhisp': [0],\n",
    "    'mhisp': [0],\n",
    "    'foreign': [0],\n",
    "    'alcohol': [1],\n",
    "    'deadkids': [0],\n",
    "    'nprenatal': [10],\n",
    "    'mrace': [1],\n",
    "    'frace': [1],\n",
    "    'fage': [30],\n",
    "    'fedu': [10]\n",
    "})\n",
    "\n",
    "# Converter variáveis categóricas em dummies, se aplicável\n",
    "X_new = pd.get_dummies(X_new, drop_first=True)\n",
    "\n",
    "# Calcular a estimativa pontual do efeito\n",
    "cate_new = estimator_Forest.effect(X_new)\n",
    "\n",
    "# Obter a inferência estatística\n",
    "effect_inf = estimator_Forest.effect_inference(X_new)\n",
    "\n",
    "# Exibir o resumo da inferência\n",
    "print(effect_inf.summary_frame())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Como podemos avaliar o desempenho do modelo CATE?**\n",
    "\n",
    "Cada uma das classes DML tem um atributo score_ depois de serem ajustadas. Então, é possível acessar esse atributo e comparar o desempenho em diferentes parâmetros de modelagem (quanto menor a pontuação, melhor):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Considerações Finais\n",
    "\n",
    "Vejamos um resumo do que foi visto.\n",
    "\n",
    "* O principal objetivo do DML é ajustar e remover a variável de confusão de forma que a variável de interesse (tratamento) e o resultado fiquem \"ortogonais\" ou \"independentes\".\n",
    "* DML combina métodos de aprendizado de máquina com técnicas econométricas para estimar efeitos causais.\n",
    "* A técnica geralmente envolve a aplicação de aprendizado de máquina para prever tanto o tratamento quanto o resultado usando as variáveis observáveis de confusão, e então os resíduos dessas previsões são utilizados em um segundo estágio para estimar o efeito causal.\n",
    "  * Primeiro Estágio: Aplicar modelos de aprendizado de máquina para prever a variável de tratamento e o resultado.\n",
    "  * Segundo Estágio: Utilizar os resíduos dessas previsões em um modelo de regressão para estimar o efeito causal.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
