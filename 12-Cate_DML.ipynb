{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CATE & Double Machine Learning\n",
    "\n",
    "Prof. Daniel de Abreu Pereira Uhr\n",
    "\n",
    "## Conteúdo\n",
    "\n",
    "* CATEs - Conditional Average Treatment Effects (Efeitos Heterogêneos do Tratamento)\n",
    "* Efeitos Heterogêneos do Tratamento e o Arcabouço de Resultados Potenciais\n",
    "* Meta-learners\n",
    "* Ortogonalização\n",
    "  * Aplicação do Procedimento de Ortogonalização no Python\n",
    "* Ortogonalização e Machine Learning  (Orthogonal/Double Machine Learning - DML)\n",
    "\n",
    "## Referências\n",
    "\n",
    "**Principais:**\n",
    "* UBER CausalML: https://causalml.readthedocs.io/en/latest/\n",
    "* Microsoft EconML: https://econml.azurewebsites.net/\n",
    "* https://github.com/DoubleML/doubleml-for-py\n",
    "* https://docs.doubleml.org/stable/index.html\n",
    "* https://github.com/MasaAsami/ReproducingDMLDiD/blob/main/notebook/Reproduction_of_DMLDiD_RO_for_NEW_SIMDATA.ipynb\n",
    "* https://econml.azurewebsites.net/\n",
    "* Schmidheiny, K., & Siegloch, S. (2023). On event studies and distributed-lags in two-way fixed effects models: Identification, equivalence, and generalization. Journal of Applied Econometrics, 1- 19. https://doi.org/10.1002/jae.2971\n",
    "* Stevenson, Betsey, Wolfers, Justin, 2006. Bargaining in the shadow of the law: Divorce laws and family distress. Q. J. Econ. 121 (1), 267–288.\n",
    "* Goodman-Bacon, A. (2021). Difference-in-differences with variation in treatment timing. Journal of Econometrics. https://doi.org/10.1016/j.jeconom.2021.03.014\n",
    "* Callaway, B. and Sant'Anna, P. H. C. (2021). Difference-in-Differences with multiple time periods. Journal of Econometrics. https://doi.org/10.1016/j.jeconom.2020.12.001\n",
    "\n",
    "**Complementares:**\n",
    "\n",
    "* Borusyak, K.; Jaravel, X. and Spiess, J. (2023). Revisiting Event Study Designs: Robust and Efficient Estimation. arXiv: https://arxiv.org/pdf/2108.12419.pdf\n",
    "* Clément deChaisemartin, Xavier d’Haultfoeuille. (2022) Difference-in-Differences Estimators of Intertemporal Treatment Effects. hal-03873903\n",
    "* Roth et al. (2022) What’s Trending in Difference-in-Differences? A Synthesis of the Recent Econometrics Literature. https://www.jonathandroth.com/assets/files/DiD_Review_Paper.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CATEs - Conditional Average Treatment Effects (Efeitos Heterogêneos do Tratamento)\n",
    "\n",
    "A nossa função de expectativa condicional é dada por: \n",
    "\n",
    "$$ E[Y|X, T]$$\n",
    "\n",
    "Suponha que queremos realizar inferência causal, entre $T$ e $Y$, sob um contexto $X$, e otimizá-la. Ou seja, \n",
    "\n",
    "$$ \\text{argmax}_{T} E[Y|X, T]$$\n",
    "\n",
    "Ou seja, agora queremos mais do que apenas o efeito médio do tratamento (ATE). Sabemos que o tratamento tem impacto positivo em algumas pessoas, mas não em outras. Os recursos $X$ (covariáveis) desempenham um papel na definição de diferentes perfis de unidades (indivíduos), e cada perfil (indivíduo) **pode responder de forma diferente ao tratamento**. Sendo assim, **agora queremos personalizar o tratamento, dando-o apenas às unidades que melhor respondem a ele** (a ideia é \"direcionar o tratamento\"). Por exemplo, se um medicamento tem efeitos colaterais graves para crianças, podemos querer restringir sua distribuição apenas para adultos. Ou se uma campanha publicitária é eficaz apenas em países de língua inglesa, não vale a pena mostrá-la em outro lugar.\n",
    "\n",
    "**CATE**: \n",
    "\n",
    "O **Conditional Average Treatment Effect** (CATE) é a média do efeito do tratamento condicional a um conjunto de características. Para o caso de tratamento binário, temos: \n",
    "\n",
    "$$ E[Y_{1}-Y_{0}|X]$$\n",
    "\n",
    "ou, para o caso de tratamento contínuo:\n",
    "\n",
    "$$ E[y´(t)|X]$$\n",
    "\n",
    "O condicionamento em $X$ significa que agora permitimos que o efeito do tratamento seja diferente dependendo das características de cada unidade (indivíduo). Queremos tratar apenas as unidades certas (no caso binário) ou descobrir qual é a dosagem de tratamento ideal para cada unidade (no caso contínuo).\n",
    "\n",
    "Considere o exemplo gráfico:\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"images\\elast-partition.png\"  alt=\"Imagem\" style=\"width: 400px;\"/>\n",
    "</div>\n",
    "\n",
    "Desagregando os dados, podemos ver que o efeito do tratamento é diferente para diferentes os 3 grupos de pessoas. \n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"images\\elast-split.png\"  alt=\"Imagem\" style=\"width: 800px;\"/>\n",
    "</div>\n",
    "\n",
    "Assim, a ideia é buscar a indetificação do efeito heterogêneo do tratamento, para que possamos personalizar o tratamento para cada unidade. Vamos tentar entender esse mesmo gráfico de outra maneira. Suponha que queremos encontrar os dias em que a $ \\frac{d \\text{Vendas}}{d \\text{Preço}} $ é menor. Isso significa que, para esses dias, a elasticidade-preço é menor. Logo, do ponto de vista do vendedor, poderíamos aumentar o preço nesses dias sem perder muitas vendas (será que isso ocorre no mercado de aluguéis de imóveis durante às férias?).\n",
    "\n",
    "A elasticidade (sensibilidade do indivíduo) é não observável. Podemos pensar cada unidade como tendo um valor $Y_{i}$, com uma elasticidade (sensibilidade) individual ($ \\frac{d Y_{i}}{d T} $). \n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"images\\elasticity.png\"  alt=\"Imagem\" style=\"width: 400px;\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "Para vermos as inclinações individuais, teríamos que observar cada dia sob dois preços diferentes e calcular como as vendas mudam para cada um desses preços. \n",
    "\n",
    "$$ \\frac{dY_{i}}{dT} = \\frac{Y_{i}(T_{i}) - Y_{i}(T_{i} + \\epsilon)}{T_{i}-(T_{i} + \\epsilon)}$$\n",
    "\n",
    "Este é o problema fundamental da inferência causal novamente. Nunca podemos ver a mesma unidade sob diferentes condições de tratamento. Então, o que podemos fazer?\n",
    "\n",
    "\n",
    "Um possível ajuste aos dados é utilizar o OLS.\n",
    "\n",
    "$$ y_{i} = \\alpha + \\beta T_{i} + \\gamma X_{i} + \\epsilon_{i} $$\n",
    "\n",
    "Diferenciando o tratamento,\n",
    "\n",
    "$$ \\frac{dY_{i}}{dT} = \\beta$$\n",
    "\n",
    "Nesse caso, é um modelo simples, e um valor constante de beta para todos os indivíduos. Esse é o ATE. No entento, se fizermos a seguinte mudança simples:\n",
    "\n",
    "$$ y_{i} = \\alpha + \\beta T_{i} + \\gamma X_{i} + \\delta X_{i}T_{i} + \\epsilon_{i} $$\n",
    "\n",
    "Teremos uma sensibilidade diferente para cada indivíduo:\n",
    "\n",
    "$$ \\frac{dY_{i}}{dT} = \\beta + \\delta X_{i}$$\n",
    "\n",
    "Onde $\\delta$ é um coeficiente que depende das características de cada indivíduo ($X_{i}$). Em outras palavras, a previsão de sensibilidade mudará conforme $X$. Com essas previsões de sensibilidade, podemos agrupar as unidades por quanto achamos que elas responderão ao tratamento.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efeitos Heterogêneos do Tratamento e o Arcabouço de Resultados Potenciais\n",
    "\n",
    "Podemos definir o efeito do tratamento individual (Individual Treatment Effect - ITE - $\\beta_{i}$) como a diferença entre os resultados potenciais. \n",
    "\n",
    "$$ \\beta_{i}^{ITE} = Y_{i}(1) - Y_{i}(0)$$\n",
    "\n",
    "ou, no caso do tratamento contínuo, $\\beta_{i}^{ITE} = dY_{i}/dt$, onde t é a variável de tratamento. \n",
    "\n",
    "Segundo o problema fundamental da inferência causal, nunca podemos observar o mesmo indivíduo sob diferentes condições de tratamento. \n",
    "\n",
    "$$\n",
    "Y^{obs}_i(t)= \n",
    "\\begin{cases}\n",
    "Y_i(1), & \\text{se } t=1\\\\\n",
    "Y_i(0), & \\text{se } t=0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "Podemos definir o efeito médio do tratamento (Average Treatment Effect - ATE) como\n",
    "\n",
    "\n",
    "$$\n",
    "\\beta^{ATE}= E[Y_i(1) − Y_i(0)] = E[\\beta_i]\n",
    "$$\n",
    "\n",
    "e o efeito do tratamento médio condicional (Conditional Average Treatment Effect - CATE) como\n",
    "\n",
    "\n",
    "$$\n",
    "\\beta^{CATE}(x) = E[Y_i(1) − Y_i(0)|X] = E[\\beta_i|X_{i}=x]\n",
    "$$\n",
    "\n",
    "Os ITE são inerentemente não observáveis. O que pode ser estimado em vez disso é o Conditional Average Treatment Effect (CATE) , ou seja, o efeito esperado do tratamento individual, condicional em covariáveis ​​X.\n",
    "\n",
    "Para recuperar o CATE, precisamo fazer 3 suposições:\n",
    "\n",
    "* Inconfundibilidade (Unconfoundedness): $Y_{i}(0), Y_{i}(1) \\perp T|X$\n",
    "* Sobreposição (Overlap): $0 < p(x) < 1$\n",
    "* Consistência (Consistency): $Y_{i} = Y_{i}(1)T_{i} + Y_{i}(0)(1-T_{i})$\n",
    "\n",
    "Onde $p(x)$ é o escore de propensão, ou seja, a probabilidade esperada de ser tratado, condicional às covariáveis ​​$X$.\n",
    "\n",
    "Cabe destacar que os modelos lineares têm algumas desvantagens. A principal delas é a suposição de linearidade em $X$. Seria ótimo se pudéssemos substituir o modelo linear por um modelo de machine learning mais flexível. Poderíamos até mesmo conectar o tratamento como um recurso a um modelo de ML, como Decision Trees, rede neural ou Gradient Boosting. \n",
    "\n",
    "$$ y_{i} = M(X_{i}, T_{i}) + \\epsilon_{i}$$\n",
    "\n",
    "\n",
    "mas a partir daí, não está claro como podemos obter estimativas do efeito do tratamento, uma vez que este modelo produzirá previsões de $Y$, e não de $\\beta$. Vamos aprender uns conceitos importantes para solucionar essa questão.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta-learners\n",
    "\n",
    "Podemos usar métodos de aprendizado de máquina para estimar de forma flexível efeitos de tratamento heterogêneos. Em particular, vamos inspecionar três métodos populares que foram introduzidos por Künzel, Sekhon, Bickel, Yu, (2019), https://www.pnas.org/doi/abs/10.1073/pnas.1804597116 \n",
    "\n",
    "Contexto de análise:\n",
    "\n",
    "Assumimos que para um conjunto de indivíduos iid, e observamos uma ($X_{i}, T_{i}, Y_{i}$) composto de\n",
    "\n",
    "* Tratamento $T_{i} \\in \\{0,1\\}$\n",
    "* Recursos $X_{i} \\in \\mathbb{R}^{n}$\n",
    "* Resposta $Y_{i} \\in \\mathbb{R}$\n",
    "\n",
    "Estamos interessados ​​em estimar o efeito médio do tratamento.\n",
    "\n",
    "$$ \\beta= E[Y_{i}^{(1)} - Y_{i}^{(0)}]$$\n",
    "\n",
    "Onde $Y_{i}^{(1)}$ e $Y_{i}^{(0)}$ são os resultados potenciais de um indivíduo sob tratamento e controle, respectivamente.\n",
    "\n",
    "Hipóteses:\n",
    "\n",
    "* **Inconfundibilidade (Unconfoundedness)**: $Y_{i}(0), Y_{i}(1) \\perp T|X$. Ou seja, condicional a características observáveis $X$, a atribuição do tratamento $T$ é tão bom quanto uma aleatorização. O que estamos efetivamente assumindo é que não há outras características que não observamos que poderiam impactar tanto T quanto Y. Esta é uma forte suposição que tem mais probabilidade de ser satisfeita quanto mais características individuais observamos.\n",
    "* **valor de tratamento unitário estável (SUTVA)**: $Y_{i}(t) \\perp T $. Ou seja, o resultado potencial não depende do status do tratamento. \n",
    "\n",
    "**S-learner** (Single-learner)\n",
    "\n",
    "O meta-algoritmo mais simples é o single learner ou S-learner . Para construir o estimador S-learner, ajustamos um único modelo para todas as observações.\n",
    "\n",
    "$$ \\mu(z) = E[Y_{i}| (X_{i}, T_{i}) = z]$$\n",
    "\n",
    "O estimador é dado pela diferença entre os valores previstos avaliados com e sem o tratamento.\n",
    "\n",
    "$$ \\hat{\\beta}_{S-learner}(x) =  \\mu(x, 1) - \\mu(x, 0) $$\n",
    "\n",
    "O problema com o S-learner é que ele está aprendendo um único modelo, então temos que esperar que o modelo descubra heterogeneidade no tratamento $T$, mas pode não ser o caso. Além disso, se o modelo for fortemente regularizado devido à alta dimensionalidade de $X$, pode não recuperar nenhum efeito de tratamento.\n",
    "\n",
    "**T-learner** (Two-learner)\n",
    "\n",
    "Para construir o estimador de dois alunos/aprendizes ou T-learner, ajustamos dois modelos diferentes, um para unidades tratadas e outro para unidades de controle.\n",
    "\n",
    "$$ \\mu_{1}(x) = E[Y_{i}| X_{i}= x, T_{i}=1 ]$$\n",
    "\n",
    "$$ \\mu_{0}(x) = E[Y_{i}| X_{i}= x, T_{i}=0 ]$$\n",
    "\n",
    "O estimador é dado pela diferença entre os valores previstos avaliados com e sem o tratamento.\n",
    "\n",
    "$$ \\hat{\\beta}_{T-learner}(x) =  \\mu_{1}(x) - \\mu_{0}(x) $$\n",
    "\n",
    "o T-learner é muito mais flexível do que o S-learner porque ele se encaixa em dois modelos separados. O problema agora é que estamos usando apenas uma fração dos dados para cada problema de previsão, enquanto o S-learner estava usando todos os dados. Ao ajustar dois modelos separados, estamos perdendo algumas informações. Além disso, ao usar dois modelos diferentes, podemos obter heterogeneidade onde não há nenhuma .\n",
    "\n",
    "**X-learner**\n",
    "\n",
    "O estimador cross-learner ou X-learner é uma extensão do estimador T-learner. Ele é construído da seguinte maneira:\n",
    "\n",
    "* Quanto ao T-learner, calcule modelos separados para $\\mu_{1}(x)$ e $\\mu_{0}(x)$, utilizando as unidades tratadas e de controle, respectivamente\n",
    "* Calcule os efeitos estimados do tratamento como:\n",
    "  \n",
    "$$\n",
    "\\Delta_{i}(x) =\n",
    "\\begin{cases}\n",
    "Y_{i} - \\hat{\\mu}_{0}(x) & \\text{se } T_{i}=1 \\\\\n",
    "\\hat{\\mu}_{1}(x) - Y_{i} & \\text{se } T_{i}=0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "   * Prevendo $\\Delta$ de $X$, calcular $\\hat{\\beta}^{0}(x)$ de unidades tratadas, e $\\hat{\\beta}^{1}(x)$ de unidades de controle.\n",
    "   * Estimar o escore de propensão\n",
    "\n",
    "$$ p(x) = P(T=1|X=x) = E[T|X_{i}=x]$$\n",
    "\n",
    "   * Calcular os efeitos do tratamento\n",
    "\n",
    "$$ \\hat{\\beta}_{X-learner}(x) = p(x) \\hat{\\beta}^{0}(x) + (1-p(x)) \\hat{\\beta}^{1}(x)$$\n",
    "\n",
    "\n",
    "Assim é possivel recuperar as funções de pseudo-resposta. Podemos reescrever os efeitos do tratamento como:\n",
    "\n",
    "$$ \\hat{\\beta}_{X-learner}(x) = \\hat{\\beta}^{0}(x).\\hat{p}(x) + \\hat{\\beta}^{1}(x).(1-\\hat{p}(x)) = \\hat{p}(x)[\\hat{\\mu}^{1}(x) - Y_{i}^{0}] + (1-\\hat{p}(x))[Y_{i}^{1} - \\hat{\\mu}^{0}(x)] = [\\hat{p}(x)\\hat{\\mu}^{1}(x) + (1-\\hat{p}(x))Y_{i}^{1}] - [\\hat{p}(x)Y_{i}^{0} + (1-\\hat{p}(x))\\hat{\\mu}^{0}(x)] $$\n",
    "\n",
    "Para que as funções de pseudo-resposta estimadas pelo X-learner sejam\n",
    "\n",
    "$$ \\tilde{\\mu}^{1}(x) = \\hat{p}(x)\\hat{\\mu}^{1}(x) + (1-\\hat{p}(x))Y_{i}^{1}$$\n",
    "$$ \\tilde{\\mu}^{0}(x) = \\hat{p}(x)Y_{i}^{0} + (1-\\hat{p}(x))\\hat{\\mu}^{0}(x)$$\n",
    "\n",
    "\n",
    "Como podemos ver, o X-learner combina os verdadeiros valores $Y_{i}^{(d)}$ com os estimados $\\mu_{i}^{(d)(x)}$ ponderado pelos escores de propensão $p(x)$, ou seja, as probabilidades de tratamento estimadas.\n",
    "\n",
    "O que isso significa? Significa que se tivermos muito mais observações para um grupo (no nosso caso, o grupo de controle), a função de resposta de controle $\\hat{\\mu}^{0}(x)$ obterá a maior parte do peso. Em vez disso, para o outro grupo (o grupo de tratamento em nosso caso), as observações reais $Y_{i}^{1}$ receberá a maior parte do peso.\n",
    "\n",
    "A principal vantagem do X-learners é que ele adapta a flexibilidade das funções de resposta ao contexto. Em áreas do espaço de estado onde temos muitos dados, ele usa principalmente a função de resposta estimada, em áreas do espaço de estado com poucos dados, ele usa as próprias observações.\n",
    "\n",
    "* Considerações sobre os Learners/Aprendizes: \n",
    "  * A coisa mais simples que podemos fazer é usar um único ou S-learner com o tratamento como um recurso. Isso tende a funcionar bem quando o tratamento não é um preditor fraco do resultado. Mas se esse não for o caso, o S-learner tende a ser tendencioso para zero ou até mesmo abandonar o tratamento completamente. Adicionando um pouco mais de complexidade, podemos forçar o aluno a pegar o tratamento usando um T-learner. Aqui, ajustamos um modelo de Machine Learning por nível de tratamento. Isso funciona bem quando há amostras suficientes para todos os níveis de tratamento, mas pode falhar quando um nível de tratamento tem um tamanho de amostra pequeno, forçando um modelo a ser fortemente regularizado. Para consertar isso, podemos adicionar outro nível de complexidade usando um X-learner, onde temos dois estágios de ajuste e usamos um modelo de pontuação de propensão para corrigir erros potenciais de modelos estimados com muito poucos pontos de dados. Um grande problema desses aprendizes (exceto o aprendiz S) é que eles assumem um tratamento binário ou categórico. \n",
    "\n",
    "Há um aprendiz adicional que ainda não vimos, que é mais geral: **o aprendiz R**. Antes, precisamos aprender mais um conceito.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ortogonalização\n",
    "\n",
    "Antes de estimar o CATE, precisamos conhecer um conceito importante: a Ortogonalização, e como ela é aplicada na econometria. A ideia de ortogonalização é baseada em um teorema elaborado por três econometristas em 1933, Ragnar Frisch, Frederick V. Waugh e Michael C. Lovell. Simplificando, afirma que você pode decompor qualquer modelo de regressão linear multivariável em três estágios ou modelos. \n",
    "\n",
    "Digamos que você tem uma matriz de covariáveis X, e voce particiona ela em duas partes, $X_{1}$ e $D$. \n",
    "\n",
    "* Primeira etapa\n",
    "Pegamos o primeiro conjunto de variáveis $X_{1}$ e fazemos uma regressão linear de $X_{1}$ em $Y$, onde $\\theta_{1}$ é o vetor de parâmetros\n",
    "\n",
    "$$ y_{i} = \\theta_{0} + \\theta_{1} X_{1i} + \\epsilon_{i}$$\n",
    "\n",
    "e guardamos os resíduos dessa regressão ($y^{*}$).\n",
    "\n",
    "$$ y^{*}_{i} = y_{i} - \\hat{y}_{i} = y_{i} - ( \\hat{\\theta}_{0} + \\hat{\\theta}_{1} X_{1i} )$$\n",
    "\n",
    "* Segunda etapa\n",
    "\n",
    "Pegamos novamente o primeiro conjunto de características, mas agora executamos um modelo onde estimamos o segundo conjunto de características ($X_{2}$)\n",
    "\n",
    "$$ D_{i} = \\gamma(0) + \\gamma(1) X_{1i} + e_{i}$$\n",
    "\n",
    "Aqui, estamos usando o primeiro conjunto de recursos para prever o segundo conjunto de recursos. Por fim, consideramos também os resíduos desta segunda etapa.\n",
    "\n",
    "$$ D_{i}^{*} = D_{i} - (\\gamma(0) + \\gamma(1) X_{1i})$$\n",
    "\n",
    "* Terceira etapa\n",
    "Por fim, pegamos os resíduos do primeiro e do segundo estágio e estimamos o seguinte modelo\n",
    "\n",
    "$$ y_{i}^{*} = \\beta_{0} + \\beta_{2} D_{i}^{*} + e_{i}$$\n",
    "\n",
    "\n",
    "* Teorema Frisch – Waugh – Lovell (FWL)\n",
    "\n",
    "O teorema FWL afirma que a estimativa do parâmetro $\\hat{\\beta}_{2}$, estimado anteriormente, é equivalente ao que obtemos ao executar a regressão completa, com todas as covariáveis.\n",
    "\n",
    "$$ y_{i} = \\beta_{0} + \\beta_{1} X_{1i} + \\beta_{2} D_{i} + e_{i}$$\n",
    "\n",
    "**Intuição**\n",
    "\n",
    "Sabemos que a regressão é um modelo muito especial. Cada um de seus parâmetros tem a interpretação de uma derivada parcial, quanto seria Y se X aumentasse em uma unidade, mantendo todos as outras covariáveis constantes. Sabemos também que se omitirmos variáveis ​​da regressão, teremos viés. Especificamente, viés variável omitido (ou viés de confusão). Ainda assim, Frisch-Waugh-Lovell está dizendo que posso dividir meu modelo de regressão em duas partes, nenhuma delas contendo o conjunto completo de recursos, e ainda assim obter a mesma estimativa que obteria executando a regressão inteira. \n",
    "\n",
    "O teorema fornece algumas dicas sobre o que a regressão linear está fazendo. Para obter o coeficiente de uma variável $X_{k}$, a regressão primeiro usa todas as outras variáveis ​​para prever $X_{k}$ e pega os resíduos. Isso “limpa” $X_{k}$ de qualquer influência dessas variáveis. Dessa forma, quando tentamos entender o impacto de $X_{k}$ sobre $Y$, estará livre de viés de variável omitida. Em segundo lugar, a regressão usa todas as outras variáveis ​​para prever $Y$ e pega os resíduos. Isso “limpa” $Y$ de qualquer influência dessas variáveis, reduzindo a variância de $Y$ para que seja mais fácil ver como $X_{k}$ afeta $Y$.\n",
    "\n",
    "A regressão linear está estimando o impacto de $D$ sobre $y$ enquanto contabiliza $X_{1}$. Isso é importante para inferência causal. \n",
    "\n",
    "Assim, podemos construir um modelo que preveja um tratamento ($T$) usando as covariáveis $X$, um modelo que prevê o resultado $y$ usando as mesmas covariáveis, pega os resíduos de ambos os modelos e executa um modelo que estima como o resíduo de $T$ afeta o resíduo de $y$. Este último modelo vai me dizer como $T$ afeta $y$ enquanto controla por $X$. Ou seja, os dois primeiros modelos controlam as variáveis de confusão. Eles estão gerando dados que são praticamente aleatórios. Isso está distorcendo meus dados. É isso que usamos no modelo final para estimar a elasticidade.\n",
    "\n",
    "### Aplicação do Procedimento de Ortogonalização no Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame\n",
    "df = pd.read_stata(\"https://github.com/Daniel-Uhr/data/raw/main/cattaneo2.dta\")\n",
    "\n",
    "# Criar a variável de resultado\n",
    "df['Y'] = df['bweight']\n",
    "\n",
    "# Crie a variável 'Treated' com valor inicial de 0\n",
    "df['Treated'] = 0\n",
    "# Recodifique 'Treated' para 1 se 'mbsmoke' for igual a 'smoker'\n",
    "df.loc[df['mbsmoke'] == 'smoker', 'Treated'] = 1\n",
    "\n",
    "df['casada'] = 0\n",
    "df.loc[df['mmarried']=='married', 'casada'] = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para desviar este conjunto de dados, precisaremos de dois modelos. O primeiro modelo, vamos chamá-lo $M_{t}(X)$, prevê o tratamento (Se a gestante é fumante, no nosso caso) utilizando os confundidores. É um dos estágios que vimos acima, no teorema de Frisch–Waugh–Lovell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_D = smf.ols(\"Treated ~ 1 + casada + mage + medu + fhisp + mhisp + foreign + alcohol + deadkids + nprenatal + mrace + frace + fage + fedu\", data=df).fit()\n",
    "df['m_D_star'] = df['Treated'] - m_D.predict(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim que tivermos este modelo, construiremos os resíduos\n",
    "\n",
    "$$ \\hat{D}_{i} = D_{i} - M_{t}(X_{i})$$\n",
    "\n",
    "Você pode pensar neste resíduo como uma versão do tratamento que é imparcial ou, melhor ainda, que é impossível de prever a partir dos fatores de confusão $X$. Como os fatores de confusão já eram usados ​​para prever $t$, o resíduo é, por definição, imprevisível com com $X$. Outra maneira de dizer isso é que o viés foi explicado pelo modelo $M_{t}(X)$, produzindo $\\hat{t}_{i}$ que é tão bom quanto atribuído aleatoriamente. É claro que isso só funciona se tivermos em $X$ todos os fatores de confusão que causam ambos $T$ e $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Também podemos construir resíduos para o resultado.\n",
    "\n",
    "$$ \\hat{y}_{i} = y_{i} - M_{y}(X_{i})$$\n",
    "\n",
    "\n",
    "Este é outro estágio do teorema de Frisch – Waugh – Lovell. Isso não torna o conjunto menos tendencioso, mas facilita a estimativa do efeito, reduzindo a variância em $y$. Mais uma vez você pode pensar $\\hat{y}_{i}$ como uma versão de $y_{i}$ imprevisível de $X$ ou que teve todas as suas variações devido a $X$ explicadas. Pense nisso. Nós já usamos $X$ para prever $y$ com $M_{y}(X_{i})$. E $\\hat{y}_{i}$ é o erro dessa previsão. Então, por definição, não é possível prever isso a partir de $X$. Todas as informações em $X$ para prever $y$ já foram usadas. Se for esse o caso, a única coisa que resta para explicar $\\hat{y}_{i}$ é algo que não usamos usamos para construí-lo (não incluído em $X$), que é apenas o tratamento (novamente, assumindo que não há fatores de confusão não medidos).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_y = smf.ols(\"Y ~  1 + casada + mage + medu + fhisp + mhisp + foreign + alcohol + deadkids + nprenatal + mrace + frace + fage + fedu\", data=df).fit()\n",
    "df['m_y_star'] = df['Y'] - m_y.predict(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considerando o OLS tradicional com covariáveis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      Y   R-squared:                       0.104\n",
      "Model:                            OLS   Adj. R-squared:                  0.102\n",
      "Method:                 Least Squares   F-statistic:                     38.49\n",
      "Date:                qua, 14 ago 2024   Prob (F-statistic):          5.80e-100\n",
      "Time:                        17:17:13   Log-Likelihood:                -35858.\n",
      "No. Observations:                4642   AIC:                         7.175e+04\n",
      "Df Residuals:                    4627   BIC:                         7.184e+04\n",
      "Df Model:                          14                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept   2851.5203     54.983     51.861      0.000    2743.727    2959.314\n",
      "Treated     -220.0220     21.963    -10.018      0.000    -263.080    -176.964\n",
      "casada        42.7633     23.589      1.813      0.070      -3.482      89.009\n",
      "mage           3.2878      1.935      1.699      0.089      -0.506       7.082\n",
      "medu          -1.5659      4.130     -0.379      0.705      -9.664       6.532\n",
      "fhisp        -22.9258     63.841     -0.359      0.720    -148.084     102.232\n",
      "mhisp         15.0538     68.152      0.221      0.825    -118.557     148.664\n",
      "foreign       -6.8607     39.628     -0.173      0.863     -84.550      70.828\n",
      "alcohol       -9.5560     46.486     -0.206      0.837    -100.690      81.578\n",
      "deadkids     -13.8988     18.872     -0.736      0.461     -50.897      23.099\n",
      "nprenatal     26.1331      2.328     11.225      0.000      21.569      30.697\n",
      "mrace        176.3951     44.950      3.924      0.000      88.272     264.518\n",
      "frace         50.9355     44.172      1.153      0.249     -35.663     137.534\n",
      "fage          -0.9152      1.194     -0.767      0.443      -3.256       1.425\n",
      "fedu           1.0262      3.139      0.327      0.744      -5.128       7.180\n",
      "==============================================================================\n",
      "Omnibus:                      553.208   Durbin-Watson:                   1.999\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1427.823\n",
      "Skew:                          -0.673   Prob(JB):                    8.96e-311\n",
      "Kurtosis:                       5.360   Cond. No.                         485.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "ols = smf.ols(\"Y ~ Treated + 1 + casada + mage + medu + fhisp + mhisp + foreign + alcohol + deadkids + nprenatal + mrace + frace + fage + fedu\", data=df).fit()\n",
    "print(ols.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos verificar o teorema de Frisch-Waugh-Lovell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:               m_y_star   R-squared:                       0.021\n",
      "Model:                            OLS   Adj. R-squared:                  0.021\n",
      "Method:                 Least Squares   F-statistic:                     100.6\n",
      "Date:                qua, 14 ago 2024   Prob (F-statistic):           1.91e-23\n",
      "Time:                        17:17:17   Log-Likelihood:                -35858.\n",
      "No. Observations:                4642   AIC:                         7.172e+04\n",
      "Df Residuals:                    4640   BIC:                         7.173e+04\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept   2.348e-11      8.041   2.92e-12      1.000     -15.764      15.764\n",
      "m_D_star    -220.0220     21.932    -10.032      0.000    -263.020    -177.024\n",
      "==============================================================================\n",
      "Omnibus:                      553.208   Durbin-Watson:                   1.999\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1427.823\n",
      "Skew:                          -0.673   Prob(JB):                    8.96e-311\n",
      "Kurtosis:                       5.360   Cond. No.                         2.73\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "FWL1 = smf.ols(\"m_y_star ~ m_D_star\", data=df).fit()\n",
    "print(FWL1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depois de fazermos as duas transformações, a única coisa que resta para prever esses resíduos é o tratamento. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para resumir, ao prever o tratamento, construímos $\\hat{t}$ que funciona como uma versão imparcial do tratamento; ao prever o resultado, construímos $\\hat{y}$ que é uma versão do resultado que só pode ser explicada se usarmos o tratamento. Esses dados, onde substituímos por $y$ por $\\hat{y}$ e $t$ por $\\hat{t}$, são os dados desviados que queríamos. Podemos usá-lo para avaliar nosso modelo causal da mesma forma que fizemos anteriormente, usando dados aleatórios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  DML - Orthogonal/Double Machine Learning\n",
    "\n",
    "Quando temos muitas variáveis ​​de controle, podemos querer selecionar as mais relevantes , possivelmente capturando não linearidades e interações. Algoritmos de aprendizado de máquina são perfeitos para essa tarefa. No entanto, nesses casos, estamos introduzindo um viés que é chamado de regularização ou pré-teste, ou viés de seleção de recursos. \n",
    "\n",
    "Ou seja, o que acontece se a dimensão de X aumenta e não conhecemos a formafuncional através da qual X afeta Y e D? \n",
    "\n",
    "Nesses casos, podemos usar algoritmos de aprendizado de máquina para descobrir essas relações não lineares de alta dimensão.\n",
    "\n",
    "No artigo de **Chernozhukov et al (2016)**, os autores mostraram que também é possível fazer ortogonalização com modelos de aprendizado de máquina. \n",
    "\n",
    "$$ \\hat{y}_{i} = y_{i} - M_{y}(X_{i})$$\n",
    "\n",
    "$$ \\hat{D}_{i} = D_{i} - M_{t}(X_{i})$$\n",
    "\n",
    "Os modelos de aprendizado de máquina podem ajustar-se perfeitamente aos dados, ou melhor, superajustá-los (overfitting). \n",
    "\n",
    "* Apenas olhando para as equações acima, podemos saber o que acontecerá nesse caso. Se $M_{y}$ de alguma forma, os resíduos serão todos muito próximos de zero. Se isso acontecer, será difícil descobrir como $t$ afeta isso. \n",
    "* Da mesma forma, se $M_{t}$ de alguma forma superajusta, seus resíduos também serão próximos de zero. Conseqüentemente, não haverá variação no resíduo do tratamento para ver como isso pode impactar o resultado.\n",
    "\n",
    "Para explicar isso, precisamos fazer a divisão da amostra. Ou seja, estimamos o modelo com uma parte do conjunto de dados e fazemos previsões na outra parte. A maneira mais simples de fazer isso é dividir a amostra de teste ao meio, fazer dois modelos de forma que cada um seja estimado em uma metade do conjunto de dados e faça previsões na outra metade.\n",
    "\n",
    "Uma implementação um pouco mais elegante usa **validação cruzada K-fold**. A vantagem é que podemos treinar todos os modelos em uma amostra maior que metade do conjunto de teste.\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"images\\kfold-cv.png\"  alt=\"Imagem\" style=\"width: 300px;\"/>\n",
    "</div>\n",
    "\n",
    "Felizmente, esse tipo de previsão cruzada é muito fácil de implementar usando `cross_val_predicta` função do Sklearn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos utilizar o Random Forest para fazer a previsão do tratamento e do resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "X = ['casada', 'mage', 'medu', 'fhisp', 'mhisp', 'foreign', 'alcohol', 'deadkids', 'nprenatal', 'mrace', 'frace', 'fage', 'fedu']\n",
    "D = \"Treated\"\n",
    "y = \"Y\"\n",
    "\n",
    "folds = 5\n",
    "\n",
    "np.random.seed(123)\n",
    "m_D = RandomForestRegressor(n_estimators=100)\n",
    "D_res1 = df[D] - cross_val_predict(m_D, df[X], df[D], cv=folds)\n",
    "\n",
    "m_y = RandomForestRegressor(n_estimators=100)\n",
    "y_res1 = df[y] - cross_val_predict(m_y, df[X], df[y], cv=folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que temos os resíduos, vamos armazená-los como colunas em um novo conjunto de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bweight</th>\n",
       "      <th>mmarried</th>\n",
       "      <th>mhisp</th>\n",
       "      <th>fhisp</th>\n",
       "      <th>foreign</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>deadkids</th>\n",
       "      <th>mage</th>\n",
       "      <th>medu</th>\n",
       "      <th>fage</th>\n",
       "      <th>...</th>\n",
       "      <th>fbaby</th>\n",
       "      <th>prenatal1</th>\n",
       "      <th>Y</th>\n",
       "      <th>Treated</th>\n",
       "      <th>casada</th>\n",
       "      <th>m_t_star</th>\n",
       "      <th>m_y_star</th>\n",
       "      <th>m_D_star</th>\n",
       "      <th>Y-ML_y(X)</th>\n",
       "      <th>Treated-ML_t(X)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3459</td>\n",
       "      <td>married</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>14</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3459</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.086026</td>\n",
       "      <td>47.203835</td>\n",
       "      <td>-0.086026</td>\n",
       "      <td>-295.130000</td>\n",
       "      <td>-0.026667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3260</td>\n",
       "      <td>notmarried</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3260</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.315814</td>\n",
       "      <td>277.930277</td>\n",
       "      <td>-0.315814</td>\n",
       "      <td>303.140000</td>\n",
       "      <td>-0.410000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3572</td>\n",
       "      <td>married</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3572</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.186338</td>\n",
       "      <td>196.895284</td>\n",
       "      <td>-0.186338</td>\n",
       "      <td>132.310000</td>\n",
       "      <td>-0.070000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2948</td>\n",
       "      <td>married</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>12</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2948</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.156887</td>\n",
       "      <td>-451.977522</td>\n",
       "      <td>-0.156887</td>\n",
       "      <td>-460.669000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2410</td>\n",
       "      <td>married</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2410</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.121311</td>\n",
       "      <td>-1040.633348</td>\n",
       "      <td>-0.121311</td>\n",
       "      <td>-1350.973333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   bweight    mmarried  mhisp  fhisp  foreign  alcohol  deadkids  mage  medu  \\\n",
       "0     3459     married      0      0        0        0         0    24    14   \n",
       "1     3260  notmarried      0      0        1        0         0    20    10   \n",
       "2     3572     married      0      0        1        0         0    22     9   \n",
       "3     2948     married      0      0        0        0         0    26    12   \n",
       "4     2410     married      0      0        0        0         0    20    12   \n",
       "\n",
       "   fage  ...  fbaby  prenatal1     Y  Treated casada  m_t_star     m_y_star  \\\n",
       "0    28  ...     No        Yes  3459        0      1 -0.086026    47.203835   \n",
       "1     0  ...     No        Yes  3260        0      0 -0.315814   277.930277   \n",
       "2    30  ...     No        Yes  3572        0      1 -0.186338   196.895284   \n",
       "3    30  ...     No        Yes  2948        0      1 -0.156887  -451.977522   \n",
       "4    21  ...    Yes        Yes  2410        0      1 -0.121311 -1040.633348   \n",
       "\n",
       "   m_D_star    Y-ML_y(X)  Treated-ML_t(X)  \n",
       "0 -0.086026  -295.130000        -0.026667  \n",
       "1 -0.315814   303.140000        -0.410000  \n",
       "2 -0.186338   132.310000        -0.070000  \n",
       "3 -0.156887  -460.669000         0.000000  \n",
       "4 -0.121311 -1350.973333         0.000000  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DML1 = df.assign(**{\n",
    "    \"Y-ML_y(X)\": y_res1,\n",
    "    \"Treated-ML_t(X)\": D_res1,\n",
    "})\n",
    "DML1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 y_res1   R-squared:                       0.014\n",
      "Model:                            OLS   Adj. R-squared:                  0.014\n",
      "Method:                 Least Squares   F-statistic:                     66.82\n",
      "Date:                qua, 14 ago 2024   Prob (F-statistic):           3.80e-16\n",
      "Time:                        17:08:49   Log-Likelihood:                -36189.\n",
      "No. Observations:                4642   AIC:                         7.238e+04\n",
      "Df Residuals:                    4640   BIC:                         7.239e+04\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      4.1388      8.646      0.479      0.632     -12.812      21.089\n",
      "D_res1      -182.0597     22.272     -8.174      0.000    -225.723    -138.396\n",
      "==============================================================================\n",
      "Omnibus:                      289.287   Durbin-Watson:                   1.991\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              619.258\n",
      "Skew:                          -0.414   Prob(JB):                    3.39e-135\n",
      "Kurtosis:                       4.587   Cond. No.                         2.58\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "FWL_DML1 = smf.ols(\"y_res1 ~ D_res1\", data=DML1).fit()\n",
    "print(FWL_DML1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vejamos outro exemplo do DML. Considerando Gradient Boosting Machines (GBM) para prever o tratamento e o resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "X = ['casada', 'mage', 'medu', 'fhisp', 'mhisp', 'foreign', 'alcohol', 'deadkids', 'nprenatal', 'mrace', 'frace', 'fage', 'fedu']\n",
    "D = \"Treated\"\n",
    "y = \"Y\"\n",
    "\n",
    "folds = 5\n",
    "\n",
    "np.random.seed(123)\n",
    "m_D = GradientBoostingRegressor(n_estimators=100)\n",
    "D_res2 = df[D] - cross_val_predict(m_D, df[X], df[D], cv=folds)\n",
    "\n",
    "m_y = GradientBoostingRegressor(n_estimators=100)\n",
    "y_res2 = df[y] - cross_val_predict(m_y, df[X], df[y], cv=folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bweight</th>\n",
       "      <th>mmarried</th>\n",
       "      <th>mhisp</th>\n",
       "      <th>fhisp</th>\n",
       "      <th>foreign</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>deadkids</th>\n",
       "      <th>mage</th>\n",
       "      <th>medu</th>\n",
       "      <th>fage</th>\n",
       "      <th>...</th>\n",
       "      <th>fbaby</th>\n",
       "      <th>prenatal1</th>\n",
       "      <th>Y</th>\n",
       "      <th>Treated</th>\n",
       "      <th>casada</th>\n",
       "      <th>m_t_star</th>\n",
       "      <th>m_y_star</th>\n",
       "      <th>m_D_star</th>\n",
       "      <th>Y-ML_y(X)</th>\n",
       "      <th>Treated-ML_t(X)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3459</td>\n",
       "      <td>married</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>14</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3459</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.086026</td>\n",
       "      <td>47.203835</td>\n",
       "      <td>-0.086026</td>\n",
       "      <td>44.846311</td>\n",
       "      <td>-0.065141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3260</td>\n",
       "      <td>notmarried</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3260</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.315814</td>\n",
       "      <td>277.930277</td>\n",
       "      <td>-0.315814</td>\n",
       "      <td>173.141492</td>\n",
       "      <td>-0.379986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3572</td>\n",
       "      <td>married</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3572</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.186338</td>\n",
       "      <td>196.895284</td>\n",
       "      <td>-0.186338</td>\n",
       "      <td>84.286938</td>\n",
       "      <td>-0.182789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2948</td>\n",
       "      <td>married</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>12</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2948</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.156887</td>\n",
       "      <td>-451.977522</td>\n",
       "      <td>-0.156887</td>\n",
       "      <td>-412.680405</td>\n",
       "      <td>-0.171932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2410</td>\n",
       "      <td>married</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2410</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.121311</td>\n",
       "      <td>-1040.633348</td>\n",
       "      <td>-0.121311</td>\n",
       "      <td>-1096.367541</td>\n",
       "      <td>-0.070470</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   bweight    mmarried  mhisp  fhisp  foreign  alcohol  deadkids  mage  medu  \\\n",
       "0     3459     married      0      0        0        0         0    24    14   \n",
       "1     3260  notmarried      0      0        1        0         0    20    10   \n",
       "2     3572     married      0      0        1        0         0    22     9   \n",
       "3     2948     married      0      0        0        0         0    26    12   \n",
       "4     2410     married      0      0        0        0         0    20    12   \n",
       "\n",
       "   fage  ...  fbaby  prenatal1     Y  Treated casada  m_t_star     m_y_star  \\\n",
       "0    28  ...     No        Yes  3459        0      1 -0.086026    47.203835   \n",
       "1     0  ...     No        Yes  3260        0      0 -0.315814   277.930277   \n",
       "2    30  ...     No        Yes  3572        0      1 -0.186338   196.895284   \n",
       "3    30  ...     No        Yes  2948        0      1 -0.156887  -451.977522   \n",
       "4    21  ...    Yes        Yes  2410        0      1 -0.121311 -1040.633348   \n",
       "\n",
       "   m_D_star    Y-ML_y(X)  Treated-ML_t(X)  \n",
       "0 -0.086026    44.846311        -0.065141  \n",
       "1 -0.315814   173.141492        -0.379986  \n",
       "2 -0.186338    84.286938        -0.182789  \n",
       "3 -0.156887  -412.680405        -0.171932  \n",
       "4 -0.121311 -1096.367541        -0.070470  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DML2 = df.assign(**{\n",
    "    \"Y-ML_y(X)\": y_res2,\n",
    "    \"Treated-ML_t(X)\": D_res2,\n",
    "})\n",
    "DML2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 y_res2   R-squared:                       0.019\n",
      "Model:                            OLS   Adj. R-squared:                  0.019\n",
      "Method:                 Least Squares   F-statistic:                     88.64\n",
      "Date:                qua, 14 ago 2024   Prob (F-statistic):           7.27e-21\n",
      "Time:                        17:09:05   Log-Likelihood:                -35883.\n",
      "No. Observations:                4642   AIC:                         7.177e+04\n",
      "Df Residuals:                    4640   BIC:                         7.178e+04\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      0.8925      8.084      0.110      0.912     -14.956      16.741\n",
      "D_res2      -209.2824     22.229     -9.415      0.000    -252.862    -165.703\n",
      "==============================================================================\n",
      "Omnibus:                      476.250   Durbin-Watson:                   1.981\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1221.498\n",
      "Skew:                          -0.588   Prob(JB):                    5.69e-266\n",
      "Kurtosis:                       5.221   Cond. No.                         2.75\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "FWL_DML2 = smf.ols(\"y_res2 ~ D_res2\", data=DML2).fit()\n",
    "print(FWL_DML2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posteriormente, **Chernozhukov, Chetverikov, Demirer, Duflo, Hansen, Newey e Robins (2018)** consideram o seguinte **modelo parcialmente linear**.\n",
    "\n",
    "$$ Y = \\alpha D + g(X) + u D = m(X) + v $$\n",
    "\n",
    "Onde $Y$ é o resultado, $D$ é o tratamento, $X$ é o vetor de características de controle de alta dimensão.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimação do CATE com DML\n",
    "\n",
    "Até agora, vimos como o Double/Debiased ML nos permite focar na estimativa do Efeito Médio do Tratamento (ATE), mas ele também pode ser usado para estimar a heterogeneidade dos efeitos do tratamento ou o Efeito Médio Condicional do Tratamento (CATE). Essencialmente, agora estamos dizendo que o parâmetro causal $\\tau$ muda dependendo das covariáveis da unidade.\n",
    "\n",
    "$$\n",
    "Y_i - {M}_y(X_i) = \\tau(X_i) \\cdot (T_i - {M}_t(X_i)) + \\epsilon_i\n",
    "$$\n",
    "\n",
    "Para estimar este modelo, usaremos a mesma versão residualizada de preço e vendas, mas agora vamos interagir os resíduos do preço com as outras covariáveis. Em seguida, podemos ajustar um modelo linear de CATE.\n",
    "\n",
    "$$\n",
    "\\tilde{Y_i} = \\alpha + \\beta_1 \\tilde{T_i} + \\pmb{\\beta}_2 \\pmb{X_i} \\tilde{T_i} + \\epsilon_i\n",
    "$$\n",
    "\n",
    "Depois de estimarmos tal modelo, para fazer previsões de CATE, usaremos o conjunto de teste randomizado. Como este modelo final é linear, podemos calcular o CATE mecanicamente:\n",
    "\n",
    "$$\n",
    "\\hat{\\mu}(\\partial PesoBebê_i, X_i) = M(Fumante=1, X_i) - M(Fumante=0, X_i)\n",
    "$$\n",
    "\n",
    "onde $M$ é nosso modelo linear final.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bweight</th>\n",
       "      <th>mmarried</th>\n",
       "      <th>mhisp</th>\n",
       "      <th>fhisp</th>\n",
       "      <th>foreign</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>deadkids</th>\n",
       "      <th>mage</th>\n",
       "      <th>medu</th>\n",
       "      <th>fage</th>\n",
       "      <th>...</th>\n",
       "      <th>birthmonth</th>\n",
       "      <th>lbweight</th>\n",
       "      <th>fbaby</th>\n",
       "      <th>prenatal1</th>\n",
       "      <th>Y</th>\n",
       "      <th>Treated</th>\n",
       "      <th>casada</th>\n",
       "      <th>m_D_star</th>\n",
       "      <th>m_y_star</th>\n",
       "      <th>cate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3459</td>\n",
       "      <td>married</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>14</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3459</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.086026</td>\n",
       "      <td>47.203835</td>\n",
       "      <td>-170.004779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3260</td>\n",
       "      <td>notmarried</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3260</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.315814</td>\n",
       "      <td>277.930277</td>\n",
       "      <td>-134.736065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3572</td>\n",
       "      <td>married</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3572</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.186338</td>\n",
       "      <td>196.895284</td>\n",
       "      <td>-152.370422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2948</td>\n",
       "      <td>married</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>12</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2948</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.156887</td>\n",
       "      <td>-451.977522</td>\n",
       "      <td>-187.639136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2410</td>\n",
       "      <td>married</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2410</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.121311</td>\n",
       "      <td>-1040.633348</td>\n",
       "      <td>-134.736065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4637</th>\n",
       "      <td>3317</td>\n",
       "      <td>notmarried</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>3317</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.683205</td>\n",
       "      <td>5.917001</td>\n",
       "      <td>-143.553243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4638</th>\n",
       "      <td>3030</td>\n",
       "      <td>married</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>3030</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.478816</td>\n",
       "      <td>-259.794082</td>\n",
       "      <td>-222.907850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4639</th>\n",
       "      <td>2950</td>\n",
       "      <td>notmarried</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2950</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.191829</td>\n",
       "      <td>-242.054511</td>\n",
       "      <td>-161.187600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4640</th>\n",
       "      <td>3969</td>\n",
       "      <td>married</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>12</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3969</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.142649</td>\n",
       "      <td>518.911072</td>\n",
       "      <td>-161.187600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4641</th>\n",
       "      <td>3289</td>\n",
       "      <td>married</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>12</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3289</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.142584</td>\n",
       "      <td>-156.900024</td>\n",
       "      <td>-152.370422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4642 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      bweight    mmarried  mhisp  fhisp  foreign  alcohol  deadkids  mage  \\\n",
       "0        3459     married      0      0        0        0         0    24   \n",
       "1        3260  notmarried      0      0        1        0         0    20   \n",
       "2        3572     married      0      0        1        0         0    22   \n",
       "3        2948     married      0      0        0        0         0    26   \n",
       "4        2410     married      0      0        0        0         0    20   \n",
       "...       ...         ...    ...    ...      ...      ...       ...   ...   \n",
       "4637     3317  notmarried      0      0        0        0         0    21   \n",
       "4638     3030     married      0      0        0        1         1    30   \n",
       "4639     2950  notmarried      0      0        0        0         0    23   \n",
       "4640     3969     married      0      0        0        0         0    23   \n",
       "4641     3289     married      0      0        0        0         0    22   \n",
       "\n",
       "      medu  fage  ...  birthmonth  lbweight  fbaby  prenatal1     Y Treated  \\\n",
       "0       14    28  ...          12         0     No        Yes  3459       0   \n",
       "1       10     0  ...           7         0     No        Yes  3260       0   \n",
       "2        9    30  ...           3         0     No        Yes  3572       0   \n",
       "3       12    30  ...           1         0     No        Yes  2948       0   \n",
       "4       12    21  ...           3         1    Yes        Yes  2410       0   \n",
       "...    ...   ...  ...         ...       ...    ...        ...   ...     ...   \n",
       "4637    12    24  ...          10         0    Yes         No  3317       1   \n",
       "4638    12    23  ...           2         0     No         No  3030       1   \n",
       "4639    12     0  ...           5         0    Yes        Yes  2950       0   \n",
       "4640    12    25  ...           2         0    Yes        Yes  3969       0   \n",
       "4641    12    26  ...           7         0     No        Yes  3289       0   \n",
       "\n",
       "      casada  m_D_star     m_y_star        cate  \n",
       "0          1 -0.086026    47.203835 -170.004779  \n",
       "1          0 -0.315814   277.930277 -134.736065  \n",
       "2          1 -0.186338   196.895284 -152.370422  \n",
       "3          1 -0.156887  -451.977522 -187.639136  \n",
       "4          1 -0.121311 -1040.633348 -134.736065  \n",
       "...      ...       ...          ...         ...  \n",
       "4637       0  0.683205     5.917001 -143.553243  \n",
       "4638       1  0.478816  -259.794082 -222.907850  \n",
       "4639       0 -0.191829  -242.054511 -161.187600  \n",
       "4640       1 -0.142649   518.911072 -161.187600  \n",
       "4641       1 -0.142584  -156.900024 -152.370422  \n",
       "\n",
       "[4642 rows x 29 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Estimar o modelo\n",
    "final_model_cate = smf.ols(formula='y_res1 ~ D_res1*(mage) + casada + mage + medu + fhisp + mhisp + foreign + alcohol + deadkids + nprenatal + mrace + frace + fage + fedu', data=df).fit()\n",
    "# Calcular o CATE\n",
    "cate_test = df.assign(cate=final_model_cate.predict(df.assign(D_res1=1)) - final_model_cate.predict(df.assign( D_res1=0)))\n",
    "cate_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedimento de Double/Debiased ML com um Modelo Linear Final\n",
    "\n",
    "O procedimento de Double/Debiased ML com um modelo linear final já é muito bom, como podemos ver pela curva acima. Mas, talvez possamos fazer ainda melhor. De fato, este é um procedimento muito geral que podemos entender como um meta-aprendizado (*meta-learner*). Nie e Wager o chamaram de R-Learner, como uma forma de reconhecer o trabalho de Robinson (1988) e enfatizar o papel da residualização.\n",
    "\n",
    "Essa generalização vem do entendimento de que o procedimento de Double/Debiased ML define uma nova função de perda que podemos minimizar da forma que quisermos. A seguir, veremos como fazer isso de maneira muito semelhante ao que vimos antes ao discutir o método de transformação de alvo ou o F-learner.\n",
    "\n",
    "## Double/Debiased ML Não Paramétrico\n",
    "\n",
    "O interessante sobre o Double-ML é que ele nos livra de toda a complicação de aprender os parâmetros de incômodo em um modelo causal. Com isso, podemos concentrar toda a nossa atenção em aprender o parâmetro causal de interesse, seja o ATE ou o CATE. No entanto, com a especificação acima, ainda estávamos usando um modelo linear após a residualização por ML, como modelo causal final. Em nosso exemplo, isso significa que estamos assumindo que o preço impacta as vendas de forma linear. Isso provavelmente está OK para uma faixa pequena de preços, mas sabemos pela teoria microeconômica que isso não é necessariamente o caso. Pode ser que, em preços baixos, um aumento unitário no preço reduza a demanda em 2 unidades. Mas então, em preços mais altos, um aumento unitário no preço pode reduzir a demanda em apenas 1 unidade. Isso não é uma relação linear.\n",
    "\n",
    "Poderíamos usar a teoria microeconômica aqui para especular sobre a forma funcional do desfecho em relação ao tratamento, mas talvez possamos também delegar isso a um modelo de ML. Em outras palavras, deixar a máquina aprender essa forma funcional complicada. Acontece que isso é totalmente possível se fizermos algumas mudanças em nosso algoritmo original de Double/Debiased ML.\n",
    "\n",
    "Primeiro, começamos exatamente como antes, ortogonalizando o tratamento e o desfecho com previsões cruzadas de um modelo de ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ver:\n",
    "\n",
    "https://matteocourthoud.github.io/post/double_ml/ \n",
    "\n",
    "https://econml.azurewebsites.net/spec/estimation/dml.html#\n",
    "https://github.com/py-why/EconML/blob/main/notebooks/CATE%20validation.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DRML - Doubly Robust Machine Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DDML - Double/Debiased Machine Learning - Chernozhukov et al. (2018)\n",
    "\n",
    "Também conhecido como o ML duplo/desviado não paramétrico\n",
    "\n",
    "\n",
    "O bom do Double/Debiased-ML (DDML) é que ele nos livra de todo o incômodo de aprender os parâmetros incômodos em um modelo causal. Com isso, podemos concentrar toda a nossa atenção em aprender o parâmetro causal de interesse, seja o ATE ou o CATE. No entanto, com a especificação acima, ainda estávamos usando um modelo linear após a residualização do ML, como o modelo causal final. Em nosso exemplo, isso significa que estamos assumindo que o preço impacta as vendas linearmente. Isso provavelmente é OK para uma pequena faixa de preços, mas sabemos pela teoria microeconômica que esse não é necessariamente o caso. Pode ser que, a preços baixos, um aumento unitário no preço reduza a demanda em 2 unidades. Mas então, a preços mais altos, um aumento unitário no preço reduza a demanda em apenas 1 unidade. Essa não é uma relação linear.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000265 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 211\n",
      "[LightGBM] [Info] Number of data points in the train set: 4000, number of used features: 3\n",
      "[LightGBM] [Info] Start training from score 6.143650\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000103 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 213\n",
      "[LightGBM] [Info] Number of data points in the train set: 4000, number of used features: 3\n",
      "[LightGBM] [Info] Start training from score 6.116900\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000061 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 210\n",
      "[LightGBM] [Info] Number of data points in the train set: 4000, number of used features: 3\n",
      "[LightGBM] [Info] Start training from score 6.139250\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000023 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 214\n",
      "[LightGBM] [Info] Number of data points in the train set: 4000, number of used features: 3\n",
      "[LightGBM] [Info] Start training from score 6.140200\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000070 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 214\n",
      "[LightGBM] [Info] Number of data points in the train set: 4000, number of used features: 3\n",
      "[LightGBM] [Info] Start training from score 6.138000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "y = \"sales\"\n",
    "T = \"price\"\n",
    "X = [\"temp\", \"weekday\", \"cost\"]\n",
    "\n",
    "debias_m = LGBMRegressor(max_depth=3)\n",
    "\n",
    "train_pred = train.assign(price_res =  train[T] -\n",
    "                          cross_val_predict(debias_m, train[X], train[T], cv=5)\n",
    "                          + train[T].mean()) # add mu_t for visualization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000026 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 211\n",
      "[LightGBM] [Info] Number of data points in the train set: 4000, number of used features: 3\n",
      "[LightGBM] [Info] Start training from score 6.143650\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000077 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 213\n",
      "[LightGBM] [Info] Number of data points in the train set: 4000, number of used features: 3\n",
      "[LightGBM] [Info] Start training from score 6.116900\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000065 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 210\n",
      "[LightGBM] [Info] Number of data points in the train set: 4000, number of used features: 3\n",
      "[LightGBM] [Info] Start training from score 6.139250\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000027 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 214\n",
      "[LightGBM] [Info] Number of data points in the train set: 4000, number of used features: 3\n",
      "[LightGBM] [Info] Start training from score 6.140200\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000137 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 214\n",
      "[LightGBM] [Info] Number of data points in the train set: 4000, number of used features: 3\n",
      "[LightGBM] [Info] Start training from score 6.138000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000290 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 211\n",
      "[LightGBM] [Info] Number of data points in the train set: 4000, number of used features: 3\n",
      "[LightGBM] [Info] Start training from score 200.602750\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000100 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 213\n",
      "[LightGBM] [Info] Number of data points in the train set: 4000, number of used features: 3\n",
      "[LightGBM] [Info] Start training from score 200.822250\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000024 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 210\n",
      "[LightGBM] [Info] Number of data points in the train set: 4000, number of used features: 3\n",
      "[LightGBM] [Info] Start training from score 200.386250\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000081 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 214\n",
      "[LightGBM] [Info] Number of data points in the train set: 4000, number of used features: 3\n",
      "[LightGBM] [Info] Start training from score 200.575250\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000028 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 214\n",
      "[LightGBM] [Info] Number of data points in the train set: 4000, number of used features: 3\n",
      "[LightGBM] [Info] Start training from score 200.698500\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    }
   ],
   "source": [
    "y = \"sales\"\n",
    "T = \"price\"\n",
    "X = [\"temp\", \"weekday\", \"cost\"]\n",
    "\n",
    "debias_m = LGBMRegressor(max_depth=3)\n",
    "denoise_m = LGBMRegressor(max_depth=3)\n",
    "\n",
    "train_pred = train.assign(price_res =  train[T] - cross_val_predict(debias_m, train[X], train[T], cv=5),\n",
    "                          sales_res =  train[y] - cross_val_predict(denoise_m, train[X], train[y], cv=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso é nada menos que incrível, porque agora podemos chamar isso de função de perda causal . O que significa que, se minimizarmos o quadrado dessa perda, estaremos estimando o valor esperado de\n",
    ", que é o CATE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000039 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 219\n",
      "[LightGBM] [Info] Number of data points in the train set: 5000, number of used features: 3\n",
      "[LightGBM] [Info] Start training from score -3.857895\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMRegressor(max_depth=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;LGBMRegressor<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LGBMRegressor(max_depth=3)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LGBMRegressor(max_depth=3)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_final = LGBMRegressor(max_depth=3)\n",
    " \n",
    "# create the weights\n",
    "w = train_pred[\"price_res\"] ** 2 \n",
    " \n",
    "# create the transformed target\n",
    "y_star = (train_pred[\"sales_res\"] / train_pred[\"price_res\"])\n",
    " \n",
    "# use a weighted regression ML model to predict the target with the weights.\n",
    "model_final.fit(X=train[X], y=y_star, sample_weight=w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deveríamos usar DD se temos DML?\n",
    "https://matteocourthoud.github.io/post/did_or_dml/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em suma:\n",
    "\n",
    "O principal objetivo do DML é ajustar e remover a variável de confusão de forma que a variável de interesse (tratamento) e o desfecho (resultado) fiquem \"ortogonais\" ou \"independentes\".\n",
    "\n",
    "Abordagem:\n",
    "\n",
    "DML combina métodos de aprendizado de máquina com técnicas econométricas para estimar efeitos causais.\n",
    "Utiliza a ideia de \"nuisance parameters\" (parâmetros de incômodo) que são ajustados e removidos do modelo principal para reduzir o viés.\n",
    "A técnica geralmente envolve a aplicação de aprendizado de máquina para prever tanto o tratamento quanto o desfecho usando variáveis de confusão, e então os resíduos dessas previsões são utilizados em um segundo estágio para estimar o efeito causal.\n",
    "\n",
    "Implementação:\n",
    "\n",
    "Primeiro Estágio: Aplicar modelos de aprendizado de máquina para prever a variável de tratamento e o resultado.\n",
    "Segundo Estágio: Utilizar os resíduos dessas previsões em um modelo de regressão para estimar o efeito causal.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doubly Robust Machine Learning (DRML)\n",
    "\n",
    "Objetivo:\n",
    "\n",
    "DRML visa fornecer estimativas consistentes do efeito causal mesmo que uma das duas especificações de modelo (do tratamento ou do desfecho) esteja incorreta.\n",
    "Abordagem:\n",
    "\n",
    "A metodologia DRML combina a ponderação de probabilidade inversa (IPW) com métodos de regressão.\n",
    "Essa abordagem é \"duplamente robusta\" porque, para a consistência da estimativa, é necessário que apenas um dos modelos - ou o modelo de probabilidade de tratamento (propensity score) ou o modelo de desfecho (outcome model) - esteja corretamente especificado.\n",
    "Implementação:\n",
    "\n",
    "Modelagem do Propensity Score: Estimar a probabilidade de cada unidade receber o tratamento.\n",
    "Modelagem do Desfecho: Estimar o desfecho esperado condicional ao tratamento e às covariáveis.\n",
    "Combinação dos Modelos: Utilizar uma combinação das estimativas dos dois modelos para obter uma estimativa do efeito causal que é robusta a erros na especificação de um dos modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resumo Comparativo: DML vs DRML\n",
    "\n",
    "* DML: Foca na remoção de viés através de uma abordagem em dois estágios, utilizando previsões de aprendizado de máquina e resíduos. Busca a ortogonalidade entre o tratamento e o desfecho.\n",
    "* DRML: Utiliza a ponderação de probabilidade inversa e a modelagem do desfecho para obter estimativas robustas. É duplamente robusta porque requer que apenas um dos dois modelos esteja corretamente especificado.\n",
    "\n",
    "Ambas as metodologias são úteis em inferência causal e se beneficiam da flexibilidade e poder dos métodos de aprendizado de máquina, mas cada uma tem suas especificidades e contextos de aplicação onde são mais adequadas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
