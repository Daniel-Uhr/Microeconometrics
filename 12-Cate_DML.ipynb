{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CATE & Double Machine Learning\n",
    "\n",
    "Prof. Daniel de Abreu Pereira Uhr\n",
    "\n",
    "## Conteúdo\n",
    "\n",
    "* CATEs - Conditional Average Treatment Effects (Efeitos Heterogêneos do Tratamento)\n",
    "* Efeitos Heterogêneos do Tratamento e o Arcabouço de Resultados Potenciais\n",
    "* Ortogonalização\n",
    "  * Aplicação do Procedimento de Ortogonalização no Python\n",
    "* Ortogonalização e Machine Learning  (Orthogonal/Double Machine Learning - DML)\n",
    "\n",
    "\n",
    "## Referências\n",
    "\n",
    "**Principais:**\n",
    "* UBER CausalML: https://causalml.readthedocs.io/en/latest/\n",
    "* Microsoft EconML: https://econml.azurewebsites.net/\n",
    "* https://github.com/DoubleML/doubleml-for-py\n",
    "* https://docs.doubleml.org/stable/index.html\n",
    "* https://github.com/MasaAsami/ReproducingDMLDiD/blob/main/notebook/Reproduction_of_DMLDiD_RO_for_NEW_SIMDATA.ipynb\n",
    "* https://econml.azurewebsites.net/\n",
    "* Schmidheiny, K., & Siegloch, S. (2023). On event studies and distributed-lags in two-way fixed effects models: Identification, equivalence, and generalization. Journal of Applied Econometrics, 1- 19. https://doi.org/10.1002/jae.2971\n",
    "* Stevenson, Betsey, Wolfers, Justin, 2006. Bargaining in the shadow of the law: Divorce laws and family distress. Q. J. Econ. 121 (1), 267–288.\n",
    "* Goodman-Bacon, A. (2021). Difference-in-differences with variation in treatment timing. Journal of Econometrics. https://doi.org/10.1016/j.jeconom.2021.03.014\n",
    "* Callaway, B. and Sant'Anna, P. H. C. (2021). Difference-in-Differences with multiple time periods. Journal of Econometrics. https://doi.org/10.1016/j.jeconom.2020.12.001\n",
    "\n",
    "**Complementares:**\n",
    "\n",
    "* Borusyak, K.; Jaravel, X. and Spiess, J. (2023). Revisiting Event Study Designs: Robust and Efficient Estimation. arXiv: https://arxiv.org/pdf/2108.12419.pdf\n",
    "* Clément deChaisemartin, Xavier d’Haultfoeuille. (2022) Difference-in-Differences Estimators of Intertemporal Treatment Effects. hal-03873903\n",
    "* Roth et al. (2022) What’s Trending in Difference-in-Differences? A Synthesis of the Recent Econometrics Literature. https://www.jonathandroth.com/assets/files/DiD_Review_Paper.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CATEs - Conditional Average Treatment Effects (Efeitos Heterogêneos do Tratamento)\n",
    "\n",
    "A nossa função de expectativa condicional é dada por: \n",
    "\n",
    "$$ E[Y|X, T]$$\n",
    "\n",
    "Suponha que queremos realizar inferência causal, entre $T$ e $Y$, sob um contexto $X$, e otimizá-la. Ou seja, \n",
    "\n",
    "$$ \\text{argmax}_{T} E[Y|X, T]$$\n",
    "\n",
    "Ou seja, agora queremos mais do que apenas o efeito médio do tratamento (ATE). Sabemos que o tratamento tem impacto positivo em algumas pessoas, mas não em outras. Os recursos $X$ (covariáveis) desempenham um papel na definição de diferentes perfis de unidades (indivíduos), e cada perfil (indivíduo) **pode responder de forma diferente ao tratamento**. Sendo assim, **agora queremos personalizar o tratamento, dando-o apenas às unidades que melhor respondem a ele** (a ideia é \"direcionar o tratamento\"). Por exemplo, se um medicamento tem efeitos colaterais graves para crianças, podemos querer restringir sua distribuição apenas para adultos. Ou se uma campanha publicitária é eficaz apenas em países de língua inglesa, não vale a pena mostrá-la em outro lugar.\n",
    "\n",
    "**CATE**: \n",
    "\n",
    "O **Conditional Average Treatment Effect** (CATE) é a média do efeito do tratamento condicional a um conjunto de características. Para o caso de tratamento binário, temos: \n",
    "\n",
    "$$ E[Y_{1}-Y_{0}|X]$$\n",
    "\n",
    "ou, para o caso de tratamento contínuo:\n",
    "\n",
    "$$ E[y´(t)|X]$$\n",
    "\n",
    "O condicionamento em $X$ significa que agora permitimos que o efeito do tratamento seja diferente dependendo das características de cada unidade (indivíduo). Queremos tratar apenas as unidades certas (no caso binário) ou descobrir qual é a dosagem de tratamento ideal para cada unidade (no caso contínuo).\n",
    "\n",
    "Considere o exemplo gráfico:\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"images\\elast-partition.png\"  alt=\"Imagem\" style=\"width: 400px;\"/>\n",
    "</div>\n",
    "\n",
    "Desagregando os dados, podemos ver que o efeito do tratamento é diferente para diferentes os 3 grupos de pessoas. \n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"images\\elast-split.png\"  alt=\"Imagem\" style=\"width: 800px;\"/>\n",
    "</div>\n",
    "\n",
    "Assim, a ideia é buscar a indetificação do efeito heterogêneo do tratamento, para que possamos personalizar o tratamento para cada unidade. Vamos tentar entender esse mesmo gráfico de outra maneira. Suponha que queremos encontrar os dias em que a $ \\frac{d \\text{Vendas}}{d \\text{Preço}} $ é menor. Isso significa que, para esses dias, a elasticidade-preço é menor. Logo, do ponto de vista do vendedor, poderíamos aumentar o preço nesses dias sem perder muitas vendas (será que isso ocorre no mercado de aluguéis de imóveis durante às férias?).\n",
    "\n",
    "A elasticidade (sensibilidade do indivíduo) é não observável. Podemos pensar cada unidade como tendo um valor $Y_{i}$, com uma elasticidade (sensibilidade) individual ($ \\frac{d Y_{i}}{d T} $). \n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"images\\elasticity.png\"  alt=\"Imagem\" style=\"width: 400px;\"/>\n",
    "</div>\n",
    "\n",
    "\n",
    "Para vermos as inclinações individuais, teríamos que observar cada dia sob dois preços diferentes e calcular como as vendas mudam para cada um desses preços. \n",
    "\n",
    "$$ \\frac{dY_{i}}{dT} = \\frac{Y_{i}(T_{i}) - Y_{i}(T_{i} + \\epsilon)}{T_{i}-(T_{i} + \\epsilon)}$$\n",
    "\n",
    "Este é o problema fundamental da inferência causal novamente. Nunca podemos ver a mesma unidade sob diferentes condições de tratamento. Então, o que podemos fazer?\n",
    "\n",
    "\n",
    "Um possível ajuste aos dados é utilizar o OLS.\n",
    "\n",
    "$$ y_{i} = \\alpha + \\beta T_{i} + \\gamma X_{i} + \\epsilon_{i} $$\n",
    "\n",
    "Diferenciando o tratamento,\n",
    "\n",
    "$$ \\frac{dY_{i}}{dT} = \\beta$$\n",
    "\n",
    "Nesse caso, é um modelo simples, e um valor constante de beta para todos os indivíduos. Esse é o ATE. No entento, se fizermos a seguinte mudança simples:\n",
    "\n",
    "$$ y_{i} = \\alpha + \\beta T_{i} + \\gamma X_{i} + \\delta X_{i}T_{i} + \\epsilon_{i} $$\n",
    "\n",
    "Teremos uma sensibilidade diferente para cada indivíduo:\n",
    "\n",
    "$$ \\frac{dY_{i}}{dT} = \\beta + \\delta X_{i}$$\n",
    "\n",
    "Onde $\\delta$ é um coeficiente que depende das características de cada indivíduo ($X_{i}$). Em outras palavras, a previsão de sensibilidade mudará conforme $X$. Com essas previsões de sensibilidade, podemos agrupar as unidades por quanto achamos que elas responderão ao tratamento.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efeitos Heterogêneos do Tratamento e o Arcabouço de Resultados Potenciais\n",
    "\n",
    "Podemos definir o efeito do tratamento individual (Individual Treatment Effect - ITE - $\\beta_{i}$) como a diferença entre os resultados potenciais. \n",
    "\n",
    "$$ \\beta_{i}^{ITE} = Y_{i}(1) - Y_{i}(0)$$\n",
    "\n",
    "ou, no caso do tratamento contínuo, $\\beta_{i}^{ITE} = dY_{i}/dt$, onde t é a variável de tratamento. \n",
    "\n",
    "Segundo o problema fundamental da inferência causal, nunca podemos observar o mesmo indivíduo sob diferentes condições de tratamento. \n",
    "\n",
    "$$\n",
    "Y^{obs}_i(t)= \n",
    "\\begin{cases}\n",
    "Y_i(1), & \\text{se } t=1\\\\\n",
    "Y_i(0), & \\text{se } t=0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "Podemos definir o efeito médio do tratamento (Average Treatment Effect - ATE) como\n",
    "\n",
    "\n",
    "$$\n",
    "\\beta^{ATE}= E[Y_i(1) − Y_i(0)] = E[\\beta_i]\n",
    "$$\n",
    "\n",
    "e o efeito do tratamento médio condicional (Conditional Average Treatment Effect - CATE) como\n",
    "\n",
    "\n",
    "$$\n",
    "\\beta^{CATE}(x) = E[Y_i(1) − Y_i(0)|X] = E[\\beta_i|X_{i}=x]\n",
    "$$\n",
    "\n",
    "**Os ITE são inerentemente não observáveis.**\n",
    "\n",
    "O que pode ser estimado em vez disso é o **Conditional Average Treatment Effect (CATE)** , ou seja, o efeito esperado do tratamento individual, condicional em covariáveis $​​X$.\n",
    "\n",
    "Para recuperar o CATE, precisamo fazer 3 suposições:\n",
    "\n",
    "* Não Confundimento / Inconfundibilidade (Unconfoundedness): \n",
    "\n",
    "$$ Y_{i}(0), Y_{i}(1) \\perp T|X $$\n",
    "\n",
    "* Sobreposição (Overlap): \n",
    "\n",
    "$$0 < p(x) < 1$$\n",
    "\n",
    "* Consistência (Consistency): \n",
    "\n",
    "$$Y_{i} = Y_{i}(1)T_{i} + Y_{i}(0)(1-T_{i})$$\n",
    "\n",
    "Onde $p(x)$ é o escore de propensão, ou seja, a probabilidade esperada de ser tratado, condicional às covariáveis ​​$X$.\n",
    "\n",
    "Cabe destacar que os modelos lineares têm algumas desvantagens. A principal delas é a suposição de linearidade em $X$. Seria ótimo se pudéssemos substituir o modelo linear por um modelo de machine learning mais flexível. Poderíamos até mesmo conectar o tratamento como um recurso a um modelo de ML, como Decision Trees, rede neural ou Gradient Boosting. \n",
    "\n",
    "$$ y_{i} = M(X_{i}, T_{i}) + \\epsilon_{i}$$\n",
    "\n",
    "\n",
    "mas a partir daí, não está claro como podemos obter estimativas do efeito do tratamento, uma vez que este modelo produzirá previsões de $Y$, e não de $\\beta$. Vamos aprender uns conceitos importantes para solucionar essa questão.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ortogonalização\n",
    "\n",
    "Antes de estimar o CATE, precisamos conhecer um conceito importante: a Ortogonalização, e como ela é aplicada na econometria. A ideia de ortogonalização é baseada em um teorema elaborado por três econometristas em 1933, Ragnar Frisch, Frederick V. Waugh e Michael C. Lovell. Simplificando, afirma que você pode decompor qualquer modelo de regressão linear multivariável em três estágios ou modelos. \n",
    "\n",
    "Digamos que você tem uma matriz de covariáveis $X$, e voce particiona ela em duas partes, $X_{1}$ e $D$. \n",
    "\n",
    "* **Primeira Etapa**\n",
    "\n",
    "Pegamos o primeiro conjunto de variáveis $X_{1}$ e fazemos uma regressão linear de $X_{1}$ em $Y$, onde $\\theta_{1}$ é o vetor de parâmetros\n",
    "\n",
    "$$ y_{i} = \\theta_{0} + \\theta_{1} X_{1i} + \\epsilon_{i}$$\n",
    "\n",
    "e guardamos os resíduos dessa regressão ($y^{*}$).\n",
    "\n",
    "$$ y^{*}_{i} = y_{i} - \\hat{y}_{i} = y_{i} - ( \\hat{\\theta}_{0} + \\hat{\\theta}_{1} X_{1i} )$$\n",
    "\n",
    "* **Segunda Etapa**\n",
    "\n",
    "Pegamos novamente o primeiro conjunto de características, mas agora executamos um modelo onde estimamos o segundo conjunto de características ($X_{2}$)\n",
    "\n",
    "$$ D_{i} = \\gamma(0) + \\gamma(1) X_{1i} + e_{i}$$\n",
    "\n",
    "Aqui, estamos usando o primeiro conjunto de recursos para prever o segundo conjunto de recursos. Por fim, consideramos também os resíduos desta segunda etapa.\n",
    "\n",
    "$$ D_{i}^{*} = D_{i} - (\\gamma(0) + \\gamma(1) X_{1i})$$\n",
    "\n",
    "* **Terceira etapa**\n",
    "\n",
    "Por fim, pegamos os resíduos do primeiro e do segundo estágio e estimamos o seguinte modelo\n",
    "\n",
    "$$ y_{i}^{*} = \\beta_{0} + \\beta_{2} D_{i}^{*} + e_{i}$$\n",
    "\n",
    "\n",
    "* **Teorema Frisch – Waugh – Lovell (FWL)**\n",
    "\n",
    "O teorema FWL afirma que a estimativa do parâmetro $\\hat{\\beta}_{2}$, estimado anteriormente, é equivalente ao que obtemos ao executar a regressão completa, com todas as covariáveis.\n",
    "\n",
    "$$ y_{i} = \\beta_{0} + \\beta_{1} X_{1i} + \\beta_{2} D_{i} + e_{i}$$\n",
    "\n",
    "\n",
    "**Intuição do teorema FWL**\n",
    "\n",
    "Sabemos que a regressão é um modelo muito especial. Cada um de seus parâmetros tem a interpretação de uma derivada parcial, quanto seria Y se X aumentasse em uma unidade, mantendo todos as outras covariáveis constantes. Sabemos também que se omitirmos variáveis ​​da regressão, teremos viés. Especificamente, viés variável omitido (ou viés de confusão). Ainda assim, Frisch-Waugh-Lovell está dizendo que posso dividir meu modelo de regressão em duas partes, nenhuma delas contendo o conjunto completo de recursos, e ainda assim obter a mesma estimativa que obteria executando a regressão inteira. \n",
    "\n",
    "O teorema fornece algumas dicas sobre o que a regressão linear está fazendo. Para obter o coeficiente de uma variável $X_{k}$, a regressão primeiro usa todas as outras variáveis ​​para prever $X_{k}$ e pega os resíduos. Isso “limpa” $X_{k}$ de qualquer influência dessas variáveis. Dessa forma, quando tentamos entender o impacto de $X_{k}$ sobre $Y$, estará livre de viés de variável omitida. Em segundo lugar, a regressão usa todas as outras variáveis ​​para prever $Y$ e pega os resíduos. Isso “limpa” $Y$ de qualquer influência dessas variáveis, reduzindo a variância de $Y$ para que seja mais fácil ver como $X_{k}$ afeta $Y$.\n",
    "\n",
    "A regressão linear está estimando o impacto de $D$ sobre $y$ enquanto contabiliza $X_{1}$. Isso é importante para inferência causal. \n",
    "\n",
    "Assim, podemos construir um modelo que preveja um tratamento ($T$) usando as covariáveis $X$, um modelo que prevê o resultado $y$ usando as mesmas covariáveis, pega os resíduos de ambos os modelos e executa um modelo que estima como o resíduo de $T$ afeta o resíduo de $y$. Este último modelo vai me dizer como $T$ afeta $y$ enquanto controla por $X$. Ou seja, os dois primeiros modelos controlam as variáveis de confusão. Eles estão gerando dados que são praticamente aleatórios. Isso está distorcendo meus dados. É isso que usamos no modelo final para estimar a elasticidade.\n",
    "\n",
    "### Aplicação do Procedimento de Ortogonalização no Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame\n",
    "df = pd.read_stata(\"https://github.com/Daniel-Uhr/data/raw/main/cattaneo2.dta\")\n",
    "\n",
    "# Criar a variável de resultado\n",
    "df['Y'] = df['bweight']\n",
    "\n",
    "# Crie a variável 'Treated' com valor inicial de 0\n",
    "df['Treated'] = 0\n",
    "# Recodifique 'Treated' para 1 se 'mbsmoke' for igual a 'smoker'\n",
    "df.loc[df['mbsmoke'] == 'smoker', 'Treated'] = 1\n",
    "\n",
    "df['casada'] = 0\n",
    "df.loc[df['mmarried']=='married', 'casada'] = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para desviar este conjunto de dados, precisaremos de dois modelos. O primeiro modelo, vamos chamá-lo $M_{t}(X)$, prevê o tratamento (Se a gestante é fumante, no nosso caso) utilizando os confundidores. É um dos estágios que vimos acima, no teorema de Frisch–Waugh–Lovell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_D = smf.ols(\"Treated ~ 1 + casada + mage + medu + fhisp + mhisp + foreign + alcohol + deadkids + nprenatal + mrace + frace + fage + fedu\", data=df).fit()\n",
    "df['m_D_star'] = df['Treated'] - m_D.predict(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim que tivermos este modelo, construiremos os resíduos\n",
    "\n",
    "$$ \\hat{D}_{i} = D_{i} - M_{t}(X_{i})$$\n",
    "\n",
    "Você pode pensar neste resíduo como uma versão do tratamento que é imparcial ou, melhor ainda, que é impossível de prever a partir dos fatores de confusão $X$. Como os fatores de confusão já eram usados ​​para prever $t$, o resíduo é, por definição, imprevisível com com $X$. Outra maneira de dizer isso é que o viés foi explicado pelo modelo $M_{t}(X)$, produzindo $\\hat{t}_{i}$ que é tão bom quanto atribuído aleatoriamente. É claro que isso só funciona se tivermos em $X$ todos os fatores de confusão que causam ambos $T$ e $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Também podemos construir resíduos para o resultado.\n",
    "\n",
    "$$ \\hat{y}_{i} = y_{i} - M_{y}(X_{i})$$\n",
    "\n",
    "\n",
    "Este é outro estágio do teorema de Frisch – Waugh – Lovell. Isso não torna o conjunto menos tendencioso, mas facilita a estimativa do efeito, reduzindo a variância em $y$. Mais uma vez você pode pensar $\\hat{y}_{i}$ como uma versão de $y_{i}$ imprevisível de $X$ ou que teve todas as suas variações devido a $X$ explicadas. Pense nisso. Nós já usamos $X$ para prever $y$ com $M_{y}(X_{i})$. E $\\hat{y}_{i}$ é o erro dessa previsão. Então, por definição, não é possível prever isso a partir de $X$. Todas as informações em $X$ para prever $y$ já foram usadas. Se for esse o caso, a única coisa que resta para explicar $\\hat{y}_{i}$ é algo que não usamos usamos para construí-lo (não incluído em $X$), que é apenas o tratamento (novamente, assumindo que não há fatores de confusão não medidos).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_y = smf.ols(\"Y ~  1 + casada + mage + medu + fhisp + mhisp + foreign + alcohol + deadkids + nprenatal + mrace + frace + fage + fedu\", data=df).fit()\n",
    "df['m_y_star'] = df['Y'] - m_y.predict(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considerando o OLS tradicional com covariáveis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      Y   R-squared:                       0.104\n",
      "Model:                            OLS   Adj. R-squared:                  0.102\n",
      "Method:                 Least Squares   F-statistic:                     38.49\n",
      "Date:                Wed, 23 Oct 2024   Prob (F-statistic):          5.80e-100\n",
      "Time:                        11:34:49   Log-Likelihood:                -35858.\n",
      "No. Observations:                4642   AIC:                         7.175e+04\n",
      "Df Residuals:                    4627   BIC:                         7.184e+04\n",
      "Df Model:                          14                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept   2851.5203     54.983     51.861      0.000    2743.727    2959.314\n",
      "Treated     -220.0220     21.963    -10.018      0.000    -263.080    -176.964\n",
      "casada        42.7633     23.589      1.813      0.070      -3.482      89.009\n",
      "mage           3.2878      1.935      1.699      0.089      -0.506       7.082\n",
      "medu          -1.5659      4.130     -0.379      0.705      -9.664       6.532\n",
      "fhisp        -22.9258     63.841     -0.359      0.720    -148.084     102.232\n",
      "mhisp         15.0538     68.152      0.221      0.825    -118.557     148.664\n",
      "foreign       -6.8607     39.628     -0.173      0.863     -84.550      70.828\n",
      "alcohol       -9.5560     46.486     -0.206      0.837    -100.690      81.578\n",
      "deadkids     -13.8988     18.872     -0.736      0.461     -50.897      23.099\n",
      "nprenatal     26.1331      2.328     11.225      0.000      21.569      30.697\n",
      "mrace        176.3951     44.950      3.924      0.000      88.272     264.518\n",
      "frace         50.9355     44.172      1.153      0.249     -35.663     137.534\n",
      "fage          -0.9152      1.194     -0.767      0.443      -3.256       1.425\n",
      "fedu           1.0262      3.139      0.327      0.744      -5.128       7.180\n",
      "==============================================================================\n",
      "Omnibus:                      553.208   Durbin-Watson:                   1.999\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1427.823\n",
      "Skew:                          -0.673   Prob(JB):                    8.96e-311\n",
      "Kurtosis:                       5.360   Cond. No.                         485.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "ols = smf.ols(\"Y ~ Treated + 1 + casada + mage + medu + fhisp + mhisp + foreign + alcohol + deadkids + nprenatal + mrace + frace + fage + fedu\", data=df).fit()\n",
    "print(ols.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos verificar se teorema de Frisch-Waugh-Lovell funciona mesmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:               m_y_star   R-squared:                       0.021\n",
      "Model:                            OLS   Adj. R-squared:                  0.021\n",
      "Method:                 Least Squares   F-statistic:                     100.6\n",
      "Date:                Wed, 23 Oct 2024   Prob (F-statistic):           1.91e-23\n",
      "Time:                        11:34:51   Log-Likelihood:                -35858.\n",
      "No. Observations:                4642   AIC:                         7.172e+04\n",
      "Df Residuals:                    4640   BIC:                         7.173e+04\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept   2.348e-11      8.041   2.92e-12      1.000     -15.764      15.764\n",
      "m_D_star    -220.0220     21.932    -10.032      0.000    -263.020    -177.024\n",
      "==============================================================================\n",
      "Omnibus:                      553.208   Durbin-Watson:                   1.999\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1427.823\n",
      "Skew:                          -0.673   Prob(JB):                    8.96e-311\n",
      "Kurtosis:                       5.360   Cond. No.                         2.73\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "FWL1 = smf.ols(\"m_y_star ~ m_D_star\", data=df).fit()\n",
    "print(FWL1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depois de fazermos as duas transformações, a única coisa que resta para prever esses resíduos é o tratamento. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para resumir, ao prever o tratamento, construímos $\\hat{t}$ que funciona como uma versão imparcial do tratamento; ao prever o resultado, construímos $\\hat{y}$ que é uma versão do resultado que só pode ser explicada se usarmos o tratamento. Esses dados, onde substituímos por $y$ por $\\hat{y}$ e $t$ por $\\hat{t}$, são os dados desviados que queríamos. Podemos usá-lo para avaliar nosso modelo causal da mesma forma que fizemos anteriormente, usando dados aleatórios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  DML - Orthogonal/Double Machine Learning\n",
    "\n",
    "Quando temos muitas possíveis variáveis ​​de controle, podemos querer selecionar as mais relevantes, possivelmente capturando não linearidades e interações. Algoritmos de aprendizado de máquina são perfeitos para essa tarefa. No entanto, nesses casos, estamos introduzindo um viés que é chamado de regularização ou pré-teste, ou viés de seleção de recursos ($X$). \n",
    "\n",
    "Ou seja, o que acontece se a dimensão de $X$ aumenta e não conhecemos a forma funcional através da qual $X$ afeta $Y$ e $D$?\n",
    "\n",
    "Nesses casos, podemos usar **algoritmos de aprendizado de máquina** para **descobrir essas relações não lineares de alta dimensão**.\n",
    "\n",
    "No artigo de **Chernozhukov et al (2016)**, os autores mostraram que também é possível fazer ortogonalização com modelos de aprendizado de máquina. \n",
    "\n",
    "$$ \\hat{y}_{i} = y_{i} - M_{y}(X_{i})$$\n",
    "\n",
    "$$ \\hat{D}_{i} = D_{i} - M_{t}(X_{i})$$\n",
    "\n",
    "\n",
    "***Machine Learning (ML)* e *Overfitting***\n",
    "\n",
    "Os modelos de aprendizado de máquina (ML) podem ajustar-se perfeitamente aos dados, ou melhor, superajustá-los (*Overfitting* / \"sobreajuste\"). \n",
    "\n",
    "  * Apenas olhando para as equações anteriores, podemos saber o que acontecerá nesse caso. Se $M_{y}$ de alguma forma, os resíduos serão todos muito próximos de zero. Se isso acontecer, será difícil descobrir como $t$ afeta isso. \n",
    "\n",
    "  * Da mesma forma, se $M_{t}$ de alguma forma superajusta, seus resíduos também serão próximos de zero. Conseqüentemente, não haverá variação no resíduo do tratamento para ver como isso pode impactar o resultado.\n",
    "\n",
    "  * OBS1: O que podemos fazer para evitar isso o overfitting? A solução é simples: **regularização**. A regularização é uma técnica que adiciona um termo à função de perda que penaliza os coeficientes do modelo. Isso faz com que o modelo seja menos sensível aos dados de treinamento, evitando o superajuste.\n",
    "  * OBS2: Os modelos mais comuns de ML para previsão são: Random Forest, Gradient Boosting, Redes Neurais, Support Vector Machines, etc.\n",
    "    * Random Forest: é um modelo de aprendizado de máquina que pode ser usado tanto para classificação quanto para regressão. Ele é um modelo de conjunto que treina várias árvores de decisão em subconjuntos aleatórios dos dados e faz a média de suas previsões.\n",
    "    * Gradient Boosting: é um modelo de aprendizado de máquina que constrói um modelo aditivo de forma progressiva. Ele permite a otimização de funções de perda diferenciáveis arbitrárias.\n",
    "    * Redes Neurais: são modelos de aprendizado de máquina que são inspirados na forma como o cérebro humano funciona. Eles são compostos por camadas de neurônios que processam e transmitem informações.\n",
    "    * Support Vector Machines: são modelos de aprendizado de máquina que são usados para classificação e regressão. Eles são eficazes em espaços de alta dimensão e são capazes de lidar com dados não lineares.\n",
    "    * Outros modelos de ML mais usados em economia são: LASSO, Ridge, Elastic Net, etc. LASSO é um método de regressão que adiciona uma penalidade L1 à função de perda. Isso faz com que alguns coeficientes sejam exatamente zero, o que é útil para seleção de recursos. Ridge é um método de regressão que adiciona uma penalidade L2 à função de perda. Isso faz com que os coeficientes sejam menores, o que é útil para reduzir a variância. Elastic Net é um método de regressão que combina as penalidades L1 e L2. Isso permite que você selecione recursos e reduza a variância ao mesmo tempo.\n",
    "\n",
    "**Validação Cruzada (Cross-Validation)**\n",
    "\n",
    "Para explicar esse procedimemto, precisamos fazer a divisão da amostra. Ou seja, estimamos o modelo com uma parte do conjunto de dados e fazemos previsões na outra parte. Uma maneira simples e intuitiva seria dividir a amostra de teste ao meio, fazer dois modelos de forma que cada um seja estimado em uma metade do conjunto de dados e faça previsões na outra metade.\n",
    "\n",
    "Já uma implementação um pouco mais elegante usa a chamada **validação cruzada K-fold**. Ist\n",
    "\n",
    "\n",
    "\n",
    "A vantagem é que podemos treinar todos os modelos em uma amostra maior que metade do conjunto de teste.\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"images\\kfold-cv.png\"  alt=\"Imagem\" style=\"width: 300px;\"/>\n",
    "</div>\n",
    "\n",
    "Felizmente, esse tipo de previsão cruzada é muito fácil de implementar usando `cross_val_predicta` função do Sklearn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos utilizar o Random Forest para fazer a previsão do tratamento e do resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "X = ['casada', 'mage', 'medu', 'fhisp', 'mhisp', 'foreign', 'alcohol', 'deadkids', 'nprenatal', 'mrace', 'frace', 'fage', 'fedu']\n",
    "D = \"Treated\"\n",
    "y = \"Y\"\n",
    "\n",
    "folds = 5\n",
    "\n",
    "np.random.seed(123)\n",
    "m_D = RandomForestRegressor(n_estimators=100)\n",
    "D_res1 = df[D] - cross_val_predict(m_D, df[X], df[D], cv=folds)\n",
    "\n",
    "m_y = RandomForestRegressor(n_estimators=100)\n",
    "y_res1 = df[y] - cross_val_predict(m_y, df[X], df[y], cv=folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que temos os resíduos, vamos armazená-los como colunas em um novo conjunto de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bweight</th>\n",
       "      <th>mmarried</th>\n",
       "      <th>mhisp</th>\n",
       "      <th>fhisp</th>\n",
       "      <th>foreign</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>deadkids</th>\n",
       "      <th>mage</th>\n",
       "      <th>medu</th>\n",
       "      <th>fage</th>\n",
       "      <th>...</th>\n",
       "      <th>lbweight</th>\n",
       "      <th>fbaby</th>\n",
       "      <th>prenatal1</th>\n",
       "      <th>Y</th>\n",
       "      <th>Treated</th>\n",
       "      <th>casada</th>\n",
       "      <th>m_D_star</th>\n",
       "      <th>m_y_star</th>\n",
       "      <th>Y-ML_y(X)</th>\n",
       "      <th>Treated-ML_t(X)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3459</td>\n",
       "      <td>married</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>14</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3459</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.086026</td>\n",
       "      <td>47.203835</td>\n",
       "      <td>-295.130000</td>\n",
       "      <td>-0.026667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3260</td>\n",
       "      <td>notmarried</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3260</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.315814</td>\n",
       "      <td>277.930277</td>\n",
       "      <td>303.140000</td>\n",
       "      <td>-0.410000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3572</td>\n",
       "      <td>married</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3572</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.186338</td>\n",
       "      <td>196.895284</td>\n",
       "      <td>132.310000</td>\n",
       "      <td>-0.070000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2948</td>\n",
       "      <td>married</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>12</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2948</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.156887</td>\n",
       "      <td>-451.977522</td>\n",
       "      <td>-460.669000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2410</td>\n",
       "      <td>married</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2410</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.121311</td>\n",
       "      <td>-1040.633348</td>\n",
       "      <td>-1350.973333</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   bweight    mmarried  mhisp  fhisp  foreign  alcohol  deadkids  mage  medu  \\\n",
       "0     3459     married      0      0        0        0         0    24    14   \n",
       "1     3260  notmarried      0      0        1        0         0    20    10   \n",
       "2     3572     married      0      0        1        0         0    22     9   \n",
       "3     2948     married      0      0        0        0         0    26    12   \n",
       "4     2410     married      0      0        0        0         0    20    12   \n",
       "\n",
       "   fage  ...  lbweight  fbaby  prenatal1     Y Treated casada  m_D_star  \\\n",
       "0    28  ...         0     No        Yes  3459       0      1 -0.086026   \n",
       "1     0  ...         0     No        Yes  3260       0      0 -0.315814   \n",
       "2    30  ...         0     No        Yes  3572       0      1 -0.186338   \n",
       "3    30  ...         0     No        Yes  2948       0      1 -0.156887   \n",
       "4    21  ...         1    Yes        Yes  2410       0      1 -0.121311   \n",
       "\n",
       "      m_y_star    Y-ML_y(X)  Treated-ML_t(X)  \n",
       "0    47.203835  -295.130000        -0.026667  \n",
       "1   277.930277   303.140000        -0.410000  \n",
       "2   196.895284   132.310000        -0.070000  \n",
       "3  -451.977522  -460.669000         0.000000  \n",
       "4 -1040.633348 -1350.973333         0.000000  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DML1 = df.assign(**{\n",
    "    \"Y-ML_y(X)\": y_res1,\n",
    "    \"Treated-ML_t(X)\": D_res1,\n",
    "})\n",
    "DML1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 y_res1   R-squared:                       0.014\n",
      "Model:                            OLS   Adj. R-squared:                  0.014\n",
      "Method:                 Least Squares   F-statistic:                     66.82\n",
      "Date:                Wed, 23 Oct 2024   Prob (F-statistic):           3.80e-16\n",
      "Time:                        11:35:07   Log-Likelihood:                -36189.\n",
      "No. Observations:                4642   AIC:                         7.238e+04\n",
      "Df Residuals:                    4640   BIC:                         7.239e+04\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      4.1388      8.646      0.479      0.632     -12.812      21.089\n",
      "D_res1      -182.0597     22.272     -8.174      0.000    -225.723    -138.396\n",
      "==============================================================================\n",
      "Omnibus:                      289.287   Durbin-Watson:                   1.991\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              619.258\n",
      "Skew:                          -0.414   Prob(JB):                    3.39e-135\n",
      "Kurtosis:                       4.587   Cond. No.                         2.58\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "FWL_DML1 = smf.ols(\"y_res1 ~ D_res1\", data=DML1).fit()\n",
    "print(FWL_DML1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vejamos outro exemplo do DML. Considerando Gradient Boosting Machines (GBM) para prever o tratamento e o resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import numpy as np\n",
    "\n",
    "# Definir as variáveis X, D, y\n",
    "X = ['casada', 'mage', 'medu', 'fhisp', 'mhisp', 'foreign', 'alcohol', 'deadkids', 'nprenatal', 'mrace', 'frace', 'fage', 'fedu']\n",
    "D = \"Treated\"\n",
    "y = \"Y\"\n",
    "\n",
    "# Definir o número de folds para a validação cruzada\n",
    "folds = 5\n",
    "\n",
    "# Garantir reprodutibilidade nos modelos de Gradient Boosting\n",
    "m_D = GradientBoostingRegressor(n_estimators=100, random_state=123)\n",
    "D_res2 = df[D] - cross_val_predict(m_D, df[X], df[D], cv=folds)\n",
    "\n",
    "m_y = GradientBoostingRegressor(n_estimators=100, random_state=123)\n",
    "y_res2 = df[y] - cross_val_predict(m_y, df[X], df[y], cv=folds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bweight</th>\n",
       "      <th>mmarried</th>\n",
       "      <th>mhisp</th>\n",
       "      <th>fhisp</th>\n",
       "      <th>foreign</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>deadkids</th>\n",
       "      <th>mage</th>\n",
       "      <th>medu</th>\n",
       "      <th>fage</th>\n",
       "      <th>...</th>\n",
       "      <th>lbweight</th>\n",
       "      <th>fbaby</th>\n",
       "      <th>prenatal1</th>\n",
       "      <th>Y</th>\n",
       "      <th>Treated</th>\n",
       "      <th>casada</th>\n",
       "      <th>m_D_star</th>\n",
       "      <th>m_y_star</th>\n",
       "      <th>Y-ML_y(X)</th>\n",
       "      <th>Treated-ML_t(X)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3459</td>\n",
       "      <td>married</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>14</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3459</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.086026</td>\n",
       "      <td>47.203835</td>\n",
       "      <td>44.846311</td>\n",
       "      <td>-0.065141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3260</td>\n",
       "      <td>notmarried</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3260</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.315814</td>\n",
       "      <td>277.930277</td>\n",
       "      <td>173.141492</td>\n",
       "      <td>-0.379986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3572</td>\n",
       "      <td>married</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3572</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.186338</td>\n",
       "      <td>196.895284</td>\n",
       "      <td>84.286938</td>\n",
       "      <td>-0.182789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2948</td>\n",
       "      <td>married</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>12</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2948</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.156887</td>\n",
       "      <td>-451.977522</td>\n",
       "      <td>-412.680405</td>\n",
       "      <td>-0.171932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2410</td>\n",
       "      <td>married</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2410</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.121311</td>\n",
       "      <td>-1040.633348</td>\n",
       "      <td>-1096.367541</td>\n",
       "      <td>-0.070470</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   bweight    mmarried  mhisp  fhisp  foreign  alcohol  deadkids  mage  medu  \\\n",
       "0     3459     married      0      0        0        0         0    24    14   \n",
       "1     3260  notmarried      0      0        1        0         0    20    10   \n",
       "2     3572     married      0      0        1        0         0    22     9   \n",
       "3     2948     married      0      0        0        0         0    26    12   \n",
       "4     2410     married      0      0        0        0         0    20    12   \n",
       "\n",
       "   fage  ...  lbweight  fbaby  prenatal1     Y Treated casada  m_D_star  \\\n",
       "0    28  ...         0     No        Yes  3459       0      1 -0.086026   \n",
       "1     0  ...         0     No        Yes  3260       0      0 -0.315814   \n",
       "2    30  ...         0     No        Yes  3572       0      1 -0.186338   \n",
       "3    30  ...         0     No        Yes  2948       0      1 -0.156887   \n",
       "4    21  ...         1    Yes        Yes  2410       0      1 -0.121311   \n",
       "\n",
       "      m_y_star    Y-ML_y(X)  Treated-ML_t(X)  \n",
       "0    47.203835    44.846311        -0.065141  \n",
       "1   277.930277   173.141492        -0.379986  \n",
       "2   196.895284    84.286938        -0.182789  \n",
       "3  -451.977522  -412.680405        -0.171932  \n",
       "4 -1040.633348 -1096.367541        -0.070470  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DML2 = df.assign(**{\n",
    "    \"Y-ML_y(X)\": y_res2,\n",
    "    \"Treated-ML_t(X)\": D_res2,\n",
    "})\n",
    "DML2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 y_res2   R-squared:                       0.019\n",
      "Model:                            OLS   Adj. R-squared:                  0.018\n",
      "Method:                 Least Squares   F-statistic:                     88.46\n",
      "Date:                Wed, 23 Oct 2024   Prob (F-statistic):           7.93e-21\n",
      "Time:                        11:59:22   Log-Likelihood:                -35882.\n",
      "No. Observations:                4642   AIC:                         7.177e+04\n",
      "Df Residuals:                    4640   BIC:                         7.178e+04\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      0.7775      8.082      0.096      0.923     -15.067      16.622\n",
      "D_res2      -209.0031     22.221     -9.405      0.000    -252.568    -165.439\n",
      "==============================================================================\n",
      "Omnibus:                      476.984   Durbin-Watson:                   1.982\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1222.895\n",
      "Skew:                          -0.589   Prob(JB):                    2.83e-266\n",
      "Kurtosis:                       5.222   Cond. No.                         2.75\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "FWL_DML2 = smf.ols(\"y_res2 ~ D_res2\", data=DML2).fit()\n",
    "print(FWL_DML2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vimos a mecânica do DML. Mas qual o melhor modelo de ML para prever o tratamento e o resultado? Precisamos avaliar.\n",
    "\n",
    "precisamos calcular o erro quadrático médio (MSE) dos resíduos gerados para $D$ e $Y$ para ambos os modelos, e então comparar seus desempenhos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for D - Random Forest: 0.15049072305334754\n",
      "MSE for y - Random Forest: 350812.5710231999\n",
      "MSE for D - Gradient Boosting: 0.1322742607784923\n",
      "MSE for y - Gradient Boosting: 309191.01191423537\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "# Definir as variáveis\n",
    "X = ['casada', 'mage', 'medu', 'fhisp', 'mhisp', 'foreign', 'alcohol', 'deadkids', 'nprenatal', 'mrace', 'frace', 'fage', 'fedu']\n",
    "D = \"Treated\"\n",
    "y = \"Y\"\n",
    "folds = 5\n",
    "\n",
    "# RandomForest - Estimando m_D e m_y\n",
    "rf_model_D = RandomForestRegressor(n_estimators=100)\n",
    "D_res_rf = df[D] - cross_val_predict(rf_model_D, df[X], df[D], cv=folds)\n",
    "\n",
    "rf_model_y = RandomForestRegressor(n_estimators=100)\n",
    "y_res_rf = df[y] - cross_val_predict(rf_model_y, df[X], df[y], cv=folds)\n",
    "\n",
    "# GradientBoosting - Estimando m_D e m_y\n",
    "gb_model_D = GradientBoostingRegressor(n_estimators=100)\n",
    "D_res_gb = df[D] - cross_val_predict(gb_model_D, df[X], df[D], cv=folds)\n",
    "\n",
    "gb_model_y = GradientBoostingRegressor(n_estimators=100)\n",
    "y_res_gb = df[y] - cross_val_predict(gb_model_y, df[X], df[y], cv=folds)\n",
    "\n",
    "# Calculando o MSE para cada modelo\n",
    "mse_D_rf = mean_squared_error(df[D], df[D] - D_res_rf)  # Random Forest para D\n",
    "mse_y_rf = mean_squared_error(df[y], df[y] - y_res_rf)  # Random Forest para y\n",
    "\n",
    "mse_D_gb = mean_squared_error(df[D], df[D] - D_res_gb)  # Gradient Boosting para D\n",
    "mse_y_gb = mean_squared_error(df[y], df[y] - y_res_gb)  # Gradient Boosting para y\n",
    "\n",
    "# Exibir os resultados de MSE\n",
    "print(\"MSE for D - Random Forest:\", mse_D_rf)\n",
    "print(\"MSE for y - Random Forest:\", mse_y_rf)\n",
    "print(\"MSE for D - Gradient Boosting:\", mse_D_gb)\n",
    "print(\"MSE for y - Gradient Boosting:\", mse_y_gb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparação para $D$ (Variável de Tratamento):\n",
    "\n",
    "* Random Forest MSE: 0.1506\n",
    "* Gradient Boosting MSE: 0.1323\n",
    "\n",
    "Aqui, o Gradient Boosting tem um desempenho melhor na predição de $D$, pois seu MSE é menor. Isso significa que ele foi mais eficiente na captura da relação entre as variáveis explicativas $X$ e o tratamento $D$, resultando em resíduos menores.\n",
    "\n",
    "Comparação para $Y$\n",
    "* Random Forest MSE: 352060.35\n",
    "* Gradient Boosting MSE: 309063.98\n",
    "\n",
    "Novamente, o Gradient Boosting apresenta um MSE menor na predição de $y$, indicando que ele conseguiu capturar melhor a relação entre as variáveis explicativas  $X$ e o resultado $Y$, quando comparado ao Random Forest.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos fazer de forma geral a análise para diversos modelos e verificar qual o melhor baseados no critério de erro quadrado médio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: RandomForest\n",
      "  MSE for D: 0.15114550000030305\n",
      "  MSE for y: 351297.08558207133\n",
      "\n",
      "Model: GradientBoosting\n",
      "  MSE for D: 0.13228355511993442\n",
      "  MSE for y: 309125.79049034865\n",
      "\n",
      "Model: Lasso\n",
      "  MSE for D: 0.14514535464451975\n",
      "  MSE for y: 308621.37257196667\n",
      "\n",
      "Model: Ridge\n",
      "  MSE for D: 0.13530450869509936\n",
      "  MSE for y: 308669.9810617438\n",
      "\n",
      "Model: ElasticNet\n",
      "  MSE for D: 0.14369145438382802\n",
      "  MSE for y: 308710.90380429785\n",
      "\n",
      "Model: MLP\n",
      "  MSE for D: 0.14038924191018548\n",
      "  MSE for y: 360738.86426083324\n",
      "\n",
      "Model: SVR\n",
      "  MSE for D: 0.1582687258940923\n",
      "  MSE for y: 330887.97070007736\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Lasso, Ridge, ElasticNet\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Definir os modelos que vamos testar\n",
    "models = {\n",
    "    \"RandomForest\": RandomForestRegressor(n_estimators=100),\n",
    "    \"GradientBoosting\": GradientBoostingRegressor(n_estimators=100),\n",
    "    \"Lasso\": Lasso(alpha=0.1),\n",
    "    \"Ridge\": Ridge(alpha=1.0),\n",
    "    \"ElasticNet\": ElasticNet(alpha=0.1, l1_ratio=0.5),\n",
    "    \"MLP\": MLPRegressor(hidden_layer_sizes=(50,), max_iter=1000),\n",
    "    \"SVR\": SVR()\n",
    "}\n",
    "\n",
    "# Definir as variáveis\n",
    "X = ['casada', 'mage', 'medu', 'fhisp', 'mhisp', 'foreign', 'alcohol', 'deadkids', 'nprenatal', 'mrace', 'frace', 'fage', 'fedu']\n",
    "D = \"Treated\"\n",
    "y = \"Y\"\n",
    "folds = 5\n",
    "\n",
    "# Dicionário para armazenar os MSE de cada modelo para D e y\n",
    "mse_results = {}\n",
    "\n",
    "# Loop para ajustar os modelos e calcular os MSE para D e y\n",
    "for name, model in models.items():\n",
    "    # Estimativa de D com validação cruzada\n",
    "    D_res = df[D] - cross_val_predict(model, df[X], df[D], cv=folds)\n",
    "    # Estimativa de y com validação cruzada\n",
    "    y_res = df[y] - cross_val_predict(model, df[X], df[y], cv=folds)\n",
    "    \n",
    "    # Calcular MSE para D e y\n",
    "    mse_D = mean_squared_error(df[D], df[D] - D_res)\n",
    "    mse_y = mean_squared_error(df[y], df[y] - y_res)\n",
    "    \n",
    "    # Armazenar os resultados\n",
    "    mse_results[name] = {\"MSE for D\": mse_D, \"MSE for y\": mse_y}\n",
    "\n",
    "# Exibir os resultados\n",
    "for model_name, mse in mse_results.items():\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"  MSE for D: {mse['MSE for D']}\")\n",
    "    print(f\"  MSE for y: {mse['MSE for y']}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para os resíduos de $D$: O Gradient Boosting é o modelo preferido, pois ele obteve o menor MSE para prever a variável de tratamento.\n",
    "\n",
    "Para os resíduos de $y$: O Lasso foi o melhor em prever o resultado.\n",
    "\n",
    "Vou aumentar o número fold para 8 e verificar se os resultados mudam. Esse aumento de 5 para 8 folds é importante para garantir que o modelo seja treinado em uma amostra maior, o que pode melhorar a precisão das previsões. Além de ser uma prática para verificar a robustez dos resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: RandomForest\n",
      "  MSE for D: 0.1520980970340551\n",
      "  MSE for y: 349468.8053384342\n",
      "\n",
      "Model: GradientBoosting\n",
      "  MSE for D: 0.13244114694873113\n",
      "  MSE for y: 308077.9936950858\n",
      "\n",
      "Model: Lasso\n",
      "  MSE for D: 0.14517956178493543\n",
      "  MSE for y: 308414.018017211\n",
      "\n",
      "Model: Ridge\n",
      "  MSE for D: 0.13535137678662412\n",
      "  MSE for y: 308426.02969660814\n",
      "\n",
      "Model: ElasticNet\n",
      "  MSE for D: 0.1437445145464821\n",
      "  MSE for y: 308393.73652545904\n",
      "\n",
      "Model: MLP\n",
      "  MSE for D: 0.14012366416355287\n",
      "  MSE for y: 352474.1787789329\n",
      "\n",
      "Model: SVR\n",
      "  MSE for D: 0.1582865914491929\n",
      "  MSE for y: 330487.8627229285\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Lasso, Ridge, ElasticNet\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# Definir os modelos que vamos testar\n",
    "models = {\n",
    "    \"RandomForest\": RandomForestRegressor(n_estimators=100),\n",
    "    \"GradientBoosting\": GradientBoostingRegressor(n_estimators=100),\n",
    "    \"Lasso\": Lasso(alpha=0.1),\n",
    "    \"Ridge\": Ridge(alpha=1.0),\n",
    "    \"ElasticNet\": ElasticNet(alpha=0.1, l1_ratio=0.5),\n",
    "    \"MLP\": MLPRegressor(hidden_layer_sizes=(50,), max_iter=1000),\n",
    "    \"SVR\": SVR()\n",
    "}\n",
    "\n",
    "# Definir as variáveis\n",
    "X = ['casada', 'mage', 'medu', 'fhisp', 'mhisp', 'foreign', 'alcohol', 'deadkids', 'nprenatal', 'mrace', 'frace', 'fage', 'fedu']\n",
    "D = \"Treated\"\n",
    "y = \"Y\"\n",
    "folds = 8\n",
    "\n",
    "# Dicionário para armazenar os MSE de cada modelo para D e y\n",
    "mse_results = {}\n",
    "\n",
    "# Loop para ajustar os modelos e calcular os MSE para D e y\n",
    "for name, model in models.items():\n",
    "    # Estimativa de D com validação cruzada\n",
    "    D_res = df[D] - cross_val_predict(model, df[X], df[D], cv=folds)\n",
    "    # Estimativa de y com validação cruzada\n",
    "    y_res = df[y] - cross_val_predict(model, df[X], df[y], cv=folds)\n",
    "    \n",
    "    # Calcular MSE para D e y\n",
    "    mse_D = mean_squared_error(df[D], df[D] - D_res)\n",
    "    mse_y = mean_squared_error(df[y], df[y] - y_res)\n",
    "    \n",
    "    # Armazenar os resultados\n",
    "    mse_results[name] = {\"MSE for D\": mse_D, \"MSE for y\": mse_y}\n",
    "\n",
    "# Exibir os resultados\n",
    "for model_name, mse in mse_results.items():\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"  MSE for D: {mse['MSE for D']}\")\n",
    "    print(f\"  MSE for y: {mse['MSE for y']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aumentando para 8 folds, o Gradient Boosting torna-se o melhor modelo para prever tanto o tratamento quanto o resultado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos utilizar diretamente ou o pacote EconML, ou DoubleML, que é uma biblioteca Python para estimar e inferir sobre os efeitos heterogêneos do tratamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               coef    std err         t         P>|t|       2.5 %      97.5 %\n",
      "Treated -210.429341  22.485291 -9.358533  8.085127e-21 -254.499701 -166.358981\n"
     ]
    }
   ],
   "source": [
    "from doubleml import DoubleMLData, DoubleMLPLR\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Definir X, D, y \n",
    "X = ['casada', 'mage', 'medu', 'fhisp', 'mhisp', 'foreign', 'alcohol', 'deadkids', 'nprenatal', 'mrace', 'frace', 'fage', 'fedu']  \n",
    "D = \"Treated\"  \n",
    "y = \"Y\" \n",
    "\n",
    "# Configurando o DoubleMLData\n",
    "dml_data = DoubleMLData(df, y_col=y, d_cols=D, x_cols=X)\n",
    "\n",
    "# Ajustar Gradient Boosting com random_state para garantir reprodutibilidade\n",
    "ml_gb = GradientBoostingRegressor(n_estimators=100, random_state=123)\n",
    "\n",
    "# Definir a seed na validação cruzada\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "\n",
    "# Configurar o estimador Double Machine Learning para Regressão Linear Parcial (PLR)\n",
    "dml_plr = DoubleMLPLR(dml_data, ml_gb, ml_gb, n_folds=5)\n",
    "\n",
    "# Rodar o modelo Double Machine Learning\n",
    "dml_plr.fit()\n",
    "\n",
    "# Exibir o resultado do DML\n",
    "print(dml_plr.summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimação do CATE com DML\n",
    "\n",
    "Até agora, vimos como o Double/Debiased ML nos permite focar na estimativa do Efeito Médio do Tratamento (ATE). No entanto, ele também pode ser usado para estimar a heterogeneidade dos efeitos do tratamento ou o Efeito Médio Condicional do Tratamento (CATE). Essencialmente, agora estamos dizendo que o parâmetro causal $\\tau$ muda dependendo das covariáveis da unidade.\n",
    "\n",
    "$$\n",
    "Y_i - {M}_y(X_i) = \\tau(X_i) \\cdot (T_i - {M}_t(X_i)) + \\epsilon_i\n",
    "$$\n",
    "\n",
    "Para estimar este modelo, usaremos a mesma versão residualizada de preço e vendas, mas agora vamos interagir os resíduos do preço com as outras covariáveis. Em seguida, podemos ajustar um modelo linear de CATE.\n",
    "\n",
    "$$\n",
    "\\tilde{Y_i} = \\alpha + \\beta_1 \\tilde{T_i} + \\pmb{\\beta}_2 \\pmb{X_i} \\tilde{T_i} + \\epsilon_i\n",
    "$$\n",
    "\n",
    "Depois de estimarmos tal modelo, para fazer previsões de CATE, usaremos o conjunto de teste randomizado. Como este modelo final é linear, podemos calcular o CATE mecanicamente:\n",
    "\n",
    "$$\n",
    "\\hat{\\mu}(\\partial PesoBebê_i, X_i) = M(Fumante=1, X_i) - M(Fumante=0, X_i)\n",
    "$$\n",
    "\n",
    "onde $M$ é nosso modelo linear final.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bweight</th>\n",
       "      <th>mmarried</th>\n",
       "      <th>mhisp</th>\n",
       "      <th>fhisp</th>\n",
       "      <th>foreign</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>deadkids</th>\n",
       "      <th>mage</th>\n",
       "      <th>medu</th>\n",
       "      <th>fage</th>\n",
       "      <th>...</th>\n",
       "      <th>birthmonth</th>\n",
       "      <th>lbweight</th>\n",
       "      <th>fbaby</th>\n",
       "      <th>prenatal1</th>\n",
       "      <th>Y</th>\n",
       "      <th>Treated</th>\n",
       "      <th>casada</th>\n",
       "      <th>m_D_star</th>\n",
       "      <th>m_y_star</th>\n",
       "      <th>cate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3459</td>\n",
       "      <td>married</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>14</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3459</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.086026</td>\n",
       "      <td>47.203835</td>\n",
       "      <td>-170.004779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3260</td>\n",
       "      <td>notmarried</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3260</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.315814</td>\n",
       "      <td>277.930277</td>\n",
       "      <td>-134.736065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3572</td>\n",
       "      <td>married</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3572</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.186338</td>\n",
       "      <td>196.895284</td>\n",
       "      <td>-152.370422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2948</td>\n",
       "      <td>married</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>12</td>\n",
       "      <td>30</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2948</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.156887</td>\n",
       "      <td>-451.977522</td>\n",
       "      <td>-187.639136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2410</td>\n",
       "      <td>married</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2410</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.121311</td>\n",
       "      <td>-1040.633348</td>\n",
       "      <td>-134.736065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4637</th>\n",
       "      <td>3317</td>\n",
       "      <td>notmarried</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>3317</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.683205</td>\n",
       "      <td>5.917001</td>\n",
       "      <td>-143.553243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4638</th>\n",
       "      <td>3030</td>\n",
       "      <td>married</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>3030</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.478816</td>\n",
       "      <td>-259.794082</td>\n",
       "      <td>-222.907850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4639</th>\n",
       "      <td>2950</td>\n",
       "      <td>notmarried</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2950</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.191829</td>\n",
       "      <td>-242.054511</td>\n",
       "      <td>-161.187600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4640</th>\n",
       "      <td>3969</td>\n",
       "      <td>married</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>12</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3969</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.142649</td>\n",
       "      <td>518.911072</td>\n",
       "      <td>-161.187600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4641</th>\n",
       "      <td>3289</td>\n",
       "      <td>married</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>12</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3289</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.142584</td>\n",
       "      <td>-156.900024</td>\n",
       "      <td>-152.370422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4642 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      bweight    mmarried  mhisp  fhisp  foreign  alcohol  deadkids  mage  \\\n",
       "0        3459     married      0      0        0        0         0    24   \n",
       "1        3260  notmarried      0      0        1        0         0    20   \n",
       "2        3572     married      0      0        1        0         0    22   \n",
       "3        2948     married      0      0        0        0         0    26   \n",
       "4        2410     married      0      0        0        0         0    20   \n",
       "...       ...         ...    ...    ...      ...      ...       ...   ...   \n",
       "4637     3317  notmarried      0      0        0        0         0    21   \n",
       "4638     3030     married      0      0        0        1         1    30   \n",
       "4639     2950  notmarried      0      0        0        0         0    23   \n",
       "4640     3969     married      0      0        0        0         0    23   \n",
       "4641     3289     married      0      0        0        0         0    22   \n",
       "\n",
       "      medu  fage  ...  birthmonth  lbweight  fbaby  prenatal1     Y Treated  \\\n",
       "0       14    28  ...          12         0     No        Yes  3459       0   \n",
       "1       10     0  ...           7         0     No        Yes  3260       0   \n",
       "2        9    30  ...           3         0     No        Yes  3572       0   \n",
       "3       12    30  ...           1         0     No        Yes  2948       0   \n",
       "4       12    21  ...           3         1    Yes        Yes  2410       0   \n",
       "...    ...   ...  ...         ...       ...    ...        ...   ...     ...   \n",
       "4637    12    24  ...          10         0    Yes         No  3317       1   \n",
       "4638    12    23  ...           2         0     No         No  3030       1   \n",
       "4639    12     0  ...           5         0    Yes        Yes  2950       0   \n",
       "4640    12    25  ...           2         0    Yes        Yes  3969       0   \n",
       "4641    12    26  ...           7         0     No        Yes  3289       0   \n",
       "\n",
       "      casada  m_D_star     m_y_star        cate  \n",
       "0          1 -0.086026    47.203835 -170.004779  \n",
       "1          0 -0.315814   277.930277 -134.736065  \n",
       "2          1 -0.186338   196.895284 -152.370422  \n",
       "3          1 -0.156887  -451.977522 -187.639136  \n",
       "4          1 -0.121311 -1040.633348 -134.736065  \n",
       "...      ...       ...          ...         ...  \n",
       "4637       0  0.683205     5.917001 -143.553243  \n",
       "4638       1  0.478816  -259.794082 -222.907850  \n",
       "4639       0 -0.191829  -242.054511 -161.187600  \n",
       "4640       1 -0.142649   518.911072 -161.187600  \n",
       "4641       1 -0.142584  -156.900024 -152.370422  \n",
       "\n",
       "[4642 rows x 29 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Estimar o modelo\n",
    "final_model_cate = smf.ols(formula='y_res1 ~ D_res1*(mage) + casada + mage + medu + fhisp + mhisp + foreign + alcohol + deadkids + nprenatal + mrace + frace + fage + fedu', data=df).fit()\n",
    "# Calcular o CATE\n",
    "cate_test = df.assign(cate=final_model_cate.predict(df.assign(D_res1=1)) - final_model_cate.predict(df.assign( D_res1=0)))\n",
    "cate_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedimento de Double/Debiased ML com um Modelo Linear Final\n",
    "\n",
    "O procedimento de Double/Debiased ML com um modelo linear final já é muito bom, como podemos ver pela curva acima. Mas, talvez possamos fazer ainda melhor. De fato, este é um procedimento muito geral que podemos entender como um meta-aprendizado (*meta-learner*). Nie e Wager o chamaram de R-Learner, como uma forma de reconhecer o trabalho de Robinson (1988) e enfatizar o papel da residualização.\n",
    "\n",
    "Essa generalização vem do entendimento de que o procedimento de Double/Debiased ML define uma nova função de perda que podemos minimizar da forma que quisermos. A seguir, veremos como fazer isso de maneira muito semelhante ao que vimos antes ao discutir o método de transformação de alvo ou o F-learner.\n",
    "\n",
    "## Double/Debiased ML Não Paramétrico\n",
    "\n",
    "O interessante sobre o Double-ML é que ele nos livra de toda a complicação de aprender os parâmetros de incômodo em um modelo causal. Com isso, podemos concentrar toda a nossa atenção em aprender o parâmetro causal de interesse, seja o ATE ou o CATE. No entanto, com a especificação acima, ainda estávamos usando um modelo linear após a residualização por ML, como modelo causal final. Em nosso exemplo, isso significa que estamos assumindo que o preço impacta as vendas de forma linear. Isso provavelmente está OK para uma faixa pequena de preços, mas sabemos pela teoria microeconômica que isso não é necessariamente o caso. Pode ser que, em preços baixos, um aumento unitário no preço reduza a demanda em 2 unidades. Mas então, em preços mais altos, um aumento unitário no preço pode reduzir a demanda em apenas 1 unidade. Isso não é uma relação linear.\n",
    "\n",
    "Poderíamos usar a teoria microeconômica aqui para especular sobre a forma funcional do desfecho em relação ao tratamento, mas talvez possamos também delegar isso a um modelo de ML. Em outras palavras, deixar a máquina aprender essa forma funcional complicada. Acontece que isso é totalmente possível se fizermos algumas mudanças em nosso algoritmo original de Double/Debiased ML.\n",
    "\n",
    "Primeiro, começamos exatamente como antes, ortogonalizando o tratamento e o desfecho com previsões cruzadas de um modelo de ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ver:\n",
    "\n",
    "https://matteocourthoud.github.io/post/double_ml/ \n",
    "\n",
    "https://econml.azurewebsites.net/spec/estimation/dml.html#\n",
    "https://github.com/py-why/EconML/blob/main/notebooks/CATE%20validation.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DRML - Doubly Robust Machine Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DDML - Double/Debiased Machine Learning - Chernozhukov et al. (2018)\n",
    "\n",
    "Também conhecido como o ML duplo/desviado não paramétrico\n",
    "\n",
    "\n",
    "O bom do Double/Debiased-ML (DDML) é que ele nos livra de todo o incômodo de aprender os parâmetros incômodos em um modelo causal. Com isso, podemos concentrar toda a nossa atenção em aprender o parâmetro causal de interesse, seja o ATE ou o CATE. No entanto, com a especificação acima, ainda estávamos usando um modelo linear após a residualização do ML, como o modelo causal final. Em nosso exemplo, isso significa que estamos assumindo que o preço impacta as vendas linearmente. Isso provavelmente é OK para uma pequena faixa de preços, mas sabemos pela teoria microeconômica que esse não é necessariamente o caso. Pode ser que, a preços baixos, um aumento unitário no preço reduza a demanda em 2 unidades. Mas então, a preços mais altos, um aumento unitário no preço reduza a demanda em apenas 1 unidade. Essa não é uma relação linear.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000265 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 211\n",
      "[LightGBM] [Info] Number of data points in the train set: 4000, number of used features: 3\n",
      "[LightGBM] [Info] Start training from score 6.143650\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000103 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 213\n",
      "[LightGBM] [Info] Number of data points in the train set: 4000, number of used features: 3\n",
      "[LightGBM] [Info] Start training from score 6.116900\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000061 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 210\n",
      "[LightGBM] [Info] Number of data points in the train set: 4000, number of used features: 3\n",
      "[LightGBM] [Info] Start training from score 6.139250\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000023 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 214\n",
      "[LightGBM] [Info] Number of data points in the train set: 4000, number of used features: 3\n",
      "[LightGBM] [Info] Start training from score 6.140200\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000070 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 214\n",
      "[LightGBM] [Info] Number of data points in the train set: 4000, number of used features: 3\n",
      "[LightGBM] [Info] Start training from score 6.138000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "y = \"sales\"\n",
    "T = \"price\"\n",
    "X = [\"temp\", \"weekday\", \"cost\"]\n",
    "\n",
    "debias_m = LGBMRegressor(max_depth=3)\n",
    "\n",
    "train_pred = train.assign(price_res =  train[T] -\n",
    "                          cross_val_predict(debias_m, train[X], train[T], cv=5)\n",
    "                          + train[T].mean()) # add mu_t for visualization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000026 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 211\n",
      "[LightGBM] [Info] Number of data points in the train set: 4000, number of used features: 3\n",
      "[LightGBM] [Info] Start training from score 6.143650\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000077 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 213\n",
      "[LightGBM] [Info] Number of data points in the train set: 4000, number of used features: 3\n",
      "[LightGBM] [Info] Start training from score 6.116900\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000065 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 210\n",
      "[LightGBM] [Info] Number of data points in the train set: 4000, number of used features: 3\n",
      "[LightGBM] [Info] Start training from score 6.139250\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000027 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 214\n",
      "[LightGBM] [Info] Number of data points in the train set: 4000, number of used features: 3\n",
      "[LightGBM] [Info] Start training from score 6.140200\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000137 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 214\n",
      "[LightGBM] [Info] Number of data points in the train set: 4000, number of used features: 3\n",
      "[LightGBM] [Info] Start training from score 6.138000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000290 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 211\n",
      "[LightGBM] [Info] Number of data points in the train set: 4000, number of used features: 3\n",
      "[LightGBM] [Info] Start training from score 200.602750\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000100 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 213\n",
      "[LightGBM] [Info] Number of data points in the train set: 4000, number of used features: 3\n",
      "[LightGBM] [Info] Start training from score 200.822250\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000024 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 210\n",
      "[LightGBM] [Info] Number of data points in the train set: 4000, number of used features: 3\n",
      "[LightGBM] [Info] Start training from score 200.386250\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000081 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 214\n",
      "[LightGBM] [Info] Number of data points in the train set: 4000, number of used features: 3\n",
      "[LightGBM] [Info] Start training from score 200.575250\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000028 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 214\n",
      "[LightGBM] [Info] Number of data points in the train set: 4000, number of used features: 3\n",
      "[LightGBM] [Info] Start training from score 200.698500\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    }
   ],
   "source": [
    "y = \"sales\"\n",
    "T = \"price\"\n",
    "X = [\"temp\", \"weekday\", \"cost\"]\n",
    "\n",
    "debias_m = LGBMRegressor(max_depth=3)\n",
    "denoise_m = LGBMRegressor(max_depth=3)\n",
    "\n",
    "train_pred = train.assign(price_res =  train[T] - cross_val_predict(debias_m, train[X], train[T], cv=5),\n",
    "                          sales_res =  train[y] - cross_val_predict(denoise_m, train[X], train[y], cv=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso é nada menos que incrível, porque agora podemos chamar isso de função de perda causal . O que significa que, se minimizarmos o quadrado dessa perda, estaremos estimando o valor esperado de\n",
    ", que é o CATE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000039 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 219\n",
      "[LightGBM] [Info] Number of data points in the train set: 5000, number of used features: 3\n",
      "[LightGBM] [Info] Start training from score -3.857895\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMRegressor(max_depth=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;LGBMRegressor<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LGBMRegressor(max_depth=3)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LGBMRegressor(max_depth=3)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_final = LGBMRegressor(max_depth=3)\n",
    " \n",
    "# create the weights\n",
    "w = train_pred[\"price_res\"] ** 2 \n",
    " \n",
    "# create the transformed target\n",
    "y_star = (train_pred[\"sales_res\"] / train_pred[\"price_res\"])\n",
    " \n",
    "# use a weighted regression ML model to predict the target with the weights.\n",
    "model_final.fit(X=train[X], y=y_star, sample_weight=w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deveríamos usar DD se temos DML?\n",
    "https://matteocourthoud.github.io/post/did_or_dml/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em suma:\n",
    "\n",
    "O principal objetivo do DML é ajustar e remover a variável de confusão de forma que a variável de interesse (tratamento) e o desfecho (resultado) fiquem \"ortogonais\" ou \"independentes\".\n",
    "\n",
    "Abordagem:\n",
    "\n",
    "DML combina métodos de aprendizado de máquina com técnicas econométricas para estimar efeitos causais.\n",
    "Utiliza a ideia de \"nuisance parameters\" (parâmetros de incômodo) que são ajustados e removidos do modelo principal para reduzir o viés.\n",
    "A técnica geralmente envolve a aplicação de aprendizado de máquina para prever tanto o tratamento quanto o desfecho usando variáveis de confusão, e então os resíduos dessas previsões são utilizados em um segundo estágio para estimar o efeito causal.\n",
    "\n",
    "Implementação:\n",
    "\n",
    "Primeiro Estágio: Aplicar modelos de aprendizado de máquina para prever a variável de tratamento e o resultado.\n",
    "Segundo Estágio: Utilizar os resíduos dessas previsões em um modelo de regressão para estimar o efeito causal.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doubly Robust Machine Learning (DRML)\n",
    "\n",
    "Objetivo:\n",
    "\n",
    "DRML visa fornecer estimativas consistentes do efeito causal mesmo que uma das duas especificações de modelo (do tratamento ou do desfecho) esteja incorreta.\n",
    "Abordagem:\n",
    "\n",
    "A metodologia DRML combina a ponderação de probabilidade inversa (IPW) com métodos de regressão.\n",
    "Essa abordagem é \"duplamente robusta\" porque, para a consistência da estimativa, é necessário que apenas um dos modelos - ou o modelo de probabilidade de tratamento (propensity score) ou o modelo de desfecho (outcome model) - esteja corretamente especificado.\n",
    "Implementação:\n",
    "\n",
    "Modelagem do Propensity Score: Estimar a probabilidade de cada unidade receber o tratamento.\n",
    "Modelagem do Desfecho: Estimar o desfecho esperado condicional ao tratamento e às covariáveis.\n",
    "Combinação dos Modelos: Utilizar uma combinação das estimativas dos dois modelos para obter uma estimativa do efeito causal que é robusta a erros na especificação de um dos modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resumo Comparativo: DML vs DRML\n",
    "\n",
    "* DML: Foca na remoção de viés através de uma abordagem em dois estágios, utilizando previsões de aprendizado de máquina e resíduos. Busca a ortogonalidade entre o tratamento e o desfecho.\n",
    "* DRML: Utiliza a ponderação de probabilidade inversa e a modelagem do desfecho para obter estimativas robustas. É duplamente robusta porque requer que apenas um dos dois modelos esteja corretamente especificado.\n",
    "\n",
    "Ambas as metodologias são úteis em inferência causal e se beneficiam da flexibilidade e poder dos métodos de aprendizado de máquina, mas cada uma tem suas especificidades e contextos de aplicação onde são mais adequadas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
