{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difference-in-Differences with Covariates\n",
    "Prof. Daniel de Abreu Pereira Uhr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conteúdo\n",
    "\n",
    "* Difference-in-Differences com Covariáveis.\n",
    "* Difference in Difference Outcome Regression (Regression Adjustment) - Heckman et al (1997).\n",
    "* IPW Difference in Difference Approach - Abadie (2005)\n",
    "* Doubly Robust Difference in Difference - Sant'Anna e Zhao (2020)\n",
    "\n",
    "## Referências\n",
    "\n",
    "**Principais**\n",
    "\n",
    "* Heckman, J., Ichimura, Smith, J. and Todd, P. 1998. Characterizing Selection Bias Using Experimental Data. Econometrica 66(5): 1017--1098\n",
    "* Abadie, A. 2005. Semiparametric Difference-in-Differences Estimators. Review of Economic Studies 72: 1--19\n",
    "* Sant'Anna e Zhao (2020) Doubly robust difference-in-differences estimators. Journal of Econometrics, Volume 219, Issue 1, November 2020.\n",
    "\n",
    "**Complementares**\n",
    "\n",
    "* Heckman, Ichimura e Todd (1998) - Matching as an Econometric Evaluation Estimator: Evidence from Evaluating a Job Training Programme\n",
    "* Hirano, K., Imbens, G.W. and Ridder, G. 2003.  Efficient Estimation of Average Treatment Effects Using the Estimated Propensity Score. Econometrica 71(4): 1161--1189"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference-in-Differences com Covariáveis\n",
    "\n",
    "Vimos que o DD apresenta as seguintes hipóteses de identificação:\n",
    "\n",
    "* **Parallel Trends**: As tendências dos grupos tratado e controle são paralelas antes do tratamento.\n",
    "* **No anticipation**: O tratamento não pode ser antecipado.\n",
    "* **Distribuição dos dados**: Seja $W_{i}=(Y_{i,2}, Y_{i,1}, D_{i})´$ o vetor das variáveis de resultados e do status do tratamento para a unidade $i$. Nós observamos uma amostra de N *i.i.d.* com $W_{i}$~$F$ para alguma distribuição F (desconhecida) satisfazendo as tendências paralelas. Em outras palavras, os dados observados são amostrados de uma população maior de forma independente e identicamente distribuída, onde cada observação segue a mesma distribuição.\n",
    "\n",
    "E nosso interesse é identificar o **Average Treatment Effect on the Treated** (ATT), para o caso 2x2:\n",
    "\n",
    "$$ \\delta_{ATT}= E[Y_{i,2}(1)-Y_{i,2}(0)|D_{i}=1] $$\n",
    "\n",
    "Sob as hipóteses de identificação, o estimador de DD 2x2 é dado por:\n",
    "\n",
    "$$ \\hat{\\delta}_{ATT}^{2x2} = (\\overline{Y}_{i,t=2}(1) - \\overline{Y}_{i,t=1}(1)) - (\\overline{Y}_{i,t=2}(0) - \\overline{Y}_{i,t=1}(0)) $$\n",
    "\n",
    "Vamos representar em termos de esperanças condicionais (assumindo uma amostragem de uma população grande):\n",
    "\n",
    "$$ \\hat{\\delta}_{ATT}^{2x2} = (E[Y_{i,t=2}(1)|D_{i}=1] - E[Y_{i,t=1}(1)|D_{i}=1]) - (E[Y_{i,t=2}(0)|D_{i}=0] - E[Y_{i,t=1}(0)|D_{i}=0]) $$\n",
    "\n",
    "\n",
    "**Motivação para adicionar covariáveis**\n",
    "\n",
    "Para muitos pesquisadores, a hipótese de **Parallel Trends** não é plausível em muitos casos, mas poderia ser ao condicionarmos em algumas covariáveis. Por exemplo, se a variável de resultado de interesse são os salários, o grupo de tratamento e controle diferem em níveis de escolaridade, e as tendências para os níveis de educação que afetam os salários diferem entre os trabalhadores. Então, o condicionamento poderia ser plausível.\n",
    "\n",
    "Nesse contexto as hipóteses de identificação são alteradas e ampliadas:\n",
    "\n",
    "* 1: Conditional Parallel Trends (porque condicionamos nas covariáveis)\n",
    "* 2: No anticipation\n",
    "* 3: Distribuição dos dados (Segue conforme anteriormente, considerando a estrutura de Panel-Data, ou repeated Cross-secion data, satisfazendo as tendências paralelas condicionais)\n",
    "* 4: Common Suport (Overlap) Ao menos uma fração da população que é tratada e para cada valor de X há ao menos uma chance de uma unidade ser não tratada.\n",
    "* 5: Efeito Homogêneo do Tratameto em X\n",
    "* 6: Não há tendências específicas em X em ambos os grupos tratado e controle\n",
    "\n",
    "Observação: Convem salientar que **a suposição de tendência paralela condicional não é nem mais forte nem mais fraca do que a tendência paralela incondicional.** A tendência paralela condicional não implica tendência paralela incondicional e a tendência paralela incondicional não implica tendência paralela condicional;\n",
    "\n",
    "Considerando $X_{i}$ um vetor de covariáveis que **Não varia no tempo**:\n",
    "\n",
    "$$ \\hat{\\delta}_{ATT}^{2x2} = (E[Y_{i,t=2}|D_{i}=1, X_{i}] - E[Y_{i,t=1}|D_{i}=1, X_{i}]) - (E[Y_{i,t=2}|D_{i}=0, X_{i}] - E[Y_{i,t=1}|D_{i}=0, X_{i}]) $$\n",
    "\n",
    "\n",
    "Para identificar o efeito causal, os pesquisadores aplicados costumavam utilizar a especificação TWFE, apenas com a adição das covariáveis na regressão:\n",
    "\n",
    "$$ Y_{i,t} = \\alpha + \\beta_{1} T_{t} + \\beta_{2}D_{i} + \\delta (T_{t}.D_{i}) + \\gamma X_{i} + \\epsilon_{i,t} $$\n",
    "\n",
    "Ao adicionar as covariáveis na regressão significa que estamos **impondo uma estrutura paramétrica** para a relação entre as covariáveis e o resultado (é uma hipótese de estrutura paramétrica da relação). Essa regressão identifica o efeito causal se isso corresponde ao verdadeiro modelo que gera os resultados potenciais. O efeito do tratamento é **constante e aditivo**.\n",
    "\n",
    "Entretanto, mesmo condicionando nas covariáveis, o modelo não permite que diferentes grupos tenham trajetórias diferentes ao longo do tempo. Ou seja, as trajetórias são as mesmas para todos os grupos. Mas a razão pela qual se desejava incluir covariáveis no modelo era justamente para permitir que diferentes grupos tivessem trajetórias distintas ao longo do tempo. No entanto, a restrição do modelo impede que essa variação seja capturada, o que contradiz o objetivo original de incluir as covariáveis.\n",
    "\n",
    "Vamos tomar as expectativas condicionais do TWFE proposto:\n",
    "\n",
    "* Tratados (Post e Pre):\n",
    "$$E[Y_{1,1}|D_{i}=1] = \\alpha + \\beta_{1} + \\beta_{2} + \\delta + \\gamma X_{11}$$\n",
    "$$E[Y_{0,1}|D_{i}=1] = \\alpha + \\beta_{2} + \\gamma X_{10}$$\n",
    "* Controles (Post e Pre):\n",
    "$$E[Y_{1,0}|D_{i}=0] = \\alpha + \\beta_{1} + \\gamma X_{01}$$\n",
    "$$E[Y_{0,0}|D_{i}=0] = \\alpha + \\gamma X_{00}$$\n",
    "\n",
    "Tomando o DD:\n",
    "\n",
    "$$ \\hat{\\delta}_{ATT}^{2x2} = ( (\\alpha + \\beta_{1} + \\beta_{2} + \\delta + \\gamma X_{11}) - (\\alpha + \\beta_{2} + \\gamma X_{10}) ) - ( (\\alpha + \\beta_{1} + \\gamma X_{01}) - (\\alpha + \\gamma X_{00}) ) $$\n",
    "\n",
    "$$ \\hat{\\delta}_{ATT}^{2x2} = ( \\beta_{1} + \\delta + \\gamma X_{11} - \\gamma X_{10} ) - ( \\beta_{1} + \\gamma X_{01} - \\gamma X_{00} ) $$\n",
    "$$ \\hat{\\delta}_{ATT}^{2x2} = \\delta + ( \\gamma X_{11} - \\gamma X_{10} ) - ( \\gamma X_{01} - \\gamma X_{00} ) $$\n",
    "\n",
    "\n",
    "A segunda parte da igualdade requer que as tendências de cada X do grupo tratado seja igual a tendência das variáveis X do grupo de controle. (hipóteses 5 e 6). Por isso que, normalmente o TWFE não irá identificar o efeito causal ($\\delta$) corretamente.\n",
    "\n",
    "Vejamos como a literatura propõe a inclusão de covariáveis no modelo de DD com menos hipóteses restritivas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimação do DD sob Hipótese de Tendências Paralelas Condicionais\n",
    "\n",
    "Funciona se ambas as **unidades tratadas e de controle tiverem aproximadamente a mesma distribuição de covariáveis ​​(sobreposição forte)** e se o **efeito do tratamento for homogêneo**.\n",
    "\n",
    "Em termos de estratégia de identificação, a literatura se desenvolveu apresentando as seguintes abordagens:\n",
    "* Outcome Regression DD (Regression Adjustment DD - Heckman et al. 1997, 1998)\n",
    "* Propensity score com DD (Abadie, 2005)\n",
    "* Doubly Robust DD (Sant'Anna e Zhao, 2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outcome Regression DD\n",
    "\n",
    "Uma forma de identificar o efeito causal através do método de Diferença em Diferenças e incorporar covariáveis se dá através do Outcome Regression. A ideia é realizar a diferença entre as médias dos resultados potenciais para o grupo tratado e controle, entretanto, utilizar como ajuste do contrafactual a trajetória do grupo de controle com a distribuição de covariáveis do grupo tratado.\n",
    "\n",
    "1. **Modelo de Regressão Linear:**\n",
    "\n",
    "   $$ Y_{i,t} = \\alpha + \\beta D_{i,t} + \\gamma X_{i} + \\epsilon_{i,t} $$\n",
    "\n",
    "   onde $ Y $ é a variável dependente, $ D $ é a variável indicadora de tratamento (1 se o tratamento foi aplicado, 0 caso contrário), $ X $ são as covariáveis constantes observáveis, e $ \\epsilon $ é o termo de erro.\n",
    "\n",
    "2. **Expectativas Condicionais:**\n",
    "   \n",
    "   Definimos os resultados da expectativa condicional\n",
    "\n",
    "   $$ \\mu_{0,1}(X) = E[Y | X, D=0, t=1] $$\n",
    "   $$ \\mu_{0,2}(X) = E[Y | X, D=0, t=2] $$\n",
    "   \n",
    "   Então, $\\mu_{0,1}(X) $ e $ \\mu_{0,2}(X) $ representam as médias dos resultados potenciais para os não tratados dado suas características observáveis $ X $.\n",
    "\n",
    "3. **DD e as Expectativas Condicionais:**\n",
    "   \n",
    "   Retomando a equação do DD, temos:\n",
    "\n",
    "   $$ \\beta^{DD} = (E[Y|X, D=1, t=2] - E[Y|X, D=1, t=1]) - (E[Y|X, D=0, t=2] - E[Y|X, D=0, t=1]) $$\n",
    "\n",
    "4. **Substituição das Definições:**\n",
    "\n",
    "   A ideia dos autores é construir um contrafactual adequado, que considere a trajetória do grupo de controle com a distribuição de covariáveis do grupo tratado. Logo:\n",
    "\n",
    "   $$ \\beta^{OR-DD} = (E[Y|X, D=1, t=2] - E[Y|X, D=1, t=1]) - (\\mu_{02}(X) - \\mu_{01}(X) ) $$\n",
    "\n",
    "  Considerando que queremos construir o contrafactual considerando a trajetória do grupo de controle com a distribuição de covariáveis do grupo tratado ($X^{T}$), obtemos:\n",
    "\n",
    "   $$ \\hat{\\beta}_{ATT}^{OR-DD} = (E[Y|X, D=1, t=2] - E[Y|X, D=1, t=1]) - [\\mu_{1}(X^{T}) - \\mu_{0}(X^{T})]  $$\n",
    "\n",
    "   Simplificando a notação, com $ \\bar{Y}_{1,1} $ representando $ E[Y|X, D=1, t=1] $ e $ \\bar{Y}_{1,2} $ representando $ E[Y|X, D=1, t=2] $, temos:\n",
    "\n",
    "   $$ \\hat{\\beta}_{ATT}^{OR-DD} = (\\bar{Y}_{1,2} - \\bar{Y}_{1,1}) - [\\mu_{1}(X^{T}) - \\mu_{0}(X^{T})]  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pacotes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import plotnine as p\n",
    "import statsmodels.api as sm\n",
    "import linearmodels as lm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame\n",
    "df = pd.read_stata(\"http://fmwww.bc.edu/repec/bocode/c/CardKrueger1994.dta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Há problemas na base que precisam ser ajustados\n",
    "df.loc[(df['id'] == 407) & (df['kfc'] == 1), 'id'] = 408\n",
    "# Crie a variável 'tratado' com valor inicial de 0\n",
    "df['Treated'] = 0\n",
    "# Recodifique 'tratado' para 1 se 'treated' for igual a 1\n",
    "df.loc[df['treated'] == 'NJ', 'Treated'] = 1\n",
    "# Crie a variável 'effect' como o produto entre 'tratado' e 't'\n",
    "df['Effect'] = df['Treated'] * df['t']\n",
    "# Remover lojas sem informação em ambos os anos\n",
    "df['cont'] = df['fte'].notna().groupby(df['id']).cumsum()\n",
    "df['media'] = df.groupby('id')['cont'].transform('mean')\n",
    "df = df[df['media'] == 1.5]\n",
    "df['y'] = df['fte']\n",
    "\n",
    "df['pre'] = 0\n",
    "df.loc[df['t'] == 0, 'pre'] = 1\n",
    "df['pos'] = 0\n",
    "df.loc[df['t'] == 1, 'pos'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# salvando a base em csv para usar no R posteriormente.\n",
    "df.to_csv('card_krueger.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.008\n",
      "Model:                            OLS   Adj. R-squared:                  0.005\n",
      "Method:                 Least Squares   F-statistic:                     2.195\n",
      "Date:                Sat, 17 Aug 2024   Prob (F-statistic):             0.0873\n",
      "Time:                        17:02:55   Log-Likelihood:                -2831.8\n",
      "No. Observations:                 782   AIC:                             5672.\n",
      "Df Residuals:                     778   BIC:                             5690.\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     20.0132      1.040     19.238      0.000      17.971      22.055\n",
      "Treated       -2.9663      1.159     -2.559      0.011      -5.242      -0.691\n",
      "t             -2.4901      1.471     -1.693      0.091      -5.378       0.398\n",
      "Effect         2.9425      1.639      1.795      0.073      -0.275       6.160\n",
      "==============================================================================\n",
      "Omnibus:                      240.424   Durbin-Watson:                   1.389\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              952.495\n",
      "Skew:                           1.395   Prob(JB):                    1.47e-207\n",
      "Kurtosis:                       7.631   Cond. No.                         11.3\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# regressão DD canônico sem covariáveis:\n",
    "DD1 = smf.ols('y ~ Treated + t + Effect', data=df).fit()\n",
    "print(DD1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.190\n",
      "Model:                            OLS   Adj. R-squared:                  0.184\n",
      "Method:                 Least Squares   F-statistic:                     30.26\n",
      "Date:                Sat, 17 Aug 2024   Prob (F-statistic):           1.06e-32\n",
      "Time:                        17:03:03   Log-Likelihood:                -2752.8\n",
      "No. Observations:                 782   AIC:                             5520.\n",
      "Df Residuals:                     775   BIC:                             5552.\n",
      "Df Model:                           6                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     21.2079      1.181     17.950      0.000      18.889      23.527\n",
      "Treated       -2.3752      1.051     -2.259      0.024      -4.439      -0.312\n",
      "t             -2.4901      1.332     -1.869      0.062      -5.106       0.125\n",
      "Effect         2.9425      1.485      1.982      0.048       0.028       5.857\n",
      "bk             1.0109      0.919      1.100      0.272      -0.793       2.815\n",
      "kfc           -9.1680      1.031     -8.892      0.000     -11.192      -7.144\n",
      "roys          -0.8918      0.997     -0.894      0.371      -2.849       1.065\n",
      "==============================================================================\n",
      "Omnibus:                      298.310   Durbin-Watson:                   1.556\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1795.640\n",
      "Skew:                           1.606   Prob(JB):                         0.00\n",
      "Kurtosis:                       9.693   Cond. No.                         12.1\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# regressão DD canônico com covariáveis:\n",
    "DD2 = smf.ols('y ~ Treated + t + Effect + bk + kfc + roys', data=df).fit()\n",
    "print(DD2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lembre que sob a hipótese de tendências paralelas condicionais, o estimador de DD apenas adicionando as covariáveis pode apresentar viés se as tendências não forem paralelas nas prórias covariáveis. Os resultados aqui foram de $2.9425$ em ambos modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos realizar o OR-DD proposto por Heckman et al. (1997) para o dataset de exemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.stats import t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outcome Regression DiD (ATT): 2.6757034544143536\n"
     ]
    }
   ],
   "source": [
    "def OR_DD(df, X_cols, T_col, Y_col, Time_col, pre_time, post_time):\n",
    "    # Separar dados por grupo e período\n",
    "    df_pre = df[df[Time_col] == pre_time]\n",
    "    df_post = df[df[Time_col] == post_time]\n",
    "\n",
    "    X_pre_treated = df_pre[df_pre[T_col] == 1][X_cols]\n",
    "    Y_pre_treated = df_pre[df_pre[T_col] == 1][Y_col]\n",
    "    X_post_treated = df_post[df_post[T_col] == 1][X_cols]\n",
    "    Y_post_treated = df_post[df_post[T_col] == 1][Y_col]\n",
    "\n",
    "    X_pre_control = df_pre[df_pre[T_col] == 0][X_cols]\n",
    "    Y_pre_control = df_pre[df_pre[T_col] == 0][Y_col]\n",
    "    X_post_control = df_post[df_post[T_col] == 0][X_cols]\n",
    "    Y_post_control = df_post[df_post[T_col] == 0][Y_col]\n",
    "\n",
    "    # Ajustar modelos de regressão linear para os períodos pré e pós\n",
    "    model_pre_control = LinearRegression().fit(X_pre_control, Y_pre_control)\n",
    "    model_post_control = LinearRegression().fit(X_post_control, Y_post_control)\n",
    "\n",
    "    # Previsões para o grupo controle nos períodos pré e pós\n",
    "    mu0_X_pre = model_pre_control.predict(X_pre_treated)\n",
    "    mu0_X_post = model_post_control.predict(X_post_treated)\n",
    "\n",
    "    # Calcular a Diferença em Diferenças\n",
    "    ORdid = (Y_post_treated.mean() - Y_pre_treated.mean()) - (mu0_X_post.mean() - mu0_X_pre.mean()) \n",
    "\n",
    "    return ORdid\n",
    "\n",
    "# Exemplo de uso\n",
    "T_col = 'Treated'\n",
    "Y_col = 'y'\n",
    "X_cols = ['bk', 'kfc', 'roys']\n",
    "Time_col = 't'\n",
    "pre_time = 0\n",
    "post_time = 1\n",
    "\n",
    "result_OR_DiD = OR_DD(df, X_cols, T_col, Y_col, Time_col, pre_time, post_time)\n",
    "print(\"Outcome Regression DiD (ATT):\", result_OR_DiD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outcome Regression DiD (ATT): 2.6757034544143536\n",
      "Erro Padrão Analítico: 1.4986928085678337\n",
      "t-score: 1.785358172880861\n",
      "p-value: 0.07459269802317303\n",
      "Intervalo de Confiança 95%: (-0.2617344503786003, 5.613141359207308)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy import stats\n",
    "\n",
    "def OR_DD_with_SE(df, X_cols, T_col, Y_col, Time_col, pre_time, post_time):\n",
    "    # Separar dados por grupo e período\n",
    "    df_pre = df[df[Time_col] == pre_time]\n",
    "    df_post = df[df[Time_col] == post_time]\n",
    "\n",
    "    X_pre_treated = df_pre[df_pre[T_col] == 1][X_cols]\n",
    "    Y_pre_treated = df_pre[df_pre[T_col] == 1][Y_col]\n",
    "    X_post_treated = df_post[df_post[T_col] == 1][X_cols]\n",
    "    Y_post_treated = df_post[df_post[T_col] == 1][Y_col]\n",
    "\n",
    "    X_pre_control = df_pre[df_pre[T_col] == 0][X_cols]\n",
    "    Y_pre_control = df_pre[df_pre[T_col] == 0][Y_col]\n",
    "    X_post_control = df_post[df_post[T_col] == 0][X_cols]\n",
    "    Y_post_control = df_post[df_post[T_col] == 0][Y_col]\n",
    "\n",
    "    # Ajustar modelos de regressão linear para os períodos pré e pós\n",
    "    model_pre_control = LinearRegression().fit(X_pre_control, Y_pre_control)\n",
    "    model_post_control = LinearRegression().fit(X_post_control, Y_post_control)\n",
    "\n",
    "    # Previsões para o grupo controle nos períodos pré e pós\n",
    "    mu0_X_pre = model_pre_control.predict(X_pre_treated)\n",
    "    mu0_X_post = model_post_control.predict(X_post_treated)\n",
    "\n",
    "    # Calcular a Diferença em Diferenças\n",
    "    ORdid = (Y_post_treated.mean() - Y_pre_treated.mean()) - (mu0_X_post.mean() - mu0_X_pre.mean())\n",
    "\n",
    "    # Calcular resíduos\n",
    "    res_pre_control = Y_pre_control - model_pre_control.predict(X_pre_control)\n",
    "    res_post_control = Y_post_control - model_post_control.predict(X_post_control)\n",
    "\n",
    "    # Variância dos resíduos\n",
    "    var_res_pre_control = np.var(res_pre_control, ddof=1)\n",
    "    var_res_post_control = np.var(res_post_control, ddof=1)\n",
    "\n",
    "    # Tamanho das amostras\n",
    "    n_pre_treated = len(Y_pre_treated)\n",
    "    n_post_treated = len(Y_post_treated)\n",
    "    n_pre_control = len(Y_pre_control)\n",
    "    n_post_control = len(Y_post_control)\n",
    "\n",
    "    # Calcular erro padrão\n",
    "    SE_ORdid = np.sqrt((var_res_post_control / n_post_treated) +\n",
    "                       (var_res_pre_control / n_pre_treated) +\n",
    "                       (var_res_post_control / n_post_control) +\n",
    "                       (var_res_pre_control / n_pre_control))\n",
    "\n",
    "    # Calcular t-score\n",
    "    t_score = ORdid / SE_ORdid\n",
    "\n",
    "    # Calcular p-value (bilateral)\n",
    "    p_value = 2 * (1 - stats.t.cdf(np.abs(t_score), df=n_pre_treated + n_post_treated + n_pre_control + n_post_control - 4))\n",
    "\n",
    "    # Calcular intervalo de confiança de 95%\n",
    "    confidence_interval = (\n",
    "        ORdid - 1.96 * SE_ORdid,\n",
    "        ORdid + 1.96 * SE_ORdid\n",
    "    )\n",
    "\n",
    "    return ORdid, SE_ORdid, t_score, p_value, confidence_interval\n",
    "\n",
    "# Exemplo de uso\n",
    "result_OR_DiD, SE_OR_DiD, t_score, p_value, confidence_interval = OR_DD_with_SE(df, X_cols, T_col, Y_col, Time_col, pre_time, post_time)\n",
    "print(\"Outcome Regression DiD (ATT):\", result_OR_DiD)\n",
    "print(\"Erro Padrão Analítico:\", SE_OR_DiD)\n",
    "print(\"t-score:\", t_score)\n",
    "print(\"p-value:\", p_value)\n",
    "print(\"Intervalo de Confiança 95%:\", confidence_interval)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outcome Regression DiD (ATT): 2.6778951284167314\n",
      "Erro padrão (bootstrap): 1.4614826978902211\n",
      "Intervalo de confiança 95% (bootstrap): [-0.12008934059629131, 5.615338825397242]\n",
      "t-score (bootstrap): 1.8323139454764046\n",
      "p-valor (bootstrap): 0.06691209100604145\n"
     ]
    }
   ],
   "source": [
    "# Função para calcular o bootstrap\n",
    "def bootstrap_OR_DD(df, X_cols, T_col, Y_col, Time_col, pre_time, post_time, n_iterations):\n",
    "    np.random.seed(88)\n",
    "    \n",
    "    def calculate_did(sample_df):\n",
    "        return OR_DD(sample_df, X_cols, T_col, Y_col, Time_col, pre_time, post_time)\n",
    "    \n",
    "    results = Parallel(n_jobs=-1)(delayed(calculate_did)(df.sample(frac=1, replace=True)) for _ in range(n_iterations))\n",
    "    \n",
    "    return np.array(results)\n",
    "\n",
    "# Definir o número de amostras do bootstrap\n",
    "bootstrap_samples = 40000\n",
    "\n",
    "# Executar o bootstrap para calcular as estimativas do DiD\n",
    "did_bootstrap = bootstrap_OR_DD(df, X_cols, T_col, Y_col, Time_col, pre_time, post_time, bootstrap_samples)\n",
    "\n",
    "# Calcular a média e o erro padrão das estimativas do DiD\n",
    "did_mean_bootstrap = np.mean(did_bootstrap)\n",
    "did_std_bootstrap = np.std(did_bootstrap)\n",
    "\n",
    "# Calcular o intervalo de confiança de 95% usando percentis\n",
    "ci_lower_did_bootstrap = np.percentile(did_bootstrap, 2.5)\n",
    "ci_upper_did_bootstrap = np.percentile(did_bootstrap, 97.5)\n",
    "\n",
    "# Calcular os z-scores e p-valores\n",
    "t_score_did_bootstrap = did_mean_bootstrap / did_std_bootstrap\n",
    "p_value_did_bootstrap = 2 * (1 - t.cdf(abs(t_score_did_bootstrap), df=len(did_bootstrap)-1))\n",
    "\n",
    "# Exibir os resultados\n",
    "print(\"Outcome Regression DiD (ATT):\", did_mean_bootstrap)\n",
    "print(\"Erro padrão (bootstrap):\", did_std_bootstrap)\n",
    "print(\"Intervalo de confiança 95% (bootstrap): [{}, {}]\".format(ci_lower_did_bootstrap, ci_upper_did_bootstrap))\n",
    "print(\"t-score (bootstrap):\", t_score_did_bootstrap)\n",
    "print(\"p-valor (bootstrap):\", p_value_did_bootstrap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  IPW Difference in Difference Approach - Abadie (2005) \n",
    "\n",
    "A abordagem proposta por Abadie (2005) é um estimador de diferenças em diferenças Semiparametrico. A abordagem de Abadie (2005) utiliza técnicas de *matching* para criar grupos de comparação potencialmente mais adequados, onde unidades de tratamento e controle são pareadas com base em características observáveis. Isso ajuda a reduzir o viés de seleção por características observáveis e aumenta a validade causal da estimativa.\n",
    "\n",
    "\n",
    "$$ \\delta^{IPW} = \\frac{1}{E_{N}[D]}E\\left [\\frac{D-\\hat{p}(X)}{1-\\hat{p}(X)}(Y_{1}-Y_{0})\\right ]  $$\n",
    "\n",
    "\n",
    "Onde $\\hat{p}(X)$  é um estimador para o verdadeiro escore de propensão (probabilidade do indivíduo ser tratado baseado nas características observáveis). O qual reduz a dimensão de X em um escalar.\n",
    "\n",
    "* $E_{N}[D]$ é a proporção média de unidades que receberam o tratamento na amostra\n",
    "* $Y_{1}$ e $Y_{0}$ são as médias das variáveis de resultado dos tratados e não tratados **no período de tratamento**\n",
    "\n",
    "Pode ser reescrito como:\n",
    "\n",
    "$$ \\delta^{IPW-DD} = \\frac{1}{E_{N}[D]} E\\left [(\\bar{Y}_{1,2} - \\bar{Y}_{1,1}) - \\frac{(1-D)\\hat{p}(X)}{1-\\hat{p}(X)}(\\bar{Y}_{0,2} - \\bar{Y}_{0,1}) \\right ]  $$\n",
    "\n",
    "A ideia da ponderação é atribuir mais peso para as observações nos grupos de controle que \"parecem\" mais com as unidades tratadas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.489622\n",
      "         Iterations 6\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                Treated   No. Observations:                  782\n",
      "Model:                          Logit   Df Residuals:                      778\n",
      "Method:                           MLE   Df Model:                            3\n",
      "Date:                Sat, 17 Aug 2024   Pseudo R-squ.:                0.005849\n",
      "Time:                        17:17:32   Log-Likelihood:                -382.88\n",
      "converged:                       True   LL-Null:                       -385.14\n",
      "Covariance Type:            nonrobust   LLR p-value:                    0.2118\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          1.1239      0.226      4.979      0.000       0.681       1.566\n",
      "bk             0.2095      0.264      0.795      0.427      -0.307       0.726\n",
      "kfc            0.6107      0.316      1.931      0.053      -0.009       1.230\n",
      "roys           0.3996      0.295      1.356      0.175      -0.178       0.977\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "# Propensity Score Weighting\n",
    "D = df['Treated']\n",
    "X = df[['bk', 'kfc', 'roys']]\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "reg_logit = sm.Logit(D, X).fit()\n",
    "df['peso_logit'] = reg_logit.predict()\n",
    "\n",
    "print(reg_logit.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar a medida de média dos tratados 'ED'\n",
    "df['ED'] = df['Treated'].mean()\n",
    "ED = df['ED'].mean()\n",
    "\n",
    "# Criando os pesos e adicionando ao DataFrame\n",
    "df['W_ATT'] = (df['peso_logit'] / (1 - df['peso_logit']))/ED\n",
    "df.loc[df['Treated'] == 1, 'W_ATT'] = 1/ED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            WLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.008\n",
      "Model:                            WLS   Adj. R-squared:                  0.004\n",
      "Method:                 Least Squares   F-statistic:                     2.143\n",
      "Date:                Sat, 17 Aug 2024   Prob (F-statistic):             0.0934\n",
      "Time:                        17:17:37   Log-Likelihood:                -2942.2\n",
      "No. Observations:                 782   AIC:                             5892.\n",
      "Df Residuals:                     778   BIC:                             5911.\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         18.9768      0.674     28.135      0.000      17.653      20.301\n",
      "Treated       -1.9299      0.954     -2.023      0.043      -3.802      -0.057\n",
      "t             -2.2233      0.954     -2.331      0.020      -4.096      -0.351\n",
      "Effect         2.6757      1.349      1.984      0.048       0.028       5.324\n",
      "==============================================================================\n",
      "Omnibus:                      327.394   Durbin-Watson:                   1.373\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             2743.448\n",
      "Skew:                           1.672   Prob(JB):                         0.00\n",
      "Kurtosis:                      11.545   Cond. No.                         6.85\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# Aplicando regressão ponderada\n",
    "Y1 = df[['y']]\n",
    "X1 = df[['Treated', 't', 'Effect']]\n",
    "X1 = sm.add_constant(X1)\n",
    "\n",
    "dd_ipw = sm.WLS(Y1, X1, weights=df['W_ATT']).fit()\n",
    "\n",
    "# Imprimindo o resumo\n",
    "print(dd_ipw.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O resultado para o DD-IPW foi de $2.6757$. O mesmo econtrado anteriormente pelo OR-DD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doubly Robust DD (DRDID)\n",
    "\n",
    "O modelo Doubly Robust Difference in Differences (DRDID) de Sant'Anna e Zhao (2020) é uma extensão do método de diferenças em diferenças (DD). Ele usa **uma abordagem duplamente robusta**, que combina os dois modelos vistos anteriormente: um modelo de Outcome Regression e um modelo de probabilidade de receber o tratamento.\n",
    "\n",
    "O modelo DRDID combina esses dois modelos para produzir estimativas do efeito causal que são consistentes mesmo que um dos modelos seja mal especificado. Essa abordagem pode melhorar a precisão da estimativa do efeito causal em comparação com o método DID padrão, especialmente quando há variáveis de controle que afetam tanto a probabilidade de tratamento quanto o resultado de interesse.\n",
    "\n",
    "$$  \\beta^{DRDID}= E\\left[ \\left ( \\frac{D}{E[D]} - \\frac{\\frac{p(X)(1-D)}{(1-p(X))}}{E[\\frac{p(X)(1-D)}{(1-p(X))}]} \\right ) (\\Delta Y - \\mu_{0,\\Delta (X)} )   \\right ]  $$\n",
    "\n",
    "Podemos reescrever a equação acima como:\n",
    "\n",
    "$$ \\beta^{DRDID}= E\\left[ \\frac{D}{E[D]}(\\bar{Y}_{1,2} - \\bar{Y}_{1,1}) - \\frac{\\frac{p(X)(1-D)}{(1-p(X))}}{E[\\frac{p(X)(1-D)}{(1-p(X))}]} (\\mu_{1}(X^{T}) - \\mu_{0}(X^{T})) \\right ]  $$\n",
    "\n",
    "\n",
    "Observe como o modelo controla para $X$ você está ponderando o ajuste resultados usando o escore de propensão. A razão pela qual você controla $X$ duas vezes é porque você não sabe qual modelo está certo. DRDiD libera você de fazer uma escolha sem fazer você pagar muito por isso.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplamente Robusto DiD (ATT): 2.6757034544143536\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "\n",
    "def DD_DR(df, X_cols, T_col, Y_col, Time_col, pre_time, post_time):\n",
    "    # Separar dados por grupo e período\n",
    "    df_pre = df[df[Time_col] == pre_time]\n",
    "    df_post = df[df[Time_col] == post_time]\n",
    "\n",
    "    X_pre_treated = df_pre[df_pre[T_col] == 1][X_cols]\n",
    "    Y_pre_treated = df_pre[df_pre[T_col] == 1][Y_col]\n",
    "    X_post_treated = df_post[df_post[T_col] == 1][X_cols]\n",
    "    Y_post_treated = df_post[df_post[T_col] == 1][Y_col]\n",
    "\n",
    "    X_pre_control = df_pre[df_pre[T_col] == 0][X_cols]\n",
    "    Y_pre_control = df_pre[df_pre[T_col] == 0][Y_col]\n",
    "    X_post_control = df_post[df_post[T_col] == 0][X_cols]\n",
    "    Y_post_control = df_post[df_post[T_col] == 0][Y_col]\n",
    "    \n",
    "    # Ajustar modelos de regressão linear para os períodos pré e pós\n",
    "    model_pre_control = LinearRegression().fit(X_pre_control, Y_pre_control)\n",
    "    model_post_control = LinearRegression().fit(X_post_control, Y_post_control)\n",
    "\n",
    "    # Previsões para o grupo controle nos períodos pré e pós\n",
    "    mu0_X_pre = model_pre_control.predict(X_pre_treated)\n",
    "    mu0_X_post = model_post_control.predict(X_post_treated)\n",
    "    \n",
    "    X_pre = df_pre[X_cols]\n",
    "    Y_pre = df_pre[Y_col]\n",
    "    T_pre = df_pre[T_col]\n",
    "        \n",
    "    # Estimativa da probabilidade de tratamento p(X) no período pré\n",
    "    model_logistic = LogisticRegression(solver='liblinear').fit(X_pre, T_pre)\n",
    "    p_X_pre = model_logistic.predict_proba(X_pre)[:, 1]\n",
    "    \n",
    "    # Criar os pesos para o período pré\n",
    "    weight_treated_pre = T_pre / T_pre.mean()\n",
    "    weight_control_pre = ((1 - p_X_pre) / p_X_pre) / ((1 - p_X_pre) / p_X_pre).mean()\n",
    "    \n",
    "    # Calcular a Diferença em Diferenças com os pesos\n",
    "    term_treated = (Y_post_treated.mean() - Y_pre_treated.mean()) * weight_treated_pre.mean()\n",
    "    term_control = (mu0_X_post.mean() - mu0_X_pre.mean()) * weight_control_pre.mean()\n",
    "\n",
    "    DR_DiD = term_treated - term_control \n",
    "\n",
    "    return DR_DiD\n",
    "\n",
    "# Exemplo de uso\n",
    "T_col = 'Treated'\n",
    "Y_col = 'y'\n",
    "X_cols = ['bk', 'kfc', 'roys']\n",
    "Time_col = 't'\n",
    "pre_time = 0\n",
    "post_time = 1\n",
    "\n",
    "\n",
    "result_DD_DR = DD_DR(df, X_cols, T_col, Y_col, Time_col, pre_time, post_time)\n",
    "print(\"Duplamente Robusto DiD (ATT):\", result_DD_DR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplamente Robusto DiD (ATT): 2.6757034544143536\n",
      "Erro Padrão Analítico: 1.4988259975281915\n",
      "t-score: 1.7851995220439365\n",
      "p-value: 0.07461844182875366\n",
      "Intervalo de Confiança 95%: (-0.2619955007409014, 5.613402409569609)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from scipy import stats\n",
    "\n",
    "def DD_DR_with_SE(df, X_cols, T_col, Y_col, Time_col, pre_time, post_time):\n",
    "    # Separar dados por grupo e período\n",
    "    df_pre = df[df[Time_col] == pre_time]\n",
    "    df_post = df[df[Time_col] == post_time]\n",
    "\n",
    "    X_pre_treated = df_pre[df_pre[T_col] == 1][X_cols]\n",
    "    Y_pre_treated = df_pre[df_pre[T_col] == 1][Y_col]\n",
    "    X_post_treated = df_post[df_post[T_col] == 1][X_cols]\n",
    "    Y_post_treated = df_post[df_post[T_col] == 1][Y_col]\n",
    "\n",
    "    X_pre_control = df_pre[df_pre[T_col] == 0][X_cols]\n",
    "    Y_pre_control = df_pre[df_pre[T_col] == 0][Y_col]\n",
    "    X_post_control = df_post[df_post[T_col] == 0][X_cols]\n",
    "    Y_post_control = df_post[df_post[T_col] == 0][Y_col]\n",
    "    \n",
    "    # Ajustar modelos de regressão linear para os períodos pré e pós\n",
    "    model_pre_control = LinearRegression().fit(X_pre_control, Y_pre_control)\n",
    "    model_post_control = LinearRegression().fit(X_post_control, Y_post_control)\n",
    "\n",
    "    # Previsões para o grupo controle nos períodos pré e pós\n",
    "    mu0_X_pre = model_pre_control.predict(X_pre_treated)\n",
    "    mu0_X_post = model_post_control.predict(X_post_treated)\n",
    "    \n",
    "    X_pre = df_pre[X_cols]\n",
    "    Y_pre = df_pre[Y_col]\n",
    "    T_pre = df_pre[T_col]\n",
    "        \n",
    "    # Estimativa da probabilidade de tratamento p(X) no período pré\n",
    "    model_logistic = LogisticRegression(solver='liblinear').fit(X_pre, T_pre)\n",
    "    p_X_pre = model_logistic.predict_proba(X_pre)[:, 1]\n",
    "    \n",
    "    # Criar os pesos para o período pré\n",
    "    weight_treated_pre = T_pre / T_pre.mean()\n",
    "    weight_control_pre = ((1 - p_X_pre) / p_X_pre) / ((1 - p_X_pre) / p_X_pre).mean()\n",
    "    \n",
    "    # Calcular a Diferença em Diferenças com os pesos\n",
    "    term_treated = (Y_post_treated.mean() - Y_pre_treated.mean()) * weight_treated_pre.mean()\n",
    "    term_control = (mu0_X_post.mean() - mu0_X_pre.mean()) * weight_control_pre.mean()\n",
    "\n",
    "    DR_DiD = term_treated - term_control \n",
    "    \n",
    "    # Calcular resíduos\n",
    "    res_pre_control = Y_pre_control - model_pre_control.predict(X_pre_control)\n",
    "    res_post_control = Y_post_control - model_post_control.predict(X_post_control)\n",
    "\n",
    "    # Variância dos resíduos\n",
    "    var_res_pre_control = np.var(res_pre_control, ddof=1)\n",
    "    var_res_post_control = np.var(res_post_control, ddof=1)\n",
    "    \n",
    "    # Resíduos da regressão logística\n",
    "    res_logistic = T_pre - model_logistic.predict_proba(X_pre)[:, 1]\n",
    "    var_res_logistic = np.var(res_logistic, ddof=1)\n",
    "\n",
    "    # Tamanhos das amostras\n",
    "    n_pre_treated = len(Y_pre_treated)\n",
    "    n_post_treated = len(Y_post_treated)\n",
    "    n_pre_control = len(Y_pre_control)\n",
    "    n_post_control = len(Y_post_control)\n",
    "\n",
    "    # Calcular erro padrão\n",
    "    SE_DR_DiD = np.sqrt((var_res_post_control / n_post_treated) +\n",
    "                        (var_res_pre_control / n_pre_treated) +\n",
    "                        (var_res_post_control / n_post_control) +\n",
    "                        (var_res_pre_control / n_pre_control) +\n",
    "                        (var_res_logistic / len(X_pre)))\n",
    "\n",
    "    # Calcular t-score\n",
    "    t_score = DR_DiD / SE_DR_DiD\n",
    "\n",
    "    # Calcular p-value (bilateral)\n",
    "    p_value = 2 * (1 - stats.t.cdf(np.abs(t_score), df=n_pre_treated + n_post_treated + n_pre_control + n_post_control - 4))\n",
    "\n",
    "    # Calcular intervalo de confiança de 95%\n",
    "    confidence_interval = (\n",
    "        DR_DiD - 1.96 * SE_DR_DiD,\n",
    "        DR_DiD + 1.96 * SE_DR_DiD\n",
    "    )\n",
    "\n",
    "    return DR_DiD, SE_DR_DiD, t_score, p_value, confidence_interval\n",
    "\n",
    "# Exemplo de uso\n",
    "result_DD_DR, SE_DD_DR, t_score, p_value, confidence_interval = DD_DR_with_SE(df, X_cols, T_col, Y_col, Time_col, pre_time, post_time)\n",
    "print(\"Duplamente Robusto DiD (ATT):\", result_DD_DR)\n",
    "print(\"Erro Padrão Analítico:\", SE_DD_DR)\n",
    "print(\"t-score:\", t_score)\n",
    "print(\"p-value:\", p_value)\n",
    "print(\"Intervalo de Confiança 95%:\", confidence_interval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Y post-treated: 17.49920654296875\n",
      "Mean Y pre-treated: 17.046825408935547\n",
      "Term treated: 0.45238113403320307\n",
      "Mu0_X post mean: 16.753446934329286\n",
      "Mu0_X pre mean: 18.976769254710437\n",
      "Term control: -2.2233223203811505\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.6757034544143536"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def DD_DR_full(df, X_cols, T_col, Y_col, Time_col, pre_time, post_time):\n",
    "    # Verificar se os dados são 2x2\n",
    "    time_values = df[Time_col].unique()\n",
    "    treat_values = df[T_col].unique()\n",
    "    if len(time_values) != 2 or len(treat_values) != 2:\n",
    "        raise ValueError(\"Os dados não são 2x2.\")\n",
    "\n",
    "    # Separar dados por grupo e período\n",
    "    df_pre = df[df[Time_col] == pre_time]\n",
    "    df_post = df[df[Time_col] == post_time]\n",
    "\n",
    "    # Ajustar modelos de regressão linear para os períodos pré e pós\n",
    "    model_pre_control = LinearRegression().fit(df_pre[df_pre[T_col] == 0][X_cols], df_pre[df_pre[T_col] == 0][Y_col])\n",
    "    model_post_control = LinearRegression().fit(df_post[df_post[T_col] == 0][X_cols], df_post[df_post[T_col] == 0][Y_col])\n",
    "\n",
    "    # Previsões para o grupo controle nos períodos pré e pós\n",
    "    mu0_X_pre = model_pre_control.predict(df_pre[df_pre[T_col] == 1][X_cols])\n",
    "    mu0_X_post = model_post_control.predict(df_post[df_post[T_col] == 1][X_cols])\n",
    "\n",
    "    # Estimativa da probabilidade de tratamento p(X) no período pré\n",
    "    model_logistic = LogisticRegression(solver='liblinear').fit(df_pre[X_cols], df_pre[T_col])\n",
    "    p_X_pre = model_logistic.predict_proba(df_pre[X_cols])[:, 1]\n",
    "\n",
    "    # Criar os pesos para o período pré\n",
    "    weight_treated_pre = df_pre[T_col] / df_pre[T_col].mean()\n",
    "    weight_control_pre = ((1 - p_X_pre) / p_X_pre) / ((1 - p_X_pre) / p_X_pre).mean()\n",
    "\n",
    "    # Calcular a Diferença em Diferenças com os pesos\n",
    "    term_treated = (df_post[df_post[T_col] == 1][Y_col].mean() - df_pre[df_pre[T_col] == 1][Y_col].mean()) * weight_treated_pre.mean()\n",
    "    term_control = (mu0_X_post.mean() - mu0_X_pre.mean()) * weight_control_pre.mean()\n",
    "\n",
    "    DR_DiD = term_treated - term_control\n",
    "\n",
    "    # Debugging prints\n",
    "    print(f\"Mean Y post-treated: {df_post[df_post[T_col] == 1][Y_col].mean()}\")\n",
    "    print(f\"Mean Y pre-treated: {df_pre[df_pre[T_col] == 1][Y_col].mean()}\")\n",
    "    print(f\"Term treated: {term_treated}\")\n",
    "    print(f\"Mu0_X post mean: {mu0_X_post.mean()}\")\n",
    "    print(f\"Mu0_X pre mean: {mu0_X_pre.mean()}\")\n",
    "    print(f\"Term control: {term_control}\")\n",
    "\n",
    "    return DR_DiD\n",
    "\n",
    "# Usar a função DD_DR_full com os dados simulados\n",
    "result_DD_DR_full = DD_DR_full(df, ['bk', 'kfc', 'roys'], 'Treated', 'y', 't', 0, 1)\n",
    "result_DD_DR_full\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DR-DD (ATT): 2.676879822808422\n",
      "Erro padrão (bootstrap): 1.4567429810267496\n",
      "Intervalo de confiança 95% (bootstrap): [-0.12290625791701908, 5.604854882842082]\n",
      "t-score (bootstrap): 1.837578665333049\n",
      "p-valor (bootstrap): 0.06613439883965477\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.stats import t\n",
    "\n",
    "# Função para calcular o bootstrap\n",
    "def bootstrap_OR_DD(df, X_cols, T_col, Y_col, Time_col, pre_time, post_time, n_iterations):\n",
    "    np.random.seed(88)\n",
    "    \n",
    "    def calculate_did(sample_df):\n",
    "        return OR_DD(sample_df, X_cols, T_col, Y_col, Time_col, pre_time, post_time)\n",
    "    \n",
    "    results = Parallel(n_jobs=-1)(delayed(calculate_did)(df.sample(frac=1, replace=True)) for _ in range(n_iterations))\n",
    "    \n",
    "    return np.array(results)\n",
    "\n",
    "# Definir o número de amostras do bootstrap\n",
    "bootstrap_samples = 30000\n",
    "\n",
    "# Executar o bootstrap para calcular as estimativas do DiD\n",
    "did_bootstrap = bootstrap_OR_DD(df, X_cols, T_col, Y_col, Time_col, pre_time, post_time, bootstrap_samples)\n",
    "\n",
    "# Calcular a média e o erro padrão das estimativas do DiD\n",
    "did_mean_bootstrap = np.mean(did_bootstrap)\n",
    "did_std_bootstrap = np.std(did_bootstrap)\n",
    "\n",
    "# Calcular o intervalo de confiança de 95% usando percentis\n",
    "ci_lower_did_bootstrap = np.percentile(did_bootstrap, 2.5)\n",
    "ci_upper_did_bootstrap = np.percentile(did_bootstrap, 97.5)\n",
    "\n",
    "# Calcular os z-scores e p-valores\n",
    "t_score_did_bootstrap = did_mean_bootstrap / did_std_bootstrap\n",
    "p_value_did_bootstrap = 2 * (1 - t.cdf(abs(t_score_did_bootstrap), df=len(did_bootstrap)-1))\n",
    "\n",
    "# Exibir os resultados\n",
    "print(\"DR-DD (ATT):\", did_mean_bootstrap)\n",
    "print(\"Erro padrão (bootstrap):\", did_std_bootstrap)\n",
    "print(\"Intervalo de confiança 95% (bootstrap): [{}, {}]\".format(ci_lower_did_bootstrap, ci_upper_did_bootstrap))\n",
    "print(\"t-score (bootstrap):\", t_score_did_bootstrap)\n",
    "print(\"p-valor (bootstrap):\", p_value_did_bootstrap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro padrão (t-Student): 1.4567672606834263\n",
      "Graus de liberdade: 29999\n",
      "Intervalo de confiança 95% (t-Student): [-0.17844674529920646, 5.53220639091605]\n",
      "t-estatística (t-Student): 1.837548038766737\n",
      "p-valor (t-Student): 0.06613891556533358\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import t\n",
    "\n",
    "# Calcular o erro padrão\n",
    "standard_error = np.std(did_bootstrap, ddof=1)  # Dividido por ddof para corrigir o viés\n",
    "\n",
    "# Calcular o grau de liberdade\n",
    "degrees_of_freedom = len(did_bootstrap) - 1\n",
    "\n",
    "# Calcular o intervalo de confiança de 95% usando a distribuição t de Student\n",
    "t_score = t.ppf(0.975, df=degrees_of_freedom)  # Obtém o valor crítico para 95% de confiança\n",
    "margin_of_error = t_score * standard_error\n",
    "\n",
    "# Calcular o intervalo de confiança\n",
    "ci_lower = did_mean_bootstrap - margin_of_error\n",
    "ci_upper = did_mean_bootstrap + margin_of_error\n",
    "\n",
    "# Calcular o p-valor\n",
    "t_statistic = did_mean_bootstrap / standard_error\n",
    "p_value = 2 * (1 - t.cdf(abs(t_statistic), df=degrees_of_freedom))\n",
    "\n",
    "# Exibir os resultados\n",
    "print(\"Erro padrão (t-Student):\", standard_error)\n",
    "print(\"Graus de liberdade:\", degrees_of_freedom)\n",
    "print(\"Intervalo de confiança 95% (t-Student): [{}, {}]\".format(ci_lower, ci_upper))\n",
    "print(\"t-estatística (t-Student):\", t_statistic)\n",
    "print(\"p-valor (t-Student):\", p_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicação com o pacote CSDID\n",
    "\n",
    "Intalar o pacote no python: `pip install csdid`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csdid.att_gt import ATTgt\n",
    "import pandas as pd\n",
    "# DataFrame\n",
    "df = pd.read_stata(\"http://fmwww.bc.edu/repec/bocode/c/CardKrueger1994.dta\")\n",
    "# Há problemas na base que precisam ser ajustados\n",
    "df.loc[(df['id'] == 407) & (df['kfc'] == 1), 'id'] = 408\n",
    "# Crie a variável 'tratado' com valor inicial de 0\n",
    "df['Treated'] = 0\n",
    "# Recodifique 'tratado' para 1 se 'treated' for igual a 1\n",
    "df.loc[df['treated'] == 'NJ', 'Treated'] = 1\n",
    "# Crie a variável 'effect' como o produto entre 'tratado' e 't'\n",
    "df['Effect'] = df['Treated'] * df['t']\n",
    "# Remover lojas sem informação em ambos os anos\n",
    "df['cont'] = df['fte'].notna().groupby(df['id']).cumsum()\n",
    "df['media'] = df.groupby('id')['cont'].transform('mean')\n",
    "df = df[df['media'] == 1.5]\n",
    "df['y'] = df['fte']\n",
    "\n",
    "df['pre'] = 0\n",
    "df.loc[df['t'] == 0, 'pre'] = 1\n",
    "df['pos'] = 0\n",
    "df.loc[df['t'] == 1, 'pos'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = ATTgt(yname = \"y\",\n",
    "              gname = \"Treated\",\n",
    "              idname = \"id\",\n",
    "              tname = \"pos\",\n",
    "              xformla = f\"bk + kfc + roys\",\n",
    "              data = df,\n",
    "              ).fit(est_method = 'dr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Group</th>\n",
       "      <th>Time</th>\n",
       "      <th>ATT(g, t)</th>\n",
       "      <th>Post</th>\n",
       "      <th>Std. Error</th>\n",
       "      <th>[95% Pointwise</th>\n",
       "      <th>Conf. Band]</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.9425</td>\n",
       "      <td>1</td>\n",
       "      <td>2.823</td>\n",
       "      <td>-2.1573</td>\n",
       "      <td>8.0424</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Group  Time  ATT(g, t)  Post  Std. Error  [95% Pointwise  Conf. Band]  \n",
       "0      1     1     2.9425     1       2.823         -2.1573       8.0424  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.summ_attgt().summary2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "out2 = ATTgt(yname = \"y\",\n",
    "              gname = \"Treated\",\n",
    "              idname = \"id\",\n",
    "              tname = \"t\",\n",
    "              xformla = \"bk + kfc + roys\",\n",
    "              data = df,\n",
    "              ).fit(est_method = 'or')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Group</th>\n",
       "      <th>Time</th>\n",
       "      <th>ATT(g, t)</th>\n",
       "      <th>Post</th>\n",
       "      <th>Std. Error</th>\n",
       "      <th>[95% Pointwise</th>\n",
       "      <th>Conf. Band]</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.9425</td>\n",
       "      <td>1</td>\n",
       "      <td>2.7442</td>\n",
       "      <td>-2.1765</td>\n",
       "      <td>8.0615</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Group  Time  ATT(g, t)  Post  Std. Error  [95% Pointwise  Conf. Band]  \n",
       "0      1     1     2.9425     1      2.7442         -2.1765       8.0615  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2.summ_attgt().summary2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Group</th>\n",
       "      <th>Time</th>\n",
       "      <th>ATT(g, t)</th>\n",
       "      <th>Post</th>\n",
       "      <th>Std. Error</th>\n",
       "      <th>[95% Pointwise</th>\n",
       "      <th>Conf. Band]</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.9425</td>\n",
       "      <td>1</td>\n",
       "      <td>2.8326</td>\n",
       "      <td>-2.1787</td>\n",
       "      <td>8.0638</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Group  Time  ATT(g, t)  Post  Std. Error  [95% Pointwise  Conf. Band]  \n",
       "0      1     1     2.9425     1      2.8326         -2.1787       8.0638  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out3 = ATTgt(yname = \"y\",\n",
    "              gname = \"Treated\",\n",
    "              idname = \"id\",\n",
    "              tname = \"t\",\n",
    "              xformla = \"bk + kfc + roys\",\n",
    "              data = df,\n",
    "              ).fit(est_method = 'ipw')\n",
    "out3.summ_attgt().summary2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from differences import ATTgt\n",
    "\n",
    "# Definir os indices (estrutura de painel)\n",
    "df.set_index(['id', 't'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ATTgt in module differences.attgt.attgt:\n",
      "\n",
      "class ATTgt(builtins.object)\n",
      " |  ATTgt(data: 'DataFrame', cohort_name: 'str', strata_name: 'str' = None, base_period: 'str' = 'varying', anticipation: 'int' = 0, freq: 'str' = None)\n",
      " |  \n",
      " |  Group-Time ATT\n",
      " |  \n",
      " |  Difference in differences estimation and inference fot the following use cases\n",
      " |  \n",
      " |  - balanced panels, unbalanced panels or repeated cross-section\n",
      " |  - two or multiple periods\n",
      " |  - fixed or staggered treatment timing\n",
      " |  - binary or multi-valued treatment\n",
      " |  - heterogeneous treatment effects\n",
      " |  \n",
      " |  Group\n",
      " |  -----\n",
      " |      did\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  data: DataFrame\n",
      " |      pandas DataFrame\n",
      " |  \n",
      " |      .. code-block:: python\n",
      " |  \n",
      " |          df = df.set_index(['entity', 'time'])\n",
      " |  \n",
      " |      where *df* is the dataframe to use, *'entity'* should be replaced with the\n",
      " |      name of the entity column and *'time'* should be replaced with\n",
      " |      the name of the time column.\n",
      " |  \n",
      " |  cohort_name: str\n",
      " |      cohort name\n",
      " |  \n",
      " |  base_period: str, default: ``\"varying\"``\n",
      " |  \n",
      " |      - ``\"universal\"``\n",
      " |  \n",
      " |      - ``\"varying\"``\n",
      " |  \n",
      " |  anticipation: *int*, default: ``0``\n",
      " |      The number of time periods before participating in the treatment where units can\n",
      " |      anticipate participating in the treatment, and therefore it can affect their untreated\n",
      " |      potential outcomes\n",
      " |  \n",
      " |  strata_name: str, default: ``None``\n",
      " |      The name of the column to be used in case of multi-valued treatment, used to calculate\n",
      " |      cohort-time-stratum ATT.\n",
      " |  \n",
      " |      If stratum name is ``None``, fit() will return cohort-time ATT.\n",
      " |  \n",
      " |  freq: *str* | None, default: ``None``\n",
      " |      the date frequency of the panel data. Required if the time index is datetime.\n",
      " |      For example, if the time column is a monthly datetime then freq='M'. Check\n",
      " |      `offset aliases <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases>`_,\n",
      " |      for a list of available frequencies.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, data: 'DataFrame', cohort_name: 'str', strata_name: 'str' = None, base_period: 'str' = 'varying', anticipation: 'int' = 0, freq: 'str' = None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  aggregate(self, type_of_aggregation: 'str | None' = 'simple', overall: 'bool' = False, difference: 'bool | list | dict[str, list]' = False, alpha: 'float' = 0.05, cluster_var: 'list | str' = None, boot_iterations: 'int' = 0, random_state: 'int' = None, n_jobs: 'int' = 1, backend: 'str' = 'loky') -> 'DataFrame'\n",
      " |      Aggregate the ATTgt\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      type_of_aggregation: *str* | None, default: ``None``\n",
      " |      \n",
      " |          - ``\"simple\"``\n",
      " |              to calculate the weighted average of all cohort-time average treatment effects,\n",
      " |              with weights proportional to the cohort size.\n",
      " |      \n",
      " |          - ``\"event\"`` or ``\"event\"``\n",
      " |              to calculate the average effects in each relative period:\n",
      " |              periods relative to the treatment; as in an event study.\n",
      " |      \n",
      " |          - ``\"cohort\"``\n",
      " |              to calculate the average treatment effect in each cohort.\n",
      " |      \n",
      " |          - ``\"time\"`` or ``\"time\"``\n",
      " |              to calculate the average treatment effect in each time time.\n",
      " |      \n",
      " |      overall: *bool*, default: ``False``\n",
      " |          calculates the average effect within each type_of_aggregation.\n",
      " |      \n",
      " |          - if type_of_aggregation is set to ``\"event\"`` or ``\"event\"``\n",
      " |              to calculate the average effect of the treatment across positive relative periods\n",
      " |      \n",
      " |          - if type_of_aggregation is set to ``\"cohort\"``\n",
      " |              to calculate the average effect of the treatment across cohorts\n",
      " |      \n",
      " |          - if type_of_aggregation is set to ``\"time\"`` or ``\"time\"``\n",
      " |              to calculate the average effect of the treatment across time times\n",
      " |      \n",
      " |      difference: *bool* | *list* | *dict*, default: ``False``\n",
      " |          take the difference of the estimates\n",
      " |      \n",
      " |          Available options are:\n",
      " |      \n",
      " |          - ``True``\n",
      " |              to calculate the difference between 2 samples or 2 strata of treatments\n",
      " |      \n",
      " |          .. note::\n",
      " |              - Samples difference: if the estimation is run on 2 samples and more than\n",
      " |                2 strata,\n",
      " |                the estimates for the two samples will be subtracted, as long as there are no\n",
      " |                strata that have the same names as the samples, in that case use\n",
      " |                a dictionary as indicated below\n",
      " |      \n",
      " |              - strata difference: if the estimation is run on 2 strata and more than\n",
      " |                2 samples,\n",
      " |                the estimates for the two strata will be subtracted, as long as there are no\n",
      " |                samples that have the same names as the strata, in that case use\n",
      " |                a dictionary as indicated below\n",
      " |      \n",
      " |          - ``[sample-0, sample-1]`` or ``[stratum-A, stratum-B]``\n",
      " |              to calculate the difference between 2 samples listed in the argument or\n",
      " |              the 2 strata of treatments listed in the argument\n",
      " |      \n",
      " |          .. note::\n",
      " |              - Samples difference: if there are strata with the same name as the two samples\n",
      " |                listed, use a dictionary as indicated below\n",
      " |      \n",
      " |              - strata difference: if there are samples with the same name as the two strata\n",
      " |                listed, use a dictionary as indicated below\n",
      " |      \n",
      " |          - ``{'strata': [stratum-A, stratum-B]}`` or\n",
      " |            ``{'sample_names': [sample-0, sample-1]}``\n",
      " |      \n",
      " |      alpha: *float*, default: ``0.05``\n",
      " |      \n",
      " |          The significance level.\n",
      " |      \n",
      " |      cluster_var: *str* | *list* | *None*, default: ``None``\n",
      " |          cluster variables\n",
      " |      \n",
      " |      boot_iterations: *int*, default: ``0``\n",
      " |          bootstrap iterations\n",
      " |      \n",
      " |      random_state: *int* | None, default: ``None``\n",
      " |          seed for bootstrap\n",
      " |      \n",
      " |      n_jobs: *int*, default: ``1``\n",
      " |          The maximum number of concurrently running jobs. If -1 all CPUs are used.\n",
      " |      \n",
      " |          If ≠ 1, concurrent jobs will be run for:\n",
      " |      \n",
      " |          - computing the bootstrap; the influence function is split into n_jobs parts and the\n",
      " |            boostrap is computed concurrently for each part\n",
      " |      \n",
      " |          Parallelization is implemented using\n",
      " |          `joblib <https://joblib.readthedocs.io/en/latest/generated/joblib.Parallel.html>`_,\n",
      " |          refer to its documentation for additional details on n_jobs.\n",
      " |      \n",
      " |      backend: *int*, default: ``\"loky\"``\n",
      " |          Parallelization backend implementation.\n",
      " |      \n",
      " |          Parallelization is implemented using\n",
      " |          `joblib <https://joblib.readthedocs.io/en/latest/generated/joblib.Parallel.html>`_,\n",
      " |          refer to its documentation for additional details on backend.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      A DataFrame with the requested aggregation\n",
      " |  \n",
      " |  estimation_details(self, type_of_aggregation: 'str' = None)\n",
      " |  \n",
      " |  fit(self, formula: 'str', weights_name: 'str' = None, control_group: 'str' = 'never_treated', base_delta: 'str | list | dict' = 'base', est_method: 'str | Callable' = 'dr', as_repeated_cross_section: 'bool' = None, boot_iterations: 'int' = 0, random_state: 'int' = None, alpha: 'float' = 0.05, cluster_var: 'list | str' = None, split_sample_by: 'Callable | str | dict' = None, n_jobs: 'int' = 1, backend: 'str' = 'loky', progress_bar: 'bool' = True) -> 'DataFrame'\n",
      " |      Computes the cohort-time-(stratum) average treatment effects:\n",
      " |      \n",
      " |      effects for each cohort, in each time, (for each stratum).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      formula: str\n",
      " |          Wilkinson formula for the outcome variable and covariates\n",
      " |      \n",
      " |          If no covariates the formula must contain only the name of the outcome variable\n",
      " |      \n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              # example with covariates\n",
      " |              formula = 'y ~ a + b + a:b'\n",
      " |      \n",
      " |              # example without covariates\n",
      " |              formula = 'y'\n",
      " |      \n",
      " |          Formulas are implemented using\n",
      " |          `formulaic <https://matthewwardrop.github.io/formulaic/>`_,\n",
      " |          refer to its documentation for additional details.\n",
      " |      \n",
      " |      weights_name: *str* | None, default: ``None``\n",
      " |      \n",
      " |          The name of the column containing the sampling weights.\n",
      " |          If None, all observations have same weights.\n",
      " |      \n",
      " |      control_group: *str*, default: ``\"never_treated\"``\n",
      " |      \n",
      " |          - ``\"never_treated\"``\n",
      " |      \n",
      " |          - ``\"not_yet_treated\"``\n",
      " |      \n",
      " |      base_delta: *str* | *list* | *dict*, default: ``\"base\"``\n",
      " |      \n",
      " |          Use base period values for covariates and/or delta values, i.e. the change in value,\n",
      " |          between the value of covariates at *time* and the value at *base period*.\n",
      " |      \n",
      " |          Available options are:\n",
      " |      \n",
      " |          - ``\"base\"``\n",
      " |              the value of :underline:`each` covariate is set to its *base period* value\n",
      " |      \n",
      " |          - ``\"delta\"``\n",
      " |              the value of :underline:`each` time-varying covariate is set to the delta.\n",
      " |              Time-constant covariates included through *x_formula* are dropped, and a warning\n",
      " |              issued.\n",
      " |      \n",
      " |          - ``[\"base\", \"delta\"]`` or ``\"base_delta\"``\n",
      " |              the value of :underline:`each` covariate is set to its *base period* value, and\n",
      " |              the value of :underline:`each` time-varying covariate is set to the delta.\n",
      " |      \n",
      " |          - ``{'base': ['a', 'b', ..]}``\n",
      " |              the value of the :underline:`specified` covariates is set to its *base period*\n",
      " |              value, and the value of :underline:`each` time-varying covariate is set to\n",
      " |              the delta. A warning is issued if *x_formula* included\n",
      " |              time-constant covariates that are not included in *base_delta*.\n",
      " |      \n",
      " |          - ``{'delta': ['c', 'd', ..]}``\n",
      " |              the value of :underline:`each` covariate is set to its *base period* value, and\n",
      " |              the value of the :underline:`specified` time-varying covariates\n",
      " |              is set to the delta. If the covariates included in *'delta'* are not\n",
      " |              time-varying they will be removed from the list.\n",
      " |      \n",
      " |          - ``{'base': ['a', 'b', ..], 'delta': ['c', 'd', ..]}``\n",
      " |              the value of the :underline:`specified` covariates\n",
      " |              is set to its *base period* value, and\n",
      " |              the value of the :underline:`specified` time-varying covariates is set to\n",
      " |              the delta. A warning is issued if *x_formula* included time-constant covariates\n",
      " |              that are not included in *'delta'*. If the covariates included in 'delta' are not\n",
      " |              time-varying they will be removed from the list.\n",
      " |      \n",
      " |      est_method: *str*, default: ``\"dr-mle\"``\n",
      " |      \n",
      " |          - ``\"dr-mle\"`` or ``\"dr\"``\n",
      " |              for locally efficient doubly robust DiD estimator,\n",
      " |              with logistic propensity score model for the probability of being treated\n",
      " |      \n",
      " |          - ``\"dr-ipt\"``\n",
      " |              for locally efficient doubly robust DiD estimator,\n",
      " |              with propensity score estimated using the inverse probability tilting\n",
      " |      \n",
      " |          - ``\"reg\"``\n",
      " |              for outcome regression DiD estimator\n",
      " |      \n",
      " |          - ``\"std_ipw-mle\"`` or ``\"std_ipw\"``\n",
      " |              for standardized inverse probability weighted DiD estimator,\n",
      " |              with logistic propensity score model for the probability of being treated\n",
      " |      \n",
      " |      as_repeated_cross_section: *bool* | None, default: ``None``\n",
      " |      \n",
      " |      boot_iterations: *int*, default: ``0``\n",
      " |      \n",
      " |      random_state: *int* | None, default: ``None``\n",
      " |      \n",
      " |      alpha: *float*, default: ``0.05``\n",
      " |      \n",
      " |          The significance level.\n",
      " |      \n",
      " |      cluster_var: *str* | *list* | *None*, default: ``None``\n",
      " |      \n",
      " |      split_sample_by: *str* | *Callable* | None, default: ``None``\n",
      " |          The name of the column along which to split the data, or a function which takes the\n",
      " |          data and returns a sample mask for a binary split, for example:\n",
      " |      \n",
      " |          .. code-block:: python\n",
      " |      \n",
      " |              lambda: x = x['column name'] >= x['column name'].median()\n",
      " |      \n",
      " |          The estimation of the ATT will be run separately for each specified sample;\n",
      " |          used for heterogeneity analysis.\n",
      " |      \n",
      " |      n_jobs: *int*, default: ``1``\n",
      " |          The maximum number of concurrently running jobs. If -1 all CPUs are used.\n",
      " |      \n",
      " |          If ≠ 1, concurrent jobs will be run for two separate tasks:\n",
      " |      \n",
      " |          - computing the cohort-time ATT; each cohort-time is assigned to a job\n",
      " |      \n",
      " |          - computing the bootstrap; the influence function is split into n_jobs parts and the\n",
      " |            boostrap is computed concurrently for each part\n",
      " |      \n",
      " |          Parallelization is implemented using\n",
      " |          `joblib <https://joblib.readthedocs.io/en/latest/generated/joblib.Parallel.html>`_,\n",
      " |          refer to its documentation for additional details on n_jobs.\n",
      " |      \n",
      " |      backend: *int*, default: ``\"loky\"``\n",
      " |          Parallelization backend implementation.\n",
      " |      \n",
      " |          Parallelization is implemented using\n",
      " |          `joblib <https://joblib.readthedocs.io/en/latest/generated/joblib.Parallel.html>`_,\n",
      " |          refer to its documentation for additional details on backend.\n",
      " |      \n",
      " |      progress_bar: *bool*, default: ``True``\n",
      " |          If True, a progress bar will display the progress over the cohort-times iterations\n",
      " |          and/or the iterations over the number of boostrap concurrent splits\n",
      " |          (not the bootstrap iterations).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      A DataFrame with the group time ATTs\n",
      " |  \n",
      " |  group_time(self, feasible: 'bool' = False) -> 'list[dict]'\n",
      " |      Returns\n",
      " |      -------\n",
      " |      a list of dictionaries where each dictionary keys are:\n",
      " |      ``cohort``, ``base_period``, ``time``, (``stratum``)\n",
      " |  \n",
      " |  results(self, type_of_aggregation: 'str' = None, overall: 'bool' = False, difference: 'bool' = False, to_dataframe: 'bool' = True, add_info: 'bool' = False)\n",
      " |      provides easy access to cached results.\n",
      " |      this method must be called after fit and/or aggregate depending\n",
      " |      on the parameters requested\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      type_of_aggregation: *str* | None, default: ``None``\n",
      " |      \n",
      " |          - ``\"simple\"``\n",
      " |              to return the weighted average of all cohort-time average treatment effects,\n",
      " |              with weights proportional to the cohort size.\n",
      " |      \n",
      " |          - ``\"event\"`` or ``\"event\"``\n",
      " |              to return the average effects in each relative period:\n",
      " |              periods relative to the treatment; as in an event study.\n",
      " |      \n",
      " |          - ``\"cohort\"``\n",
      " |              to return the average treatment effect in each cohort.\n",
      " |      \n",
      " |          - ``\"time\"`` or ``\"time\"``\n",
      " |              to return the average treatment effect in each time time.\n",
      " |      \n",
      " |      overall: *bool*, default: ``False``\n",
      " |          calculates the average effect within each type_of_aggregation.\n",
      " |      \n",
      " |          - if type_of_aggregation is set to ``\"event\"`` or ``\"event\"``\n",
      " |              to return the average effect of the treatment across positive relative periods\n",
      " |      \n",
      " |          - if type_of_aggregation is set to ``\"cohort\"``\n",
      " |              to return the average effect of the treatment across cohorts\n",
      " |      \n",
      " |          - if type_of_aggregation is set to ``\"time\"`` or ``\"time\"``\n",
      " |              to return the average effect of the treatment across time times\n",
      " |      \n",
      " |      difference: *bool*, default: ``False``\n",
      " |          to return the most recent estimated difference\n",
      " |      \n",
      " |      to_dataframe\n",
      " |          whether to return the result in a DataFrame or a list of namedtuples\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Either a pandas dataframe or a list of namedtuples\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  sample_names\n",
      " |      Returns\n",
      " |      -------\n",
      " |  \n",
      " |  wald_pre_test\n",
      " |      Returns\n",
      " |      -------\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  data\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ATTgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\differences\\tools\\panel_validation.py:344: UserWarning: 391 entities have been dropped because always treated (treated from before their first time)\n",
      "  warn(\n",
      "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\differences\\tools\\panel_validation.py:404: UserWarning: No never treated entities. Using the last treated group as comparison group and keeping only the time periods before the last cohort date\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from differences import ATTgt\n",
    "\n",
    "# Definir os indices (estrutura de painel)\n",
    "df.set_index(['id', 't'], inplace=True)\n",
    "\n",
    "att_gt = ATTgt(data=df, cohort_name=\"Effect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicação do DRDID de Sant'Anna e Zhao (2020) no R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É possível utilizar também o Sant’Anna and Zhao (2020) doubly robust DiD estimator based on stabilized inverse probability weighting and ordinary least squares [dripw].\n",
    "\n",
    "Para isso precisamos recalcular o peso e utilizar as métricas estabilizadoras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Podemos utilizar o pacote do R e calcular diretamente o DRDID\n",
    "\n",
    "Lembre-se de alterar o kernell de Python para R.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using GitHub PAT from the git credential store.\n",
      "\n",
      "Skipping install of 'DRDID' from a github remote, the SHA1 (8a1c09f9) has not changed since last install.\n",
      "  Use `force = TRUE` to force installation\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# install.packages(\"remotes\")\n",
    "remotes::install_github(\"pedrohcgs/DRDID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "library(DRDID)\n",
    "data <- read.csv(\"https://github.com/Daniel-Uhr/data/raw/main/card_krueger.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Call:\n",
      "drdid(yname = \"y\", tname = \"t\", idname = \"id\", dname = \"Treated\", \n",
      "    xformla = ~bk + kfc + roys, data = data, panel = TRUE)\n",
      "------------------------------------------------------------------\n",
      " Further improved locally efficient DR DID estimator for the ATT:\n",
      " \n",
      "   ATT     Std. Error  t value    Pr(>|t|)  [95% Conf. Interval] \n",
      "  2.6757     1.2188     2.1953     0.0281     0.2868     5.0646  \n",
      "------------------------------------------------------------------\n",
      " Estimator based on panel data.\n",
      " Outcome regression est. method: weighted least squares.\n",
      " Propensity score est. method: inverse prob. tilting.\n",
      " Analytical standard error.\n",
      "------------------------------------------------------------------\n",
      " See Sant'Anna and Zhao (2020) for details."
     ]
    }
   ],
   "source": [
    "# Implement improved locally efficient DR DID:\n",
    "\n",
    "out <- drdid(yname = \"y\", tname = \"t\", idname = \"id\", dname = \"Treated\", \n",
    "             xformla= ~ bk + kfc + roys,\n",
    "             data = data, panel = TRUE)\n",
    "summary(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D.shape: (782,), deltaY.shape: (391,), int_cov.shape: (782, 4), i_weights.shape: (782,)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "boolean index did not match indexed array along dimension 0; dimension is 391 but corresponding boolean dimension is 782",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/Daniel-Uhr/data/raw/main/card_krueger.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Testar a função pydrdid sem covariáveis\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mpydrdid\u001b[49m\u001b[43m(\u001b[49m\u001b[43myname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTreated\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxformla\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m~ bk + kfc + roys\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpanel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m result\u001b[38;5;241m.\u001b[39msummary()\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyDRDID\\drdid.py:276\u001b[0m, in \u001b[0;36mpydrdid\u001b[1;34m(yname, tname, idname, dname, xformla, data, panel)\u001b[0m\n\u001b[0;32m    274\u001b[0m     covariates \u001b[38;5;241m=\u001b[39m patsy\u001b[38;5;241m.\u001b[39mdmatrix(xformla, data, return_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataframe\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m    275\u001b[0m     i_weights \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mloc[data[tname] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi_weights\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi_weights\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 276\u001b[0m     att, inf_func \u001b[38;5;241m=\u001b[39m \u001b[43mdrdid_panel\u001b[49m\u001b[43m(\u001b[49m\u001b[43my1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcovariates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    278\u001b[0m     post \u001b[38;5;241m=\u001b[39m data[tname]\u001b[38;5;241m.\u001b[39mvalues\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyDRDID\\drdid.py:216\u001b[0m, in \u001b[0;36mdrdid_panel\u001b[1;34m(y1, y0, D, covariates, i_weights)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;66;03m# Compute the Outcome regression for the control group using wols\u001b[39;00m\n\u001b[0;32m    215\u001b[0m mask \u001b[38;5;241m=\u001b[39m D \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 216\u001b[0m reg_model \u001b[38;5;241m=\u001b[39m sm\u001b[38;5;241m.\u001b[39mWLS(\u001b[43mdeltaY\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m]\u001b[49m, int_cov[mask], weights\u001b[38;5;241m=\u001b[39mi_weights[mask])\n\u001b[0;32m    217\u001b[0m reg_results \u001b[38;5;241m=\u001b[39m reg_model\u001b[38;5;241m.\u001b[39mfit()\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(np\u001b[38;5;241m.\u001b[39misnan(reg_results\u001b[38;5;241m.\u001b[39mparams)):\n",
      "\u001b[1;31mIndexError\u001b[0m: boolean index did not match indexed array along dimension 0; dimension is 391 but corresponding boolean dimension is 782"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyDRDID import pydrdid\n",
    "\n",
    "# Carregar os dados\n",
    "data = pd.read_csv(\"https://github.com/Daniel-Uhr/data/raw/main/card_krueger.csv\")\n",
    "\n",
    "# Testar a função pydrdid sem covariáveis\n",
    "result = pydrdid(yname=\"y\", tname=\"t\", idname=\"id\", dname=\"Treated\", xformla= '~ bk + kfc + roys', data=data, panel=True)\n",
    "\n",
    "result.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D.shape: (782,), deltaY.shape: (391,), int_cov.shape: (782, 1), i_weights.shape: (782,)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "boolean index did not match indexed array along dimension 0; dimension is 391 but corresponding boolean dimension is 782",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/Daniel-Uhr/data/raw/main/card_krueger.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Testar a função pydrdid sem covariáveis\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mpydrdid\u001b[49m\u001b[43m(\u001b[49m\u001b[43myname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTreated\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpanel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m result\u001b[38;5;241m.\u001b[39msummary()\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyDRDID\\drdid.py:276\u001b[0m, in \u001b[0;36mpydrdid\u001b[1;34m(yname, tname, idname, dname, xformla, data, panel)\u001b[0m\n\u001b[0;32m    274\u001b[0m     covariates \u001b[38;5;241m=\u001b[39m patsy\u001b[38;5;241m.\u001b[39mdmatrix(xformla, data, return_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataframe\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m    275\u001b[0m     i_weights \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mloc[data[tname] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi_weights\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi_weights\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 276\u001b[0m     att, inf_func \u001b[38;5;241m=\u001b[39m \u001b[43mdrdid_panel\u001b[49m\u001b[43m(\u001b[49m\u001b[43my1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcovariates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    278\u001b[0m     post \u001b[38;5;241m=\u001b[39m data[tname]\u001b[38;5;241m.\u001b[39mvalues\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyDRDID\\drdid.py:216\u001b[0m, in \u001b[0;36mdrdid_panel\u001b[1;34m(y1, y0, D, covariates, i_weights)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;66;03m# Compute the Outcome regression for the control group using wols\u001b[39;00m\n\u001b[0;32m    215\u001b[0m mask \u001b[38;5;241m=\u001b[39m D \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 216\u001b[0m reg_model \u001b[38;5;241m=\u001b[39m sm\u001b[38;5;241m.\u001b[39mWLS(\u001b[43mdeltaY\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m]\u001b[49m, int_cov[mask], weights\u001b[38;5;241m=\u001b[39mi_weights[mask])\n\u001b[0;32m    217\u001b[0m reg_results \u001b[38;5;241m=\u001b[39m reg_model\u001b[38;5;241m.\u001b[39mfit()\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(np\u001b[38;5;241m.\u001b[39misnan(reg_results\u001b[38;5;241m.\u001b[39mparams)):\n",
      "\u001b[1;31mIndexError\u001b[0m: boolean index did not match indexed array along dimension 0; dimension is 391 but corresponding boolean dimension is 782"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyDRDID import pydrdid\n",
    "\n",
    "# Carregar os dados\n",
    "data = pd.read_csv(\"https://github.com/Daniel-Uhr/data/raw/main/card_krueger.csv\")\n",
    "\n",
    "# Testar a função pydrdid sem covariáveis\n",
    "result = pydrdid(yname=\"y\", tname=\"t\", idname=\"id\", dname=\"Treated\", data=data, panel=True)\n",
    "\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Call:\n",
      "ordid(yname = \"y\", tname = \"t\", idname = \"id\", dname = \"Treated\", \n",
      "    xformla = ~bk + kfc + roys, data = data, panel = TRUE)\n",
      "------------------------------------------------------------------\n",
      " Outcome-Regression DID estimator for the ATT:\n",
      " \n",
      "   ATT     Std. Error  t value    Pr(>|t|)  [95% Conf. Interval] \n",
      "  2.6757     1.2188     2.1953     0.0281     0.2868     5.0646  \n",
      "------------------------------------------------------------------\n",
      " Estimator based on panel data.\n",
      " Outcome regression est. method: OLS.\n",
      " Analytical standard error.\n",
      "------------------------------------------------------------------\n",
      " See Sant'Anna and Zhao (2020) for details."
     ]
    }
   ],
   "source": [
    "library(DRDID)\n",
    "data <- read.csv(\"https://github.com/Daniel-Uhr/data/raw/main/card_krueger.csv\")\n",
    "\n",
    "out2 <- ordid(yname=\"y\", tname = \"t\", idname = \"id\", dname = \"Treated\", xformla= ~ bk + kfc + roys, data = data, panel = TRUE)\n",
    "summary(out2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " Call:\n",
       "drdid(yname = \"re\", tname = \"year\", idname = \"id\", dname = \"experimental\", \n",
       "    xformla = ~age + educ + black + married + nodegree + hisp + \n",
       "        re74, data = nsw_long, panel = TRUE)\n",
       "------------------------------------------------------------------\n",
       " Further improved locally efficient DR DID estimator for the ATT:\n",
       " \n",
       "   ATT     Std. Error  t value    Pr(>|t|)  [95% Conf. Interval] \n",
       "-428.4786   343.0759   -1.2489     0.2117   -1100.9073  243.9501 \n",
       "------------------------------------------------------------------\n",
       " Estimator based on panel data.\n",
       " Outcome regression est. method: weighted least squares.\n",
       " Propensity score est. method: inverse prob. tilting.\n",
       " Analytical standard error.\n",
       "------------------------------------------------------------------\n",
       " See Sant'Anna and Zhao (2020) for details."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(DRDID)\n",
    "# Implement improved DR locally efficient DiD with panel data \n",
    "drdid(yname=\"re\", tname = \"year\", idname = \"id\", dname = \"experimental\", xformla= ~ age+ educ+ black+ married+ nodegree+ hisp+ re74, data = nsw_long, panel = TRUE) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " Call:\n",
       "drdid(yname = \"re\", tname = \"year\", idname = \"id\", dname = \"experimental\", \n",
       "    xformla = ~age + educ + black + married + nodegree + hisp + \n",
       "        re74, data = nsw_long, panel = TRUE, estMethod = \"trad\")\n",
       "------------------------------------------------------------------\n",
       " Locally efficient DR DID estimator for the ATT:\n",
       " \n",
       "   ATT     Std. Error  t value    Pr(>|t|)  [95% Conf. Interval] \n",
       "-405.8749   345.4175    -1.175      0.24    -1082.8932  271.1433 \n",
       "------------------------------------------------------------------\n",
       " Estimator based on panel data.\n",
       " Outcome regression est. method: OLS.\n",
       " Propensity score est. method: maximum likelihood.\n",
       " Analytical standard error.\n",
       "------------------------------------------------------------------\n",
       " See Sant'Anna and Zhao (2020) for details."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Implement \"traditional\" DR locally efficient DiD with panel data \n",
    "drdid(yname=\"re\", tname = \"year\", idname = \"id\", dname = \"experimental\", xformla= ~ age+ educ+ black+ married+ nodegree+ hisp+ re74, data = nsw_long, panel = TRUE, estMethod = \"trad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "boolean index did not match indexed array along dimension 0; dimension is 0 but corresponding boolean dimension is 38408",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/Daniel-Uhr/data/raw/main/nsw_long.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Executar o estimador DRDID\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mpydrdid\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43myname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mre\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# Nome da variável de resultado\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43myear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# Nome da variável de tempo (antes/depois do tratamento)\u001b[39;49;00m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43midname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Nome da variável de identificação do indivíduo\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexperimental\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Nome da variável de tratamento\u001b[39;49;00m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxformla\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m~ age + educ + black + married + nodegree + hisp + re74\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Fórmula das covariáveis\u001b[39;49;00m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# Dados carregados\u001b[39;49;00m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpanel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# Indicador de que são dados de painel\u001b[39;49;00m\n\u001b[0;32m     16\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Exibir os resultados\u001b[39;00m\n\u001b[0;32m     19\u001b[0m result\u001b[38;5;241m.\u001b[39msummary()\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyDRDID\\drdid.py:285\u001b[0m, in \u001b[0;36mpydrdid\u001b[1;34m(yname, tname, idname, dname, xformla, data, panel)\u001b[0m\n\u001b[0;32m    283\u001b[0m     covariates \u001b[38;5;241m=\u001b[39m patsy\u001b[38;5;241m.\u001b[39mdmatrix(xformla, data, return_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataframe\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m    284\u001b[0m     i_weights \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mloc[data[tname] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi_weights\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi_weights\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 285\u001b[0m     att, inf_func \u001b[38;5;241m=\u001b[39m \u001b[43mdrdid_panel\u001b[49m\u001b[43m(\u001b[49m\u001b[43my1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcovariates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    287\u001b[0m     post \u001b[38;5;241m=\u001b[39m data[tname]\u001b[38;5;241m.\u001b[39mvalues\n",
      "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyDRDID\\drdid.py:225\u001b[0m, in \u001b[0;36mdrdid_panel\u001b[1;34m(y1, y0, D, covariates, i_weights)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;66;03m# Compute the Outcome regression for the control group using wols\u001b[39;00m\n\u001b[0;32m    224\u001b[0m mask \u001b[38;5;241m=\u001b[39m D \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 225\u001b[0m reg_model \u001b[38;5;241m=\u001b[39m sm\u001b[38;5;241m.\u001b[39mWLS(\u001b[43mdeltaY\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m]\u001b[49m, int_cov[mask], weights\u001b[38;5;241m=\u001b[39mi_weights[mask])\n\u001b[0;32m    226\u001b[0m reg_results \u001b[38;5;241m=\u001b[39m reg_model\u001b[38;5;241m.\u001b[39mfit()\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(np\u001b[38;5;241m.\u001b[39misnan(reg_results\u001b[38;5;241m.\u001b[39mparams)):\n",
      "\u001b[1;31mIndexError\u001b[0m: boolean index did not match indexed array along dimension 0; dimension is 0 but corresponding boolean dimension is 38408"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyDRDID import pydrdid  \n",
    "\n",
    "# Carregar os dados do CSV\n",
    "data = pd.read_csv(\"https://github.com/Daniel-Uhr/data/raw/main/nsw_long.csv\")\n",
    "\n",
    "# Executar o estimador DRDID\n",
    "result = pydrdid(\n",
    "    yname=\"re\",         # Nome da variável de resultado\n",
    "    tname=\"year\",       # Nome da variável de tempo (antes/depois do tratamento)\n",
    "    idname=\"id\",        # Nome da variável de identificação do indivíduo\n",
    "    dname=\"experimental\", # Nome da variável de tratamento\n",
    "    xformla=\"~ age + educ + black + married + nodegree + hisp + re74\", # Fórmula das covariáveis\n",
    "    data=data,          # Dados carregados\n",
    "    panel=True          # Indicador de que são dados de painel\n",
    ")\n",
    "\n",
    "# Exibir os resultados\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyDRDID.pyordid'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyDRDID\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyordid\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pyordid\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Carregar os dados\u001b[39;00m\n\u001b[0;32m      5\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/Daniel-Uhr/data/raw/main/card_krueger.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyDRDID.pyordid'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyDRDID.pyordid import pyordid\n",
    "\n",
    "# Carregar os dados\n",
    "data = pd.read_csv(\"https://github.com/Daniel-Uhr/data/raw/main/card_krueger.csv\")\n",
    "\n",
    "# Executar o estimador ORDID\n",
    "result = pyordid(\n",
    "    yname=\"y\",\n",
    "    tname=\"t\",\n",
    "    idname=\"id\",\n",
    "    dname=\"Treated\",\n",
    "    xformla=\"~ bk + kfc + roys\",\n",
    "    data=data,\n",
    "    panel=True\n",
    ")\n",
    "\n",
    "# Exibir os resultados\n",
    "result.summary()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
